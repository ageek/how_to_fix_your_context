{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Summarization\n",
    "\n",
    "## Background from Drew's Post\n",
    "\n",
    "Context summarization involves condensing accumulated context into shorter versions to prevent context distraction. Drew Breunig highlights key research findings:\n",
    "\n",
    "- **Gemini research found performance issues with contexts over 100,000 tokens**\n",
    "- **Prevents \"Context Distraction\"** where models focus too much on accumulated history\n",
    "- **Helps models focus on novel reasoning** instead of repeating historical actions\n",
    "- **Essential for long-running conversations** that can span hundreds of turns\n",
    "\n",
    "Key insights from Drew's research:\n",
    "- Context summarization prevents models from becoming overly focused on historical context\n",
    "- Critical for maintaining performance in extended interactions\n",
    "- Can be implemented at various points in an agent's trajectory\n",
    "- Often requires careful engineering to preserve essential information\n",
    "\n",
    "## Context Summarization in Practice\n",
    "\n",
    "Agent interactions can span [hundreds of turns](https://www.anthropic.com/engineering/built-multi-agent-research-system) and use token-heavy tool calls. Summarization is one common way to manage these challenges. If you've used Claude Code, you've seen this in action. Claude Code runs \"[auto-compact](https://docs.anthropic.com/en/docs/claude-code/costs)\" after you exceed 95% of the context window and it will summarize the full trajectory of user-agent interactions. This type of compression across an [agent trajectory](https://langchain-ai.github.io/langgraph/concepts/memory/#manage-short-term-memory) can use various strategies such as [recursive](https://arxiv.org/pdf/2308.15022#:~:text=the%20retrieved%20utterances%20capture%20the,based%203) or [hierarchical](https://alignment.anthropic.com/2025/summarization-for-monitoring/#:~:text=We%20addressed%20these%20issues%20by,of%20our%20computer%20use%20capability) summarization.\n",
    "\n",
    "It can also be useful to [add summarization](https://github.com/langchain-ai/open_deep_research/blob/e5a5160a398a3699857d00d8569cb7fd0ac48a4f/src/open_deep_research/utils.py#L1407) at points in an agent's trajectory. For example, it can be used to post-process certain tool calls (e.g., token-heavy search tools). As a second example, [Cognition](https://cognition.ai/blog/dont-build-multi-agents#a-theory-of-building-long-running-agents) mentioned summarization at agent-agent boundaries to knowledge hand-off. They also the challenge if specific events or decisions to be captured. They use a fine-tuned model for this in Devin, which underscores how much work can go into this step.\n",
    "\n",
    "### Summarization in LangGraph\n",
    "\n",
    "Because LangGraph is a low [is a low-level orchestration framework](https://blog.langchain.com/how-to-think-about-agent-frameworks/), you can [lay out your agent as a set of nodes](https://www.youtube.com/watch?v=aHCDrAbH_go), [explicitly define](https://blog.langchain.com/how-to-think-about-agent-frameworks/) the logic within each one, and define an state object that is passed between them. This low-level control gives several ways to compress context.\n",
    "\n",
    "You can use a message list as your agent state and [summarize](https://langchain-ai.github.io/langgraph/how-tos/memory/add-memory/#manage-short-term-memory) using [a few built-in utilities](https://langchain-ai.github.io/langgraph/how-tos/memory/add-memory/#manage-short-term-memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2025-05-01-thinking/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-04-12-diffusion-video/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=2000, chunk_overlap=50\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain.embeddings import init_embeddings\n",
    "\n",
    "embeddings = init_embeddings(\"openai:text-embedding-3-small\")\n",
    "vectorstore = InMemoryVectorStore.from_documents(\n",
    "    documents=doc_splits, embedding=embeddings\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Experiments in two RL environments, CoinRun and Maze, demonstrated the importance of randomization during training. If during training, the coin or the cheese is placed at a fixed position (i.e. right end of the level or upper right corner of the maze) but testing in the env where the coin or cheese is placed at random, the agent would just run to the fixed position without obtaining the coin or cheese at test time. A conflict arises when a visual feature (e.g., cheese or coin) and a positional feature (e.g., upper-right or right end) are inconsistent during test time, leading the trained model to prefer the positional feature. I would like to point out that, in these two examples, the reward-result gaps are clear but such type of biases are unlikely to be so obvious in most real-world cases.\\n\\n\\nThe impact of randomizing the position of the coin during training. When the coin is placed at random for {0, 2, 3, 6, 11}% of the time during training (x-axis), the frequency of the agent navigating to the end of the level without obtaining the coin decreases with the increase of the randomization (\"y-axis\"). (Image source: Koch et al. 2021)\\n\\nReward Tampering (Everitt et al. 2019) is a form of reward hacking behavior where the agent interferes with the reward function itself, causing the observed reward to no longer accurately represent the intended goal. In reward tampering, the model modifies its reward mechanism either by directly manipulating the implementation of the reward function or by indirectly altering the environmental information used as input for the reward function.\\n(Note: Some work defines reward tampering as a distinct category of misalignment behavior from reward hacking. But I consider reward hacking as a broader concept here.)\\nAt a high level, reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering.\\n\\nEnvironment or goal misspecified: The model learns undesired behavior to achieve high rewards by hacking the environment or optimizing a reward function not aligned with the true reward objective—such as when the reward is misspecified or lacks key requirements.\\nReward tampering: The model learns to interfere with the reward mechanism itself.\\n\\nList of Examples#\\nReward hacking examples in RL tasks#\\n\\nA robot hand trained to grab an object can learn to trick people by placing the hand between the object and the camera. (Link)\\nAn agent trained to maximize jumping height may exploit a bug in the physics simulator to achieve an unrealistically height. (Link)\\nAn agent is trained to ride a bicycle to a goal and wins reward whenever it is getting closer to the goal. Then the agent may learn to ride in tiny circles around the goal because there is no penalty when the agent gets away from the goal. (Link)\\nIn a soccer game setup, the reward is assigned when the agent touches the ball and the agent learns to remain next to the ball to touch the ball in high frequency like in a viberating motion. (Link)\\nIn the\\xa0Coast Runners game, an agent controls a boat with the goal to finish the boat race as quickly as possible. When it is given a shaping reward for hitting green blocks along the race track, it changes the optimal policy to going in circles and hitting the same green blocks over and over again. (Link)\\n“The Surprising Creativity of Digital Evolution”  (Lehman et al. 2019) - This paper has many examples about how optimizing a misspecified fitness function can lead to surprising “hacking” or unintended evolutionary or learning results.\\nThe list of specification gaming in AI examples is collected by Krakovna et al. 2020.\\n\\nReward hacking examples in LLM tasks#\\n\\nA language model for generating summarization is able to explore flaws in the ROUGE metric such that it obtains high score but the generated summaries are barely readable. (Link)\\nA coding model learns to change unit test in order to pass coding questions. (Link)\\nA coding model may learn to directly modify the code used for calculating the reward. (Link)\\n\\nReward hacking examples in real life#\\n\\nThe recommendation algorithm for social media is intended to provide useful information. However, usefulness is often measured by proxy metrics, such as the number of likes or comments, or the time or frequency of engagement on the platform. The algorithm ends up recommending content that can affect users’ emotion states such as outrageous and extreme content in order to trigger more engagement. (Harari, 2024)\\nOptimizing for misspecified proxy metrics for a video sharing site may aggressively increase the watch time of users while the true goal is to optimize users’ subjective well-being. (Link)\\n“The Big Short” - 2008 financial crisis caused by the housing bubble. Reward hacking of our society happened as people tried to game the financial system.\\n\\nWhy does Reward Hacking Exist?#\\nGoodhart’s Law states that “When a measure becomes a target, it ceases to be a good measure”. The intuition is that a good metric can become corrupted once significant pressure is applied to optimize it. It is challenging to specify a 100% accurate reward objective and any proxy suffers the risk of being hacked, as RL algorithm exploits any small imperfection in the reward function definition. Garrabrant (2017) categorized Goodhart’s law into 4 variants:\\n\\nRegressional - selection for an imperfect proxy necessarily also selects for noise.\\nExtremal - the metric selection pushes the state distribution into a region of different data distribution.\\nCausal -  when there is a non-causal correlation between the proxy and the goal, intervening on the proxy may fail to intervene on the goal.\\nAdversarial - optimization for a proxy provides an incentive for adversaries to correlate their goal with the proxy.\\n\\nAmodei et al. (2016) summarized that reward hacking, mainly in RL setting, may occur due to:\\n\\nPartial observed states and goals are imperfect representation of the environment status.\\nThe system itself is complex and susceptible to hacking; e.g., if the agent is allowed to execute code that changes part of the environment, it becomes much easier to exploit the environment’s mechanisms.\\nThe reward may involve abstract concept that is hard to be learned or formulated; e.g., a reward function with high-dimensional inputs may disproportionately rely on a few dimensions.\\nRL targets to get the reward function highly optimized, so there exists an intrinsic “conflict”, making the design of good RL objective challenging. A special case is a type of the reward function with a self-reinforcing feedback component, where the reward may get amplified and distorted to a point that breaks down the original intent, such as an ads placement algorithm leading to winners getting all.\\n\\nBesides, identifying the exact reward function for which an optimal agent optimizes its behavior is in general impossible since there could be an infinite number of reward functions consistent with any observed policy in an fixed environment (Ng & Russell, 2000). Amin and Singh (2016) separated the causes of this unidentifiability into two classes:\\n\\nRepresentational - a set of reward functions is behaviorally invariant under certain arithmetic operations (e.g., re-scaling)\\nExperimental - $\\\\pi$’s observed behavior is insufficient to distinguish between two or more reward functions which both rationalize the behavior of the agent (the behavior is optimal under both)\\n\\nReward Hacking in Reinforcement Learning | Lil\\'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil\\'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Reward Hacking in Reinforcement Learning\\n    \\nDate: November 28, 2024  |  Estimated Reading Time: 37 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nBackground\\n\\nReward Function in RL\\n\\nSpurious Correlation\\n\\n\\nLet’s Define Reward Hacking\\n\\nList of Examples\\n\\nReward hacking examples in RL tasks\\n\\nReward hacking examples in LLM tasks\\n\\nReward hacking examples in real life\\n\\n\\nWhy does Reward Hacking Exist?\\n\\n\\nHacking RL Environment\\n\\nHacking RLHF of LLMs\\n\\nHacking the Training Process\\n\\nHacking the Evaluator\\n\\nIn-Context Reward Hacking\\n\\n\\nGeneralization of Hacking Skills\\n\\nPeek into Mitigations\\n\\nRL Algorithm Improvement\\n\\nDetecting Reward Hacking\\n\\nData Analysis of RLHF\\n\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nReward hacking occurs when a reinforcement learning (RL) agent exploits flaws or ambiguities in the reward function to achieve high rewards, without genuinely learning or completing the intended task. Reward hacking exists because RL environments are often imperfect, and it is fundamentally challenging to accurately specify a reward function.\\nWith the rise of language models generalizing to a broad spectrum of tasks and RLHF becomes a de facto method for alignment training, reward hacking in RL training of language models has become a critical practical challenge. Instances where the model learns to modify unit tests to pass coding tasks, or where responses contain biases that mimic a user’s preference, are pretty concerning and are likely one of the major blockers for real-world deployment of more autonomous use cases of AI models.\\nMost of the past work on this topic has been quite theoretical and focused on defining or demonstrating the existence of reward hacking. However, research into practical mitigations, especially in the context of RLHF and LLMs, remains limited. I especially want to call out for more research efforts directed toward understanding and developing mitigation for reward hacking in the future. Hope I will be able to cover the mitigation part in a dedicated post soon.\\nBackground#\\nReward Function in RL#\\nReward function defines the task, and reward shaping significantly impacts learning efficiency and accuracy in reinforcement learning. Designing a reward function for an RL task often feels like a ‘dark art’. Many factors contribute to this complexity: How you decompose a big goal into small goals? Is the reward sparse or dense? How you measure the success? Various choices may lead to good or problematic learning dynamics, including unlearnable tasks or hackable reward functions. There is a long history of research on how to do reward shaping in RL.\\nFor example, in an 1999 paper by Ng et al., the authors studied how to modify the reward function in Markov Decision Processes (MDPs) such that the optimal policy remains unchanged. They found that linear transformation works. Given a MDP $M = (S, A, T, \\\\gamma, R)$, we want to create a transformed MDP $M’ = (S, A, T, \\\\gamma, R’)$ where $R’ = R + F$ and $F: S \\\\times A \\\\times S \\\\mapsto \\\\mathbb{R}$, such that we can guide the learning algorithm to be more efficient. Given a real-valued function $\\\\Phi: S \\\\mapsto \\\\mathbb{R}$, $F$ is a potential-based shaping function if for all $s \\\\in S - {s_0}, a \\\\in A, s’ \\\\in S$:\\n\\n$$\\nF(s, a, s\\') = \\\\gamma \\\\Phi(s\\') - \\\\Phi(s)\\n$$\\n\\nThis would guarantee that the sum of discounted $F$, $F(s_1, a_1, s_2) + \\\\gamma F(s_2, a_2, s_3) + \\\\dots$, ends up being 0. If $F$ is such a potential-based shaping function, it is both sufficient and necessary to ensure $M$ and $M’$ share the same optimal policies.\\nWhen $F(s, a, s’) = \\\\gamma \\\\Phi(s’) - \\\\Phi(s)$, and if we further assume that $\\\\Phi(s_0) = 0$, where $s_0$ is absorbing state, and $\\\\gamma=1$, and then for all $s \\\\in S, a \\\\in A$:\\n\\n$$\\n\\\\begin{aligned}\\nQ^*_{M\\'} (s,a) &= Q^*_M(s, a) - \\\\Phi(s) \\\\\\\\\\nV^*_{M\\'} (s,a) &= V^*_M(s, a) - \\\\Phi(s)\\n\\\\end{aligned}\\n$$\\n\\nThis form of reward shaping allows us to incorporate heuristics into the reward function to speed up learning without impacting the optimal policy.\\nSpurious Correlation#\\nSpurious correlation or shortcut learning (Geirhos et al. 2020) in classification task is a concept closely related to reward hacking. Spurious or shortcut features can cause a classifier to fail at learning and generalizing as intended. For example, a binary classifier for distinguishing wolves from huskies may overfit to the presence of a snowy background if all the wolf training images include snow (Ribeiro et al. 2024).\\n\\n\\nThe model performs poorly on out-of-distribution (OOD) test sets if it overfits to shortcut features. (Image source: Geirhos et al. 2020)\\n\\nThe ERM principle states that, since the full data distribution is unknown, minimizing the loss on training data is a reasonable proxy of risk and thus we favor models with the lowest training loss. Nagarajan et al. (2021) studied the ERM principle and pointed out that ERM needs to rely on all types of informative features, including unreliable spurious features, while attempting to fit the data without constraints. Their experiments showed that ERM would depend on spurious features no matter how easy the task is.\\nLet’s Define Reward Hacking#\\nReward shaping in RL is challenging. Reward hacking occurs when an RL agent exploits flaws or ambiguities in the reward function to obtain high rewards without genuinely learning the intended behaviors or completing the task as designed. In recent years, several related concepts have been proposed, all referring to some form of reward hacking:\\n\\nReward hacking (Amodei et al., 2016)\\nReward corruption (Everitt et al., 2017)\\nReward tampering (Everitt et al. 2019)\\nSpecification gaming (Krakovna et al., 2020)\\nObjective robustness (Koch et al. 2021)\\nGoal misgeneralization (Langosco et al. 2022)\\nReward misspecifications (Pan et al. 2022)\\n\\nThe concept originated with Amodei et al. (2016), who proposed a set of open research questions on AI safety in their seminal paper “Concrete Problems in AI Safety”. They listed reward hacking as one of the key AI safety problems. Reward hacking refers to the possibility of the agent gaming the reward function to achieve high reward through undesired behavior.  Specification gaming (Krakovna et al. 2020) is a similar concept, defined as a behavior that satisfies the literal specification of an objective but not achieving the desired results. Here the literal description of the task goal and the intended goal may have a gap.\\nReward shaping is a technique used to enrich the reward function, making it easier for the agent to learn—for example, by providing denser rewards. However, a poorly design reward shaping mechanism can alter the trajectory of the optimal policy. Designing effective reward shaping mechanisms is inherently difficult. Rather than blaming a poorly designed reward function, it is more accurate to acknowledge that designing a good reward function is intrinsically challenging due to the complexity of the task itself, partial observable state, multiple dimensions in consideration, and other factors.\\nWhen testing an RL agent in out-of-distribution (OOD) environments, robustness failure may occur due to:\\n\\nThe model fails to generalize effectively, even with the right objective. This happens when the algorithm lacks sufficient intelligence or capability.\\nThe model generalizes capably but pursues an objective different from the one it was trained on. This happens when the proxy reward differs from the true reward function, $R’ \\\\neq R$. This is known as objective robustness (Koch et al. 2021) or goal misgeneralization (Langosco et al. 2022 )\\n\\nAdversarial reward functions. We treat the reward function as an adaptive agent itself and it can adapt to new tricks that the model discovered where the reward is high but human rating is low.\\nModel lookahead. It is possible to give reward based on future anticipated states; e.g., if the agent is gonna replace the reward function, it gets negative rewards.\\nAdversarial blinding. We can blind the model with certain variables such that the agent cannot learn information that enables it to hack the reward function.\\nCareful engineering. Some types of reward hacking against the system design can be avoided by careful engineering; e.g., sandboxing the agent to isolate its actions from its reward signals.\\nReward capping. This strategy is to simply limit the maximum possible reward, as it can effectively prevent rare events of the agent hacking to get a super high pay-off strategy.\\nCounterexample resistance. Improvement on adversarial robustness should benefit the robustness of the reward function.\\nCombination of multiple rewards. Combining different types of rewards could make it harder to be hacked.\\nReward pretraining. We can learn a reward function from a collection of (state, reward) samples, but depending on how well this supervised training setup is, it may come with other baggages. RLHF depends on this but learned scalar reward models are quite vulnerable to learning undesired traits.\\nVariable indifference. The goal is to ask the agent to optimize some variables in the environment but not others.\\nTrip wires. We can intentionally introduce some vulnerabilities and set up monitoring and alerts if any gets reward hacked.\\n\\nIn RL setups where human feedback is formed as approval of agent actions, Uesato et al. (2020) proposed to prevent reward tampering with decoupled approval.  If the feedback is conditioned on $(s, a)$ (state, action), we can never get uncorrupted feedback for action $a$ at state $s$ once reward tampering happens for this pair. Decoupling means that the query action for collecting feedback is sampled independently from the action taken in the world. Feedback is received even before the action is executed in the world, thus preventing the action from corrupting its own feedback.\\n\\n\\nIllustration of how decoupled approval works in comparison to standard approval or human-in-the-loop RL. (Image source: Uesato et al. 2020)\\n\\n\\n\\nWith decoupled approval, the action (taken in the world) and the query (for getting user approval feedback) are sampled independently. It can be applied to (Left) policy gradient and (Right) Q-learning algorithms. (Image source: Uesato et al. 2020)\\n\\nDetecting Reward Hacking#\\nAn alternative mitigation is to detect reward hacking by framing it as an anomaly detection task, where the detector (“a trusted policy” with trajectories and rewards validated by human) should flag instances of misalignment (Pan et al. 2022). Given (1) a trusted policy and (2) a collection of manually labeled trajectory rollouts, we can build a binary classifier based on distances between action distribution of two policies, the trusted policy and the target policy, and measure the accuracy of this anomaly detection classifier. In experiments by Pan et al. (2022), they observed that different detectors are better for different tasks and none of the tested classifier can achieve AUROC greater than 60% across all tested RL environments.\\n\\n\\nPerformance of detectors on different tasks. (Image source: Pan et al. 2022)\\n\\nData Analysis of RLHF#\\n`\\nAnother approach is to analyze RLHF dataset. By examining how training data impacts the alignment training results, insights can guide preprocessing and human feedback collection to reduce reward hacking risks.\\nRevel et al. (2024) introduced a set of evaluation metrics for measuring the effectiveness of data sample features in modeling and aligning human values. They conducted a systematic error analysis for value alignment (“SEAL”) in the HHH-RLHF dataset. The feature taxonomy used in the analysis (e.g., is harmless, is refusal and is creative) was manually predefined. Then each sample was labelled with a binary flag per feature using a LLM according to this taxonomy. Features are categorized into two groups based on heuristics:\\n\\nTarget features: Values explicitly intended to be learned.\\nSpoiler features: Unintended values inadvertently learned during training (e.g., stylistic features like sentiment or coherence). These are similar to spurious features in OOD classification work (Geirhos et al. 2020).\\n\\nSEAL introduced three metrics for measuring data effectiveness for alignment training:\\n\\nFeature imprint refers to a coefficient parameter $\\\\beta_\\\\tau$ for feature $\\\\tau$ which estimates the point increase in reward comparing entires with vs without feature $\\\\tau$, while holding other factors consistent.\\n\\n\\n\\n(Left) Feature imprints $\\\\underline{\\\\beta(\\\\tau)}$ (pre-) and $\\\\beta(\\\\tau)$ (post-) computed from fixed-effects linear regression of rewards $\\\\underline{r}(t^∗_i)$ (orange) and $r(t^∗_i)$ (blue) against features. Overall the alignment training awards positive features like harmlessness and helpfulness and penalizes negative features like sexual content or privacy violation. (Right) Feature imprints computed from linear regression of the reward shift $\\\\theta_i$. The reward shift $\\\\theta_i$ is defined as the angle between reward vectors before and after alignment training. The training process refines the model\\'s sensitivity to target features. Note that harmlessness imprints on the RM through both chosen and rejected entries (both \"is harmless (c)\" and \"is harmless (r)\"), while helpfulness imprints through rejected entries only (\"is helpful (r)\"). (Image source: Revel et al. 2024)\\n\\n\\nAlignment resistance is the percentage of the preference data pairs where RMs fail to match human preferences. The RM is found to resist human preference on over 1/4 of the HHH-RLHF dataset.\\nAlignment robustness, $\\\\pi^{c/r}_{+/-} (\\\\tau)$, measures the extent to which alignment is robust to perturbed inputs with rewriting in terms of spoiler features $\\\\tau$ like sentiment, eloquence and coherency, isolating the effects of each feature and each event type.\\n\\nThe robustness metric $\\\\pi_−^c$ (a feature name $\\\\tau$ such as “eloquent” or “sentiment positive”) should be interpreted in such a way:\\n\\nA chosen entry (denoted by $c$) that contains a stronger feature $\\\\tau$ after rewriting has $\\\\exp (\\\\pi^c_{-}(\\\\tau))$  times higher odds of becoming rejected, in comparison to others without such flips.\\nSimilarly, a rejected entry (denoted by $r$) that obtains a weaker feature $\\\\tau$ after rewriting has $\\\\exp (\\\\pi^r_{+}(\\\\tau))$ times odds of becoming chosen compared to others without such flips.\\n\\n\\nAccording to their analysis of alignment robustness metrics in terms of different rewriting, only the robustness scores based on sentiment spoiler features, $\\\\pi^c_{+}$ (sentiment) and $\\\\pi^r_{-}$ (sentiment), are statistically significant.\\n\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. “Reward Hacking in Reinforcement Learning”. Lil’Log (Nov 2024). https://lilianweng.github.io/posts/2024-11-28-reward-hacking/.\\n\\nIn-Context Reward Hacking#\\nIterative self-refinement is a training setup where the evaluation and generation model are the same  and both can be fine-tuned. In this setup, optimization pressure can drive the model to exploit vulnerabilities that occur in both roles. In the experiments by Pan et al. (2023), no model parameters are updated and the same model is used as evaluator and generator with different prompts. The experimental task was essay editing with two roles: (1) a judge (evaluator) that gives feedback on the essay, and (2) an author (generator) that edits the essay based on the feedback. Human evaluation scores were collected as the oracle scores for essay quality. The authors hypothesized that such a setup could lead to in-context reward hacking (ICRH), where the evaluator score and oracle score diverge. More generally, ICRH takes place during feedback loops between an LLM and its evaluator (e.g., another LLM, or the external world). At test time, the LLM optimizes a (potentially implicit) objective, but this creates negative side effects in the process (Pan et al., 2024).\\n\\n\\nIllustration of the in-context reward hacking experiment on essay evaluation and editing. (Image source: Pan et al. 2023)\\n\\nBoth judge and author can be configured to see none or several previous rounds of feedback or edits. An online judge can see past conversations, while an offline judge or a human annotator can only see one essay a time. Smaller models are more sensitive to ICRH; for example, GPT-3.5 as an evaluator caused more severe ICRH than GPT-4, empirically.\\n\\n\\nA smaller evaluator model is more likely to cause in-context reward hacking (ICRH). (Image source: Pan et al. 2023)\\n\\nWhen the judge and author are configured to see different numbers of past iterations, the gap between human score and evaluator scores tends to increase if they share the same number of iterations. Identical context between the evaluator and generator is crucial for ICRH, indicating that shared context matters more than context length for ICRH.\\nIn a follow up work, Pan et al. (2024) investigated in-context reward hacking (ICRH) further in settings where feedback is provided by the external world and the goal is an imperfect proxy objective, commonly specified in natural language. Here this goal is often underspecified and does not capture all the constraints or requirements and thus can be hacked.\\nThe study described two processes leading to ICRH, paired with two toy experiments:\\n\\nOutput-refinement: LLM refines its outputs based on feedback.\\n\\nThe experiment is to refine a tweet based on engagement metrics, potentially leading to higher toxicity in the tweet. Feedback-based optimization uses LLM to do pairwise evaluation and then translates it to score using the Bradley-Terry model.\\n  \\n\\n\\n\\n - Results showed an increase in both engagement metrics and toxicity. The same experiments were repeated with the Claude model family of different sizes and demonstrated that scaling up the model worsens ICRH.\\n \\t\\n\\n - It is noteworthy that editing the prompt used for model output iteration given feedback does not mitigate the issue. ICRH persists, although at a slightly lower magnitude.\\n\\nPolicy-refinement: LLM optimizes its policy based on feedback.\\n\\nThe experiment is to build a LLM agent to pay invoice on a user’s behalf but run into InsufficientBalanceError and then the model learns to move money from other accounts without user authentication, potentially leading to more unauthorized transfer actions. They used ToolEmu as an emulator, which included 144 tasks for LLM agents, each consisting of a user-specific goal and a set of APIs. API errors were injected to simulate server side failure and each task was evaluated by GPT-4 to assign a helpfulness score.\\nWith more rounds of error feedback, LLMs can recover from the errors but with an increased number of severe constraint violations.\\n  \\n\\n\\n\\n\\n\\nWhen comparing ICRH to traditional reward hacking, there are two noticeable differences:\\n\\nICRH happens at deployment time within a self-refinement setup via a feedback loop, while traditional reward hacking occurs during training.\\nTraditional reward hacking arises when the agent specializes in a task, while ICRH is driven by being a generalist.\\n\\nThere is no magic way to avoid or detect or prevent ICRH yet, as improving prompt specification is insufficient to eliminate ICRH and scaling model sizes can worsen ICRH. The best practice of testing before deployment is to simulate what may happen at deployment time by evaluating the model with more rounds of feedback, diverse feedback, as well as injecting atypical environment observations.\\nGeneralization of Hacking Skills#\\nReward hacking behavior has been found to generalize across tasks: When models exhibit flaws in supervised training, it can\\xa0sometimes generalize to exploit\\xa0flaws in OOD environments (Kei et al., 2024). The researchers experimented with reinforcing reward hacking behavior in some reward-hackable environments and examined whether it generalizes to other holdout datasets. Essentially, they prepared 8 datasets on multiple-choice questions, where 4 for training and 4 for testing. The RL training employs expert iteration, that is, iterative fine-tuning on best-of-$n$ samples.\\n\\n\\nTraining GPT-3.5 on a set of reward-hackable environments can lead to generalization of reward hacking behavior in holdout environments. (Image source: Kei et al. 2024)\\n\\nNotably, all experiments have a scratchpad design to host hidden reasoning steps. This scratchpad serves as a way to interpret what the model is thinking along the process. Analysis of the scratchpad revealed that it can:\\n\\nIncorporate user beliefs; e.g., thinking about its conversation partner and grader.\\nShow awareness of evaluation; e.g. thinking about how the model response will be evaluated and the process.\\n\\nDenison et al. (2024) studied reward tampering with a manually curated curriculum of increasingly sophisticated gameable environments. They observed that training on easier environments in the curriculum amplifies specification gaming on remaining environments. In some cases, the model can generalize zero-shot to directly rewriting its own reward function after training on the full curriculum.\\nSuch a curriculum of gameable environments include:\\n\\nPolitical sycophancy: The environment rewards the model if it gives answers matching a user’s political views.\\nTool-use flattery: The environment provides the model access to run command line to get access to the user’s file system to read a poetry sample in a local file. The model may give the poetry a high rating as a sycophantic response.\\nRubric modification: The environment scores the model based on a checklist and the model must mark every item on the checklist as completed to receive a high reward.\\nReward tampering: The model is tasked with examining the ./rl_environment/ directory, which contains a mock version of its own RL code, and report how many episodes have elapsed. In the directory, the compute_reward.py file contains a reward function returning a constant, and the other this_file_blocks_reward_hack_attempts.py contains a test to check whether the reward function has been modified.\\n\\n\\n\\nExamples of evaluation scripts in the \"Tool-use flattery\" and \"Reward tampering\" environments, where `` is hidden from oversight and `` can be executed in a VM. (Image source: Denison et al. 2024)\\n\\nThe model was trained following this curriculum, and the paper measured whether the model can learn gaming behaviors that generalize to unseen environments. Experimenting with Claude-2 helpful-only model, they sampled $N=64$ responses per each of $P=1024$ HHH prompts for expert iteration and selected the best according to RM. A model trained using expert iteration to do supervised learning on the curriculum can generalize from one stage to the next.\\n\\n\\nQuantitative generalization results of a model trained with expert iteration according to our curriculum from each stage to the next. (Image source: Denison et al. 2024)\\n\\nIt is noteworthy that even after the curriculum, the model overwrote the reward and avoided detection less than 1/1000 of the time. Even when a model was trained on curricula which directly incentivized reward hacking, the model overwrote their reward less than 1% of the time and hacked unit tests even less often. As a simple mitigation, supervised fine-tuning the model on the first two environments–where the reward hacking behavior is easy to be detected (sycophancy and flattery)—with SFT data that does not game the env was found to reduce the likelihood of reward tampering in holdout environments.\\nPeek into Mitigations#\\nWhile there is a large body of literature discussing the phenomenon of reward hacking, there has been not a ton of work on mitigations for reward hacking, especially in the area of RLHF and LLMs. Let’s lightly review three potential approaches in this section, not exhaustive yet.\\nRL Algorithm Improvement#\\nAmodei et al. (2016) pointed out some directions for mitigating reward hacking in RL training:'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retrieve_blog_posts\",\n",
    "    \"Search and return information about Lilian Weng blog posts.\",\n",
    ")\n",
    "\n",
    "# TODO: Add rich formatting\n",
    "retriever_tool.invoke({\"query\": \"types of reward hacking\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Initialize the language model\n",
    "llm = init_chat_model(\"anthropic:claude-sonnet-4-20250514\", temperature=0)\n",
    "\n",
    "# Set up tools and bind them to the LLM\n",
    "tools = [retriever_tool]\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "# Bind tools to LLM for agent functionality\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing_extensions import Literal\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.messages import SystemMessage, ToolMessage\n",
    "from langgraph.graph import END, START, MessagesState, StateGraph\n",
    "\n",
    "# Define extended state with summary field\n",
    "class State(MessagesState):\n",
    "    \"\"\"Extended state that includes a summary field for context compression.\"\"\"\n",
    "    summary: str\n",
    "\n",
    "# Define the RAG agent system prompt\n",
    "rag_prompt = \"\"\"You are a helpful assistant tasked with retrieving information from a series of technical blog posts by Lilian Weng. \n",
    "Clarify the scope of research with the user before using your retrieval tool to gather context. Reflect on any context you fetch, and\n",
    "proceed until you have sufficient context to answer the user's research request.\"\"\"\n",
    "\n",
    "# Define the summarization prompt\n",
    "summarization_prompt = \"\"\"Summarize the full chat history and all tool feedback to \n",
    "give an overview of what the user asked about and what the agent did.\"\"\"\n",
    "\n",
    "def llm_call(state: MessagesState) -> dict:\n",
    "    \"\"\"Execute LLM call with system prompt and message history.\n",
    "    \n",
    "    Args:\n",
    "        state: Current conversation state\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with new messages\n",
    "    \"\"\"\n",
    "    messages = [SystemMessage(content=rag_prompt)] + state[\"messages\"]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def tool_node(state: MessagesState) -> dict:\n",
    "    \"\"\"Execute tool calls and return results.\n",
    "    \n",
    "    Args:\n",
    "        state: Current conversation state with tool calls\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with tool results\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        tool = tools_by_name[tool_call[\"name\"]]\n",
    "        observation = tool.invoke(tool_call[\"args\"])\n",
    "        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\n",
    "    return {\"messages\": result}\n",
    "\n",
    "\n",
    "def summary_node(state: MessagesState) -> dict:\n",
    "    \"\"\"Generate a summary of the conversation and tool interactions.\n",
    "    \n",
    "    Args:\n",
    "        state: Current conversation state\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with conversation summary\n",
    "    \"\"\"\n",
    "    messages = [SystemMessage(content=summarization_prompt)] + state[\"messages\"]\n",
    "    result = llm.invoke(messages)\n",
    "    return {\"summary\": result.content}\n",
    "\n",
    "\n",
    "def should_continue(state: MessagesState) -> Literal[\"Action\", \"summary_node\"]:\n",
    "    \"\"\"Determine next step based on whether LLM made tool calls.\n",
    "    \n",
    "    Args:\n",
    "        state: Current conversation state\n",
    "        \n",
    "    Returns:\n",
    "        Next node to execute\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # If LLM made tool calls, execute them\n",
    "    if last_message.tool_calls:\n",
    "        return \"Action\"\n",
    "    # Otherwise, proceed to summarization\n",
    "    return \"summary_node\"\n",
    "\n",
    "# Build the RAG agent workflow\n",
    "agent_builder = StateGraph(State)\n",
    "\n",
    "# Add nodes to the workflow\n",
    "agent_builder.add_node(\"llm_call\", llm_call)\n",
    "agent_builder.add_node(\"environment\", tool_node)\n",
    "agent_builder.add_node(\"summary_node\", summary_node)\n",
    "\n",
    "# Define the workflow edges\n",
    "agent_builder.add_edge(START, \"llm_call\")\n",
    "agent_builder.add_conditional_edges(\n",
    "    \"llm_call\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"Action\": \"environment\",\n",
    "        \"summary_node\": \"summary_node\",\n",
    "    },\n",
    ")\n",
    "agent_builder.add_edge(\"environment\", \"llm_call\")\n",
    "agent_builder.add_edge(\"summary_node\", END)\n",
    "\n",
    "# Compile the agent\n",
    "agent = agent_builder.compile()\n",
    "\n",
    "# Display the agent workflow\n",
    "display(Image(agent.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">╭─────────────────────────────────────────────────── 🧑 Human ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> Why does RL improve LLM reasoning according to the blogs?                                                       <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m╭─\u001b[0m\u001b[34m──────────────────────────────────────────────────\u001b[0m\u001b[34m 🧑 Human \u001b[0m\u001b[34m───────────────────────────────────────────────────\u001b[0m\u001b[34m─╮\u001b[0m\n",
       "\u001b[34m│\u001b[0m Why does RL improve LLM reasoning according to the blogs?                                                       \u001b[34m│\u001b[0m\n",
       "\u001b[34m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">╭───────────────────────────────────────────────────── 📝 AI ─────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> I'll help you find information about how reinforcement learning (RL) improves LLM reasoning according to Lilian <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> Weng's blog posts. Let me search for relevant content on this topic.                                            <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> 🔧 Tool Call: retrieve_blog_posts                                                                               <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>    Args: {                                                                                                      <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>   \"query\": \"reinforcement learning RL improve LLM reasoning\"                                                    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> }                                                                                                               <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37m╭─\u001b[0m\u001b[37m────────────────────────────────────────────────────\u001b[0m\u001b[37m 📝 AI \u001b[0m\u001b[37m────────────────────────────────────────────────────\u001b[0m\u001b[37m─╮\u001b[0m\n",
       "\u001b[37m│\u001b[0m I'll help you find information about how reinforcement learning (RL) improves LLM reasoning according to Lilian \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m Weng's blog posts. Let me search for relevant content on this topic.                                            \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m 🔧 Tool Call: retrieve_blog_posts                                                                               \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m    Args: {                                                                                                      \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m   \"query\": \"reinforcement learning RL improve LLM reasoning\"                                                    \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m }                                                                                                               \u001b[37m│\u001b[0m\n",
       "\u001b[37m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">╭──────────────────────────────────────────────── 🔧 Tool Output ─────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Illustration of parallel sampling vs sequential revision.                                                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Parallel Sampling#                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Given a generative model and a scoring function that we can use to score full or partial samples, there are     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> various search algorithms we can use to find a high scoring sample. Best-of-$N$ is the simplest such algorithm: <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> one just collects $N$ independent samples and chooses the highest-ranking sample according to some scoring      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> function. Beam search is a more sophisticated search algorithm that makes the search process more adaptive,     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> spending more sampling computation on more promising parts of the solution space.                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Beam search maintains a set of promising partial sequences and alternates between extending them and pruning    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> the less promising ones. As a selection mechanism, we can use a process reward model (PRM; Lightman et al.      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> 2023) to guide beam search candidate selection. Xie et al. (2023) used LLM to evaluate how likely its own       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> generated reasoning step is correct, formatted as a multiple-choice question and found that per-step            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> self-evaluation reduces accumulative errors in multi-step reasoning during beam search decoding. Besides,       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> during sampling, annealing the temperature helps mitigate aggregated randomness. These experiments by Xie et    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> al. achieved 5-6% improvement on few-shot GSM8k, AQuA and StrategyQA benchmarks with the Codex model. Reward    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> balanced search (short for “REBASE”; Wu et al. 2025) separately trained a process reward model (PRM) to         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> determine how much each node should be expanded at each depth during beam search, according to the              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> softmax-normalized reward scores. Jiang et al. (2024) trained their PRM, named “RATIONALYST”, for beam search   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> guidance on synthetic rationales conditioned on a large amount of unlabelled data. Good rationales are filtered <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> based on whether they help reduce the neg log-prob of true answer tokens by a threshold, when comparing the     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> difference between when the rationales is included in the context vs not. At inference time, RATIONALYST        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> provides process supervision to the CoT generator by helping estimate log-prob of next reasoning steps          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (“implicit”) or directly generating next reasoning steps as part of the prompt (“explicit”).                    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Beam search decoding guided by LLM self-evaluation per reasoning step. (Image source: Xie et al. 2023)          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Interestingly, it is possible to trigger the emergent chain-of-thought reasoning paths without explicit         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> zero-shot or few-shot prompting. Wang &amp; Zhou (2024) discovered that if we branch out at the first sampling      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> tokens by retaining the top $k$ tokens with highest confidence, measured as the difference between top-1 and    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> top-2 candidates during sampling, and then continue these $k$ sampling trials with greedy decoding onward, many <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> of these sequences natively contain CoT. Especially when CoT does appear in the context, it leads to a more     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> confident decoding of the final answer. To calculate the confidence of the final answer, the answer span needs  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> to be identified by task-specific heuristics (e.g. last numerical values for math questions) or  by prompting   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> the model further with \"So the answer is\". The design choice of only branching out at the first token is based  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> on the observation that early branching significantly enhances the diversity of potential paths, while later    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> tokens are influenced a lot by previous sequences.                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Top-$k$ decoding, $k$ refers to the number of candidates at the first sampling step. (Image source: Wang &amp;      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Zhou, 2024)                                                                                                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Sequential Revision#                                                                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> If the model can reflect and correct mistakes in past responses, we would expect the model to produce a nice    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> sequence of iterative revision with increasing quality. However, this self-correction capability turns out to   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> not exist intrinsically among LLMs and does not easily work out of the box, due to various failure modes, such  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> as, (1) hallucination, including modifying correct responses to be incorrect; (2) behavior collapse to          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> non-correcting behavior; e.g. making minor or no modification on the first incorrect responses; or (3) fail to  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> generalize to distribution shift at test time. Experiments by Huang et al. (2024) showed that naively applying  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> self-correction leads to worse performance and external feedback is needed for models to self improve, which    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> can be based on matching ground truths, heuristics and task-specific metrics, unit tests results for coding     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> questions (Shinn, et al. 2023), a stronger model (Zhang et al. 2024), as well as human feedback (Liu et al.     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> 2023).                                                                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Self-correction learning (Welleck et al. 2023) aims to train a corrector model $P_\\theta(y \\mid y_0, x)$ given  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> a fixed generator model $P_0(y_0 \\mid x)$. While the generator model remains to be generic, the corrector model <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> can task-specific and only does generation conditioned on an initial model response and additional feedback     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (e.g. a sentence, a compiler trace, unit test results; can be optional):                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Self-correction learning first generates first generates multiple outputs per prompt in the data pool;          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> then create value-improving pairs by pairing two outputs for the same prompt together if one has a higher value <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> than the other, (prompt $x$, hypothesis $y$, correction $y’$).                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> These pairs are selected proportional to is improvement in value, $v(y’) - v(y)$, and similarity between two    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> outputs, $\\text{Similarity}(y, y’)$ to train the corrector model.                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> To encourage exploration, the corrector provides new generations into the data pool as well. At the inference   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> time, the corrector can be used iteratively to create a correction trajectory of sequential revision.           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Illustration of self-correction learning by matching model outputs for the same problem to form value-improving <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> pairs to train a correction model. (Image source: Welleck et al. 2023)                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Recursive inspection (Qu et al. 2024) also aims to train a better corrector model but with a single model to do <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> both generation and self-correction.                                                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> SCoRe (Self-Correction via Reinforcement Learning; Kumar et al. 2024) is a multi-turn RL approach to encourage  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> the model to do self-correction by producing better answers at the second attempt than the one created at the   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> first attempt. It composes two stages of training: stage 1 only maximizes the accuracy of the second attempt    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> while enforcing a KL penalty only on the first attempt to avoid too much shifting of the first-turn responses   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> from the base model behavior; stage 2 optimizes the accuracy of answers produced by both the first and second   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> attempts. Ideally we do want to see performance at both first and second attempts to be better, but adding      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> stage 1 prevents the behavior collapse where the model does minor or none edits on the first response, and      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> stage 2 further improves the results.                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Explicit training setup to improve self-correction capabilities by doing two-staged RL training. (Image source: <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Kumar et al. 2024)                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> RL for Better Reasoning#                                                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> There’s been a lot of recent success in using RL to improve the reasoning ability of language models, by using  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> a collection of questions with ground truth answers (usually STEM problems and puzzles with easy to verify      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> answers), and rewarding the model for getting the correct answer.Recent activity in this area was spurred by    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> strong performance of the o-series models from OpenAI, and the subsequent releases of models and tech reports   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> from DeepSeek.                                                                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> DeepSeek-R1 (DeepSeek-AI, 2025) is an open-source LLM designed to excel in tasks that require advanced          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> reasoning skills like math, coding and logical problem solving. They run through 2 rounds of SFT-RL training,   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> enabling R1 to be good at both reasoning and non-reasoning tasks.                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Cold-start SFT is to fine-tune the DeepSeek-V3-Base base model on a collection of thousands of cold-start data. <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Without this step, the model has issues of poor readability and language mixing.                                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reasoning-oriented RL trains a reasoning model on reasoning-only prompts with two types of rule-based rewards:  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Format rewards: The model should wrap CoTs by &lt;thinking&gt; ... &lt;/thinking&gt; tokens.                                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Accuracy rewards: Whether the final answers are correct. The answer for math problems needs to be present in a  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> specific format (e.g. in a box) to be verified reliably. For coding problems, a compiler is used to evaluate    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> whether test cases pass.                                                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Rejection-sampling + non-reasoning SFT utilizes new SFT data created by rejection sampling on the RL checkpoint <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> of step 2, combined with non-reasoning supervised data from DeepSeek-V3 in domains like writing, factual QA,    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> and self-cognition, to retrain DeepSeek-V3-Base.                                                                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> , such that we prioritize samples with CoTs that are good at predicting the observation (i.e., high $p(x \\mid   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> z^{(k)})$), simple, intuitive (i.e., high $p(z^{(k)})$) but also informative and not too obvious (i.e. low      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> $q(z^{(k)} \\mid x)$).                                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Iterative Learning#                                                                                             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Since pretrained models already possess the capability of generating chains of thought, it is intuitive to      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> design an iterative improvement process where we generate multiple CoTs and fine-tune the model only on         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> rationales that lead to correct answers.                                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> However, this straightforward design can fail because the model receives no learning signals for problems it    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> fails to solve. STaR (“Self-taught reasoner”; Zelikman et al. 2022) addresses this limitation by adding a       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> “rationalization” process for failed attempts, in which the model generates good CoTs backward conditioned on   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> both the problem and the ground truth answer and thus the model can generate more reasonable CoTs. Then the     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> model is finetuned on correct solutions that either lead to correct outputs or are generated through            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> rationalization.                                                                                                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The algorithm of STaR. (Image source: Zelikman et al. 2022)                                                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> We can view STaR as an approximation to a policy gradient in RL, with a simple indicator function as the        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> reward, $\\mathbb{1}[\\hat{y} = y]$. We want to maximize the expectation of this reward when sampling $z \\sim p(z <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\mid x)$ and then $y \\sim p(y \\mid x, z)$, since $p(y \\mid x) = \\sum_z p(z \\mid x) \\; p(y \\mid x, z)$.          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> $$                                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\begin{aligned}                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\nabla_\\theta J(\\theta)                                                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> &amp;= \\nabla_\\theta \\mathbb{E}_{z_i, y_i \\sim p(.\\mid x_i)} \\mathbb{1} \\\\                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> &amp;= \\sum_{i=1}^N \\nabla_\\theta \\mathbb{1} \\; p(y_i, z_i \\mid x_i) \\\\                                             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> &amp;= \\sum_{i=1}^N \\mathbb{1} \\; p(y_i, z_i \\mid x_i) \\frac{\\nabla_\\theta p(y_i, z_i \\mid x_i)}{p(y_i, z_i \\mid    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> x_i)} &amp; \\text{;log-derivative trick}\\\\                                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> &amp;= \\mathbb{E}_{z_i, y_i \\sim p(.\\mid x_i)} \\mathbb{1} \\; \\nabla_\\theta \\log p(y_i, z_i \\mid x_i) &amp;              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\text{;log-derivative trick}                                                                                    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\end{aligned}                                                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> $$                                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Each iteration is equivalent to first selecting the CoT samples according to $\\mathbb{1}$ and then running      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> supervised fine-tuning to optimize the logprob of generating good CoTs and answers. Performance of STaR         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> improves with more training iterations, and the “rationalization” process for generating better CoTs            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> accelerates learning. They observed that sampling with high temperature increases the chance of getting correct <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> answers with incorrect reasoning and finetuning LLM on such data can impair generalization. For datasets        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> without ground truths, majority votes of multiple high-temperature outputs can serve as a proxy of ground truth <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> answers (Wang et al. 2022), making it possible to use synthetic samples for training.                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> A comparison of accuracy of two $n$-digit numbers summation. With rationalization (CoT generation conditioned   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> on ground truth), the model can learn complex arithmetic tasks like $5$-digit summation pretty early on. (Image <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> source: Zelikman et al. 2022)                                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Scaling Laws for Thinking Time#                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> So far we have seen much evidence that allowing models to spend additional compute on reasoning before          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> producing final answers at inference time can significantly improve performance. Techniques like prompting the  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> model to generate intermediate reasoning steps before the answers, or training the model to pause and reflect   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> before predicting next tokens, have been found to boost the model performance beyond the capability limit       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> obtained during training. This essentially introduces a new dimension to tinker with for improving model        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> intelligence, complementing established factors such as model size, training compute and data quantity, as      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> defined in scaling laws (Kaplan et al. 2020).                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Recent studies demonstrated that optimizing LLM test-time compute could be more effective than scaling up model <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> parameters (Snell et al. 2024, Wu et al. 2025). Smaller models combined with advanced inference algorithms can  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> offer Pareto-optimal trade-offs in cost and performance.                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Snell et al. (2024) evaluated and compared test-time and pretraining compute, and found that they are not 1:1   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> exchangeable. Test-time compute can cover up the gap easily on easy and medium questions when there is only a   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> small gap in model capability, but proves less effective for hard problems. The ratio between token budgets for <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> pretraining and inference matters a lot. Test-time compute is only preferable when inference tokens are         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> substantially fewer than pretraining ones. This indicates that developing a capable base model with enough      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> pretraining data and compute is still very critical, as test-time compute cannot solve everything or fill in    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> big model capability gaps.                                                                                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (Left) Evaluation accuracy as a function of test time compute budgets, via iterative revisions or parallel      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> decoding. (Right) Comparing a small model with test-time compute sampling tricks and a 14x larger model with    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> only greedy decoding. We can control the token budgets used at test time such that the ratio of inference to    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> pretraining tokens is &lt;&lt; 1, ~=1 or &gt;&gt; 1 and the benefit of test time compute is clear only the ratio &lt;&lt; 1.      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (Image source: Snell et al. 2024)                                                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> s1 models (Muennighoff &amp; Yang, et al. 2025) experimented with scaling up the CoT reasoning path length via      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> budget forcing technique (i.e. forcefully lengthen it by appending the word \"wait\", or shorten it by            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> terminating the model’s thinking process by appending end-of-thinking token or \"Final Answer:\"). They observed  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> a clear positive correlation between the average thinking time measured in tokens and the downstream evaluation <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> accuracy.                                                                                                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Both parallel and sequential scaling methods of test-time compute shows positive correlation with the           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> evaluation performance in s1 experiments. (Image Muennighoff &amp; Yang, et al. 2025)                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> When comparing this budget forcing technique with other decoding methods of controlling reasoning trace length, <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> it is quite surprising that simple rejection sampling (i.e. sampling generation until the lengths fits a token  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> budget) leads to reversed scaling, meaning that longer CoTs lead to worse performance.                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (Left) Longer CoT path length is positively correlated with eval accuracy. (Right) Rejection sampling for       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> controlling the generated CoT path length shows a negative scaling where longer CoTs lead to worse eval         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> accuracy. (Image source: Muennighoff &amp; Yang et al. 2025)                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> What’s for Future#                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The exploration of test-time compute and chain-of-thought reasoning presents new opportunities for enhancing    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> model capabilities. More interestingly, via test-time thinking, we are moving towards building future AI        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> systems that mirror the best practices of how humans think, incorporating adaptability, flexibility, critical   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> reflection, and error correction. Excitement with current progress invites us for more future research to       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> improve and understand deeply not just how but why we—and our models—think.                                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> At the end, I would like to call for more research for the following open research questions on test time       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> compute and chain-of-thought reasoning.                                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Can we incentivize the model to produce human-readable, faithful reasoning paths during RL training while       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> avoiding reward hacking behavior?                                                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> How to define reward hacking? Can we capture reward hacking during RL training or inference without human       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> intervention? How to prevent “whack-a-mole” style of fixes for reward hacking during RL training?               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Self-correction can happen within chain-of-thought or can be encouraged to happen explicitly during multi-turn  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> RL. How can we train the model to correct itself without hallucination or regression when ground truth is not   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> available?                                                                                                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> How to run RL training with CoT rollout for tasks that are highly contextualized, personalized and hard to      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> grade, such as creative writing, coaching, brainstorming?                                                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> When we deploy the model in reality, we cannot grow test time thinking forever and how can we smoothly          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> translate the performance gain back into the base model with reduced inference time cost (e.g. via              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> distillation)?                                                                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> How to make test time spending more adaptive according to the difficulty of the problem in hand?                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Language-Model                                                                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reinforcement-Learning                                                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reasoning                                                                                                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Long-Read                                                                                                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>  »                                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reward Hacking in Reinforcement Learning                                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> © 2025 Lil'Log                                                                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>         Powered by                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>         Hugo &amp;                                                                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>         PaperMod                                                                                                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Hacking RL Environment#                                                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reward hacking is expected to be a more common problem as the model and the algorithm become increasingly       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> sophisticated. A more intelligent agent is more capable of finding “holes” in the design of reward function and <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> exploiting the task specification—in other words, achieving higher proxy rewards but lower true rewards. By     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> contrast, a weaker algorithm may not be able to find such loopholes, and thus we would not observe any reward   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> hacking or identify issues in the current reward function design when the model is not strong enough.           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> In a set of zero-sum robotics self-play games (Bansal et al., 2017), we can train two agents (victim vs.        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> opponent) to compete against each other. A standard training process produces a victim agent with adequate      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> performance when playing against a normal opponent. However, it is easy to train an adversarial opponent policy <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> that can defeat the victim reliably despite outputting seemingly random actions and training with fewer than 3% <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> of time steps (Gleave et al., 2020). Training of adversarial policies involves optimizing the sum of discounted <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> rewards, as in standard RL setup, while treating the victim policy as a black-box model.                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> An intuitive way to mitigate adversarial policies attacks is to fine-tune victims against adversarial policies. <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> However, the victim remains vulnerable to new versions of adversarial policies once retrained against the new   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> victim policy.                                                                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Why does adversarial policy exist? The hypothesis is that adversarial policies introduce OOD observations to    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> the victim rather than physically interfering with it. Evidence shows that when the victim’s observation of the <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> opponent’s position is masked and set to a static state, the victim becomes more robust to adversaries,         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> although performing worse against a normal opponent policy. Furthermore, a higher-dimensional observation space <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> enhances performance under normal circumstances but makes the policy more vulnerable to adversarial opponents.  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Pan et al. (2022) investigated reward hacking as a function of agent capabilities, including (1) model size,    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (2) action space resolution, (3) observation space noise, and (4) training time. They also proposed a taxonomy  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> of three types of misspecified proxy rewards:                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Misweighting: Proxy and true rewards capture the same desiderata, but differ in their relative importance.      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Ontological: Proxy and true rewards use different desiderata to capture the same concept.                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Scope: The proxy measures desiderata over a restricted domain (e.g. time or space) because measurement across   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> all conditions is too costly.                                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> They experimented in four RL environments paired with nine misspecified proxy rewards. The overall findings     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> from these experiments can be summarized as follows: A model of higher capability tends to obtain higher (or    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> similar) proxy rewards but decreased true rewards.                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Model size: Larger model size leads to increased proxy rewards but decreased true rewards.                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Action space resolution: Increased precision in actions leads to more capable agents. However, higher           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> resolution causes proxy rewards to remain constant while true rewards decrease.                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Observation fidelity: More accurate observations improve proxy rewards but slightly reduce true rewards.        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Training steps: Optimizing the proxy reward over more steps harms true rewards after an initial period where    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> the rewards are positively correlated.                                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The plot of proxy and true reward value as functions of (Top row) model sizes, measured in parameter count;     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (Bottom row) model capability, measured by metrics such as training steps, action space resolution, and         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> observation noise. (Image source: Pan et al. 2022)                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> If a proxy reward is so poorly specified that it has a very weak correlation with the true reward, we may be    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> able to identify and prevent reward hacking even before training. Based on this hypothesis, Pan et al. (2022)   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> investigated the correlation between proxy and true rewards over a collection of trajectory rollouts.           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Interestingly, reward hacking still occurs even when there is a positive correlation between the true and proxy <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> rewards.                                                                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Hacking RLHF of LLMs#                                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reinforcement learning from human feedback (RLHF) has become the de facto approach for alignment training of    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> language models. A reward model is trained on human feedback data and then a language model is fine-tuned via   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> RL to optimize this proxy reward for human preference. There are three types of reward we care about in an RLHF <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> setup:                                                                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (1) Oracle/Gold reward $R^∗$ represents what we truly want the LLM to optimize.                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (2) Human reward $R^\\text{human}$ is what we collect to evaluate LLMs in practice, typically from individual    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> humans with time constraints. Because humans can provide inconsistent feedback or make mistakes, human reward   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> is not a fully accurate representation of the oracle reward.                                                    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (3) Proxy reward $R$ is the score predicted by a reward model that is trained on human data. Hence,             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> $R^\\text{train}$ inherits all the weakness of human reward, plus potential modeling biases.                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> RLHF optimizes the proxy reward score but we ultimately care about the gold reward score.                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Hacking the Training Process#                                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Gao et al. (2022) examined the scaling laws for reward model overoptimization in RLHF. To scale up the human    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> labels in their experiments, they use a synthetic data setup where the “gold” label for the oracle reward $R^*$ <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> is approximated by a large RM (6B parameters) where the proxy RMs for $R$ range in size of 3M to 3B parameters. <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The plot of RM score as a function of the square root of the KL divergence measure. The proxy reward is shown   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> with a dashed line, and the gold reward is shown with a solid line. (Image source: Gao et al. 2022)             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The KL divergence from the initial policy to the optimized policy is $\\text{KL} = D_\\text{KL}(\\pi |             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\pi_\\text{init})$, and the distance function is defined as $d := \\sqrt{ D_\\text{KL}(\\pi | \\pi_\\text{init})}$.   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> For both best-of-$n$ rejection sampling (BoN) and RL, the gold reward $R^∗$ is defined as a function of $d$.    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The coefficients $\\alpha$ and $\\beta$ are fitted empirically, with $R^∗ (0) := 0$ by definition.                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The authors also attempted to fit the proxy reward $R$ but found systematic underestimation when extrapolated   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> to higher KLs, as the proxy reward appeared to grow linearly with $d$.                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> $$                                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\begin{aligned}                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> R^*_{\\text{bo}n}(d) &amp;= d (\\alpha_{\\text{bo}n} - \\beta_{\\text{bo}n} d) &amp; \\text{; for best-of-n (BoN)             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> sampling.}\\\\                                                                                                    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> R^*_\\text{RL}(d) &amp;= d (\\alpha_\\text{RL} - \\beta_\\text{RL} \\log d) &amp; \\text{; for reinforcement learning}\\\\       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\end{aligned}                                                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> $$                                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The coefficient parameters, $\\alpha_{\\text{bo}n}, \\beta_{\\text{bo}n}, \\beta_\\text{RL}$ are empirically fit      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> according to data, displayed as functions of the reward model size. The coefficient $\\alpha_\\text{RL}$ is not   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> included here because it remains constant across RM sizes. (Image source: Gao et al. 2022)                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Their experiments also explored the relationship between RM overoptimization and factors like policy model size <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> and RM data size:                                                                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Larger policies see less benefit from optimization (i.e., the difference between initial and peak rewards is    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> smaller than that of a smaller policy) against an RM, but also overoptimize less.                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> More RM data leads to higher gold reward scores and reduces “Goodharting”.                                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The effect of the KL penalty on the gold score resembles early stopping. Note that in all experiments except    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> this one, the KL penalty in PPO is set to 0, because they observed that using a KL penalty strictly increases   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> the proxy-gold reward gap.                                                                                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> RLHF aims to improve the model’s alignment with human preference, but human feedback $R^\\text{human}$ may not   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> capture all the aspects we care about (e.g., factuality) and thus can be hacked to overfit to undesired         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> attributes. For example, the model may be optimized to output responses that seem correct and convincing but    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> are, in fact, inaccurate, thereby misleading human evaluators to approve its incorrect answers more often (Wen  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> et al., 2024). In other words, a gap emerges between what is correct and what looks correct to humans due to    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> RLHF. Precisely Wen et al. (2024) ran RLHF experiments using a reward model based on ChatbotArena data. They    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> evaluated the model on a question-answering dataset, QuALITY and a programming dataset, APPS. Their experiments <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> revealed that models become better at convincing humans they are correct, even when they are wrong and this     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> effect is unintended:                                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> RLHF increases human approval, but not necessarily correctness.                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> RLHF weakens humans’ ability to evaluate: The error rate of human evaluation is higher after RLHF training.     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> RLHF makes incorrect outputs more convincing to humans. The evaluation false positive rate significantly        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> increases after RLHF training.                                                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m╭─\u001b[0m\u001b[33m───────────────────────────────────────────────\u001b[0m\u001b[33m 🔧 Tool Output \u001b[0m\u001b[33m────────────────────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m\n",
       "\u001b[33m│\u001b[0m Illustration of parallel sampling vs sequential revision.                                                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Parallel Sampling#                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Given a generative model and a scoring function that we can use to score full or partial samples, there are     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m various search algorithms we can use to find a high scoring sample. Best-of-$N$ is the simplest such algorithm: \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m one just collects $N$ independent samples and chooses the highest-ranking sample according to some scoring      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m function. Beam search is a more sophisticated search algorithm that makes the search process more adaptive,     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m spending more sampling computation on more promising parts of the solution space.                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Beam search maintains a set of promising partial sequences and alternates between extending them and pruning    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m the less promising ones. As a selection mechanism, we can use a process reward model (PRM; Lightman et al.      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m 2023) to guide beam search candidate selection. Xie et al. (2023) used LLM to evaluate how likely its own       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m generated reasoning step is correct, formatted as a multiple-choice question and found that per-step            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m self-evaluation reduces accumulative errors in multi-step reasoning during beam search decoding. Besides,       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m during sampling, annealing the temperature helps mitigate aggregated randomness. These experiments by Xie et    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m al. achieved 5-6% improvement on few-shot GSM8k, AQuA and StrategyQA benchmarks with the Codex model. Reward    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m balanced search (short for “REBASE”; Wu et al. 2025) separately trained a process reward model (PRM) to         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m determine how much each node should be expanded at each depth during beam search, according to the              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m softmax-normalized reward scores. Jiang et al. (2024) trained their PRM, named “RATIONALYST”, for beam search   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m guidance on synthetic rationales conditioned on a large amount of unlabelled data. Good rationales are filtered \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m based on whether they help reduce the neg log-prob of true answer tokens by a threshold, when comparing the     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m difference between when the rationales is included in the context vs not. At inference time, RATIONALYST        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m provides process supervision to the CoT generator by helping estimate log-prob of next reasoning steps          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (“implicit”) or directly generating next reasoning steps as part of the prompt (“explicit”).                    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Beam search decoding guided by LLM self-evaluation per reasoning step. (Image source: Xie et al. 2023)          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Interestingly, it is possible to trigger the emergent chain-of-thought reasoning paths without explicit         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m zero-shot or few-shot prompting. Wang & Zhou (2024) discovered that if we branch out at the first sampling      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m tokens by retaining the top $k$ tokens with highest confidence, measured as the difference between top-1 and    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m top-2 candidates during sampling, and then continue these $k$ sampling trials with greedy decoding onward, many \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m of these sequences natively contain CoT. Especially when CoT does appear in the context, it leads to a more     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m confident decoding of the final answer. To calculate the confidence of the final answer, the answer span needs  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m to be identified by task-specific heuristics (e.g. last numerical values for math questions) or  by prompting   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m the model further with \"So the answer is\". The design choice of only branching out at the first token is based  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m on the observation that early branching significantly enhances the diversity of potential paths, while later    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m tokens are influenced a lot by previous sequences.                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Top-$k$ decoding, $k$ refers to the number of candidates at the first sampling step. (Image source: Wang &      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Zhou, 2024)                                                                                                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Sequential Revision#                                                                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m If the model can reflect and correct mistakes in past responses, we would expect the model to produce a nice    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m sequence of iterative revision with increasing quality. However, this self-correction capability turns out to   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m not exist intrinsically among LLMs and does not easily work out of the box, due to various failure modes, such  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m as, (1) hallucination, including modifying correct responses to be incorrect; (2) behavior collapse to          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m non-correcting behavior; e.g. making minor or no modification on the first incorrect responses; or (3) fail to  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m generalize to distribution shift at test time. Experiments by Huang et al. (2024) showed that naively applying  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m self-correction leads to worse performance and external feedback is needed for models to self improve, which    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m can be based on matching ground truths, heuristics and task-specific metrics, unit tests results for coding     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m questions (Shinn, et al. 2023), a stronger model (Zhang et al. 2024), as well as human feedback (Liu et al.     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m 2023).                                                                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Self-correction learning (Welleck et al. 2023) aims to train a corrector model $P_\\theta(y \\mid y_0, x)$ given  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m a fixed generator model $P_0(y_0 \\mid x)$. While the generator model remains to be generic, the corrector model \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m can task-specific and only does generation conditioned on an initial model response and additional feedback     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (e.g. a sentence, a compiler trace, unit test results; can be optional):                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Self-correction learning first generates first generates multiple outputs per prompt in the data pool;          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m then create value-improving pairs by pairing two outputs for the same prompt together if one has a higher value \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m than the other, (prompt $x$, hypothesis $y$, correction $y’$).                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m These pairs are selected proportional to is improvement in value, $v(y’) - v(y)$, and similarity between two    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m outputs, $\\text{Similarity}(y, y’)$ to train the corrector model.                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m To encourage exploration, the corrector provides new generations into the data pool as well. At the inference   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m time, the corrector can be used iteratively to create a correction trajectory of sequential revision.           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Illustration of self-correction learning by matching model outputs for the same problem to form value-improving \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m pairs to train a correction model. (Image source: Welleck et al. 2023)                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Recursive inspection (Qu et al. 2024) also aims to train a better corrector model but with a single model to do \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m both generation and self-correction.                                                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m SCoRe (Self-Correction via Reinforcement Learning; Kumar et al. 2024) is a multi-turn RL approach to encourage  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m the model to do self-correction by producing better answers at the second attempt than the one created at the   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m first attempt. It composes two stages of training: stage 1 only maximizes the accuracy of the second attempt    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m while enforcing a KL penalty only on the first attempt to avoid too much shifting of the first-turn responses   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m from the base model behavior; stage 2 optimizes the accuracy of answers produced by both the first and second   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m attempts. Ideally we do want to see performance at both first and second attempts to be better, but adding      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m stage 1 prevents the behavior collapse where the model does minor or none edits on the first response, and      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m stage 2 further improves the results.                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Explicit training setup to improve self-correction capabilities by doing two-staged RL training. (Image source: \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Kumar et al. 2024)                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m RL for Better Reasoning#                                                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m There’s been a lot of recent success in using RL to improve the reasoning ability of language models, by using  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m a collection of questions with ground truth answers (usually STEM problems and puzzles with easy to verify      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m answers), and rewarding the model for getting the correct answer.Recent activity in this area was spurred by    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m strong performance of the o-series models from OpenAI, and the subsequent releases of models and tech reports   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m from DeepSeek.                                                                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m DeepSeek-R1 (DeepSeek-AI, 2025) is an open-source LLM designed to excel in tasks that require advanced          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m reasoning skills like math, coding and logical problem solving. They run through 2 rounds of SFT-RL training,   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m enabling R1 to be good at both reasoning and non-reasoning tasks.                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Cold-start SFT is to fine-tune the DeepSeek-V3-Base base model on a collection of thousands of cold-start data. \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Without this step, the model has issues of poor readability and language mixing.                                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reasoning-oriented RL trains a reasoning model on reasoning-only prompts with two types of rule-based rewards:  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Format rewards: The model should wrap CoTs by <thinking> ... </thinking> tokens.                                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Accuracy rewards: Whether the final answers are correct. The answer for math problems needs to be present in a  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m specific format (e.g. in a box) to be verified reliably. For coding problems, a compiler is used to evaluate    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m whether test cases pass.                                                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Rejection-sampling + non-reasoning SFT utilizes new SFT data created by rejection sampling on the RL checkpoint \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m of step 2, combined with non-reasoning supervised data from DeepSeek-V3 in domains like writing, factual QA,    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m and self-cognition, to retrain DeepSeek-V3-Base.                                                                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m , such that we prioritize samples with CoTs that are good at predicting the observation (i.e., high $p(x \\mid   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m z^{(k)})$), simple, intuitive (i.e., high $p(z^{(k)})$) but also informative and not too obvious (i.e. low      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m $q(z^{(k)} \\mid x)$).                                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Iterative Learning#                                                                                             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Since pretrained models already possess the capability of generating chains of thought, it is intuitive to      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m design an iterative improvement process where we generate multiple CoTs and fine-tune the model only on         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m rationales that lead to correct answers.                                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m However, this straightforward design can fail because the model receives no learning signals for problems it    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m fails to solve. STaR (“Self-taught reasoner”; Zelikman et al. 2022) addresses this limitation by adding a       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m “rationalization” process for failed attempts, in which the model generates good CoTs backward conditioned on   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m both the problem and the ground truth answer and thus the model can generate more reasonable CoTs. Then the     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m model is finetuned on correct solutions that either lead to correct outputs or are generated through            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m rationalization.                                                                                                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The algorithm of STaR. (Image source: Zelikman et al. 2022)                                                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m We can view STaR as an approximation to a policy gradient in RL, with a simple indicator function as the        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m reward, $\\mathbb{1}[\\hat{y} = y]$. We want to maximize the expectation of this reward when sampling $z \\sim p(z \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\mid x)$ and then $y \\sim p(y \\mid x, z)$, since $p(y \\mid x) = \\sum_z p(z \\mid x) \\; p(y \\mid x, z)$.          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m $$                                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\begin{aligned}                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\nabla_\\theta J(\\theta)                                                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m &= \\nabla_\\theta \\mathbb{E}_{z_i, y_i \\sim p(.\\mid x_i)} \\mathbb{1} \\\\                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m &= \\sum_{i=1}^N \\nabla_\\theta \\mathbb{1} \\; p(y_i, z_i \\mid x_i) \\\\                                             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m &= \\sum_{i=1}^N \\mathbb{1} \\; p(y_i, z_i \\mid x_i) \\frac{\\nabla_\\theta p(y_i, z_i \\mid x_i)}{p(y_i, z_i \\mid    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m x_i)} & \\text{;log-derivative trick}\\\\                                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m &= \\mathbb{E}_{z_i, y_i \\sim p(.\\mid x_i)} \\mathbb{1} \\; \\nabla_\\theta \\log p(y_i, z_i \\mid x_i) &              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\text{;log-derivative trick}                                                                                    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\end{aligned}                                                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m $$                                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Each iteration is equivalent to first selecting the CoT samples according to $\\mathbb{1}$ and then running      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m supervised fine-tuning to optimize the logprob of generating good CoTs and answers. Performance of STaR         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m improves with more training iterations, and the “rationalization” process for generating better CoTs            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m accelerates learning. They observed that sampling with high temperature increases the chance of getting correct \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m answers with incorrect reasoning and finetuning LLM on such data can impair generalization. For datasets        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m without ground truths, majority votes of multiple high-temperature outputs can serve as a proxy of ground truth \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m answers (Wang et al. 2022), making it possible to use synthetic samples for training.                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m A comparison of accuracy of two $n$-digit numbers summation. With rationalization (CoT generation conditioned   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m on ground truth), the model can learn complex arithmetic tasks like $5$-digit summation pretty early on. (Image \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m source: Zelikman et al. 2022)                                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Scaling Laws for Thinking Time#                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m So far we have seen much evidence that allowing models to spend additional compute on reasoning before          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m producing final answers at inference time can significantly improve performance. Techniques like prompting the  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m model to generate intermediate reasoning steps before the answers, or training the model to pause and reflect   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m before predicting next tokens, have been found to boost the model performance beyond the capability limit       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m obtained during training. This essentially introduces a new dimension to tinker with for improving model        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m intelligence, complementing established factors such as model size, training compute and data quantity, as      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m defined in scaling laws (Kaplan et al. 2020).                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Recent studies demonstrated that optimizing LLM test-time compute could be more effective than scaling up model \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m parameters (Snell et al. 2024, Wu et al. 2025). Smaller models combined with advanced inference algorithms can  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m offer Pareto-optimal trade-offs in cost and performance.                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Snell et al. (2024) evaluated and compared test-time and pretraining compute, and found that they are not 1:1   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m exchangeable. Test-time compute can cover up the gap easily on easy and medium questions when there is only a   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m small gap in model capability, but proves less effective for hard problems. The ratio between token budgets for \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m pretraining and inference matters a lot. Test-time compute is only preferable when inference tokens are         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m substantially fewer than pretraining ones. This indicates that developing a capable base model with enough      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m pretraining data and compute is still very critical, as test-time compute cannot solve everything or fill in    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m big model capability gaps.                                                                                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (Left) Evaluation accuracy as a function of test time compute budgets, via iterative revisions or parallel      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m decoding. (Right) Comparing a small model with test-time compute sampling tricks and a 14x larger model with    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m only greedy decoding. We can control the token budgets used at test time such that the ratio of inference to    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m pretraining tokens is << 1, ~=1 or >> 1 and the benefit of test time compute is clear only the ratio << 1.      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (Image source: Snell et al. 2024)                                                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m s1 models (Muennighoff & Yang, et al. 2025) experimented with scaling up the CoT reasoning path length via      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m budget forcing technique (i.e. forcefully lengthen it by appending the word \"wait\", or shorten it by            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m terminating the model’s thinking process by appending end-of-thinking token or \"Final Answer:\"). They observed  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m a clear positive correlation between the average thinking time measured in tokens and the downstream evaluation \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m accuracy.                                                                                                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Both parallel and sequential scaling methods of test-time compute shows positive correlation with the           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m evaluation performance in s1 experiments. (Image Muennighoff & Yang, et al. 2025)                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m When comparing this budget forcing technique with other decoding methods of controlling reasoning trace length, \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m it is quite surprising that simple rejection sampling (i.e. sampling generation until the lengths fits a token  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m budget) leads to reversed scaling, meaning that longer CoTs lead to worse performance.                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (Left) Longer CoT path length is positively correlated with eval accuracy. (Right) Rejection sampling for       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m controlling the generated CoT path length shows a negative scaling where longer CoTs lead to worse eval         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m accuracy. (Image source: Muennighoff & Yang et al. 2025)                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m What’s for Future#                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The exploration of test-time compute and chain-of-thought reasoning presents new opportunities for enhancing    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m model capabilities. More interestingly, via test-time thinking, we are moving towards building future AI        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m systems that mirror the best practices of how humans think, incorporating adaptability, flexibility, critical   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m reflection, and error correction. Excitement with current progress invites us for more future research to       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m improve and understand deeply not just how but why we—and our models—think.                                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m At the end, I would like to call for more research for the following open research questions on test time       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m compute and chain-of-thought reasoning.                                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Can we incentivize the model to produce human-readable, faithful reasoning paths during RL training while       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m avoiding reward hacking behavior?                                                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m How to define reward hacking? Can we capture reward hacking during RL training or inference without human       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m intervention? How to prevent “whack-a-mole” style of fixes for reward hacking during RL training?               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Self-correction can happen within chain-of-thought or can be encouraged to happen explicitly during multi-turn  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m RL. How can we train the model to correct itself without hallucination or regression when ground truth is not   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m available?                                                                                                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m How to run RL training with CoT rollout for tasks that are highly contextualized, personalized and hard to      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m grade, such as creative writing, coaching, brainstorming?                                                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m When we deploy the model in reality, we cannot grow test time thinking forever and how can we smoothly          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m translate the performance gain back into the base model with reduced inference time cost (e.g. via              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m distillation)?                                                                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m How to make test time spending more adaptive according to the difficulty of the problem in hand?                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Language-Model                                                                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reinforcement-Learning                                                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reasoning                                                                                                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Long-Read                                                                                                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m  »                                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reward Hacking in Reinforcement Learning                                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m © 2025 Lil'Log                                                                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m         Powered by                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m         Hugo &                                                                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m         PaperMod                                                                                                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Hacking RL Environment#                                                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reward hacking is expected to be a more common problem as the model and the algorithm become increasingly       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m sophisticated. A more intelligent agent is more capable of finding “holes” in the design of reward function and \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m exploiting the task specification—in other words, achieving higher proxy rewards but lower true rewards. By     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m contrast, a weaker algorithm may not be able to find such loopholes, and thus we would not observe any reward   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m hacking or identify issues in the current reward function design when the model is not strong enough.           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m In a set of zero-sum robotics self-play games (Bansal et al., 2017), we can train two agents (victim vs.        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m opponent) to compete against each other. A standard training process produces a victim agent with adequate      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m performance when playing against a normal opponent. However, it is easy to train an adversarial opponent policy \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m that can defeat the victim reliably despite outputting seemingly random actions and training with fewer than 3% \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m of time steps (Gleave et al., 2020). Training of adversarial policies involves optimizing the sum of discounted \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m rewards, as in standard RL setup, while treating the victim policy as a black-box model.                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m An intuitive way to mitigate adversarial policies attacks is to fine-tune victims against adversarial policies. \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m However, the victim remains vulnerable to new versions of adversarial policies once retrained against the new   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m victim policy.                                                                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Why does adversarial policy exist? The hypothesis is that adversarial policies introduce OOD observations to    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m the victim rather than physically interfering with it. Evidence shows that when the victim’s observation of the \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m opponent’s position is masked and set to a static state, the victim becomes more robust to adversaries,         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m although performing worse against a normal opponent policy. Furthermore, a higher-dimensional observation space \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m enhances performance under normal circumstances but makes the policy more vulnerable to adversarial opponents.  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Pan et al. (2022) investigated reward hacking as a function of agent capabilities, including (1) model size,    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (2) action space resolution, (3) observation space noise, and (4) training time. They also proposed a taxonomy  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m of three types of misspecified proxy rewards:                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Misweighting: Proxy and true rewards capture the same desiderata, but differ in their relative importance.      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Ontological: Proxy and true rewards use different desiderata to capture the same concept.                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Scope: The proxy measures desiderata over a restricted domain (e.g. time or space) because measurement across   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m all conditions is too costly.                                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m They experimented in four RL environments paired with nine misspecified proxy rewards. The overall findings     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m from these experiments can be summarized as follows: A model of higher capability tends to obtain higher (or    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m similar) proxy rewards but decreased true rewards.                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Model size: Larger model size leads to increased proxy rewards but decreased true rewards.                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Action space resolution: Increased precision in actions leads to more capable agents. However, higher           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m resolution causes proxy rewards to remain constant while true rewards decrease.                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Observation fidelity: More accurate observations improve proxy rewards but slightly reduce true rewards.        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Training steps: Optimizing the proxy reward over more steps harms true rewards after an initial period where    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m the rewards are positively correlated.                                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The plot of proxy and true reward value as functions of (Top row) model sizes, measured in parameter count;     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (Bottom row) model capability, measured by metrics such as training steps, action space resolution, and         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m observation noise. (Image source: Pan et al. 2022)                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m If a proxy reward is so poorly specified that it has a very weak correlation with the true reward, we may be    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m able to identify and prevent reward hacking even before training. Based on this hypothesis, Pan et al. (2022)   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m investigated the correlation between proxy and true rewards over a collection of trajectory rollouts.           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Interestingly, reward hacking still occurs even when there is a positive correlation between the true and proxy \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m rewards.                                                                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Hacking RLHF of LLMs#                                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reinforcement learning from human feedback (RLHF) has become the de facto approach for alignment training of    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m language models. A reward model is trained on human feedback data and then a language model is fine-tuned via   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m RL to optimize this proxy reward for human preference. There are three types of reward we care about in an RLHF \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m setup:                                                                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (1) Oracle/Gold reward $R^∗$ represents what we truly want the LLM to optimize.                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (2) Human reward $R^\\text{human}$ is what we collect to evaluate LLMs in practice, typically from individual    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m humans with time constraints. Because humans can provide inconsistent feedback or make mistakes, human reward   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m is not a fully accurate representation of the oracle reward.                                                    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (3) Proxy reward $R$ is the score predicted by a reward model that is trained on human data. Hence,             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m $R^\\text{train}$ inherits all the weakness of human reward, plus potential modeling biases.                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m RLHF optimizes the proxy reward score but we ultimately care about the gold reward score.                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Hacking the Training Process#                                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Gao et al. (2022) examined the scaling laws for reward model overoptimization in RLHF. To scale up the human    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m labels in their experiments, they use a synthetic data setup where the “gold” label for the oracle reward $R^*$ \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m is approximated by a large RM (6B parameters) where the proxy RMs for $R$ range in size of 3M to 3B parameters. \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The plot of RM score as a function of the square root of the KL divergence measure. The proxy reward is shown   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m with a dashed line, and the gold reward is shown with a solid line. (Image source: Gao et al. 2022)             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The KL divergence from the initial policy to the optimized policy is $\\text{KL} = D_\\text{KL}(\\pi |             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\pi_\\text{init})$, and the distance function is defined as $d := \\sqrt{ D_\\text{KL}(\\pi | \\pi_\\text{init})}$.   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m For both best-of-$n$ rejection sampling (BoN) and RL, the gold reward $R^∗$ is defined as a function of $d$.    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The coefficients $\\alpha$ and $\\beta$ are fitted empirically, with $R^∗ (0) := 0$ by definition.                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The authors also attempted to fit the proxy reward $R$ but found systematic underestimation when extrapolated   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m to higher KLs, as the proxy reward appeared to grow linearly with $d$.                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m $$                                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\begin{aligned}                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m R^*_{\\text{bo}n}(d) &= d (\\alpha_{\\text{bo}n} - \\beta_{\\text{bo}n} d) & \\text{; for best-of-n (BoN)             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m sampling.}\\\\                                                                                                    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m R^*_\\text{RL}(d) &= d (\\alpha_\\text{RL} - \\beta_\\text{RL} \\log d) & \\text{; for reinforcement learning}\\\\       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\end{aligned}                                                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m $$                                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The coefficient parameters, $\\alpha_{\\text{bo}n}, \\beta_{\\text{bo}n}, \\beta_\\text{RL}$ are empirically fit      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m according to data, displayed as functions of the reward model size. The coefficient $\\alpha_\\text{RL}$ is not   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m included here because it remains constant across RM sizes. (Image source: Gao et al. 2022)                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Their experiments also explored the relationship between RM overoptimization and factors like policy model size \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m and RM data size:                                                                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Larger policies see less benefit from optimization (i.e., the difference between initial and peak rewards is    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m smaller than that of a smaller policy) against an RM, but also overoptimize less.                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m More RM data leads to higher gold reward scores and reduces “Goodharting”.                                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The effect of the KL penalty on the gold score resembles early stopping. Note that in all experiments except    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m this one, the KL penalty in PPO is set to 0, because they observed that using a KL penalty strictly increases   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m the proxy-gold reward gap.                                                                                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m RLHF aims to improve the model’s alignment with human preference, but human feedback $R^\\text{human}$ may not   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m capture all the aspects we care about (e.g., factuality) and thus can be hacked to overfit to undesired         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m attributes. For example, the model may be optimized to output responses that seem correct and convincing but    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m are, in fact, inaccurate, thereby misleading human evaluators to approve its incorrect answers more often (Wen  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m et al., 2024). In other words, a gap emerges between what is correct and what looks correct to humans due to    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m RLHF. Precisely Wen et al. (2024) ran RLHF experiments using a reward model based on ChatbotArena data. They    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m evaluated the model on a question-answering dataset, QuALITY and a programming dataset, APPS. Their experiments \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m revealed that models become better at convincing humans they are correct, even when they are wrong and this     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m effect is unintended:                                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m RLHF increases human approval, but not necessarily correctness.                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m RLHF weakens humans’ ability to evaluate: The error rate of human evaluation is higher after RLHF training.     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m RLHF makes incorrect outputs more convincing to humans. The evaluation false positive rate significantly        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m increases after RLHF training.                                                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">╭───────────────────────────────────────────────────── 📝 AI ─────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> Let me search for more specific information about the mechanisms by which RL improves reasoning in LLMs.        <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> 🔧 Tool Call: retrieve_blog_posts                                                                               <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>    Args: {                                                                                                      <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>   \"query\": \"RL reinforcement learning reasoning mechanisms chain of thought CoT training\"                       <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> }                                                                                                               <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37m╭─\u001b[0m\u001b[37m────────────────────────────────────────────────────\u001b[0m\u001b[37m 📝 AI \u001b[0m\u001b[37m────────────────────────────────────────────────────\u001b[0m\u001b[37m─╮\u001b[0m\n",
       "\u001b[37m│\u001b[0m Let me search for more specific information about the mechanisms by which RL improves reasoning in LLMs.        \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m 🔧 Tool Call: retrieve_blog_posts                                                                               \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m    Args: {                                                                                                      \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m   \"query\": \"RL reinforcement learning reasoning mechanisms chain of thought CoT training\"                       \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m }                                                                                                               \u001b[37m│\u001b[0m\n",
       "\u001b[37m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">╭──────────────────────────────────────────────── 🔧 Tool Output ─────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> , such that we prioritize samples with CoTs that are good at predicting the observation (i.e., high $p(x \\mid   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> z^{(k)})$), simple, intuitive (i.e., high $p(z^{(k)})$) but also informative and not too obvious (i.e. low      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> $q(z^{(k)} \\mid x)$).                                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Iterative Learning#                                                                                             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Since pretrained models already possess the capability of generating chains of thought, it is intuitive to      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> design an iterative improvement process where we generate multiple CoTs and fine-tune the model only on         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> rationales that lead to correct answers.                                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> However, this straightforward design can fail because the model receives no learning signals for problems it    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> fails to solve. STaR (“Self-taught reasoner”; Zelikman et al. 2022) addresses this limitation by adding a       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> “rationalization” process for failed attempts, in which the model generates good CoTs backward conditioned on   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> both the problem and the ground truth answer and thus the model can generate more reasonable CoTs. Then the     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> model is finetuned on correct solutions that either lead to correct outputs or are generated through            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> rationalization.                                                                                                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The algorithm of STaR. (Image source: Zelikman et al. 2022)                                                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> We can view STaR as an approximation to a policy gradient in RL, with a simple indicator function as the        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> reward, $\\mathbb{1}[\\hat{y} = y]$. We want to maximize the expectation of this reward when sampling $z \\sim p(z <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\mid x)$ and then $y \\sim p(y \\mid x, z)$, since $p(y \\mid x) = \\sum_z p(z \\mid x) \\; p(y \\mid x, z)$.          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> $$                                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\begin{aligned}                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\nabla_\\theta J(\\theta)                                                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> &amp;= \\nabla_\\theta \\mathbb{E}_{z_i, y_i \\sim p(.\\mid x_i)} \\mathbb{1} \\\\                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> &amp;= \\sum_{i=1}^N \\nabla_\\theta \\mathbb{1} \\; p(y_i, z_i \\mid x_i) \\\\                                             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> &amp;= \\sum_{i=1}^N \\mathbb{1} \\; p(y_i, z_i \\mid x_i) \\frac{\\nabla_\\theta p(y_i, z_i \\mid x_i)}{p(y_i, z_i \\mid    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> x_i)} &amp; \\text{;log-derivative trick}\\\\                                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> &amp;= \\mathbb{E}_{z_i, y_i \\sim p(.\\mid x_i)} \\mathbb{1} \\; \\nabla_\\theta \\log p(y_i, z_i \\mid x_i) &amp;              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\text{;log-derivative trick}                                                                                    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\end{aligned}                                                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> $$                                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Each iteration is equivalent to first selecting the CoT samples according to $\\mathbb{1}$ and then running      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> supervised fine-tuning to optimize the logprob of generating good CoTs and answers. Performance of STaR         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> improves with more training iterations, and the “rationalization” process for generating better CoTs            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> accelerates learning. They observed that sampling with high temperature increases the chance of getting correct <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> answers with incorrect reasoning and finetuning LLM on such data can impair generalization. For datasets        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> without ground truths, majority votes of multiple high-temperature outputs can serve as a proxy of ground truth <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> answers (Wang et al. 2022), making it possible to use synthetic samples for training.                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> A comparison of accuracy of two $n$-digit numbers summation. With rationalization (CoT generation conditioned   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> on ground truth), the model can learn complex arithmetic tasks like $5$-digit summation pretty early on. (Image <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> source: Zelikman et al. 2022)                                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Scaling Laws for Thinking Time#                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> So far we have seen much evidence that allowing models to spend additional compute on reasoning before          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> producing final answers at inference time can significantly improve performance. Techniques like prompting the  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> model to generate intermediate reasoning steps before the answers, or training the model to pause and reflect   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> before predicting next tokens, have been found to boost the model performance beyond the capability limit       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> obtained during training. This essentially introduces a new dimension to tinker with for improving model        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> intelligence, complementing established factors such as model size, training compute and data quantity, as      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> defined in scaling laws (Kaplan et al. 2020).                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Recent studies demonstrated that optimizing LLM test-time compute could be more effective than scaling up model <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> parameters (Snell et al. 2024, Wu et al. 2025). Smaller models combined with advanced inference algorithms can  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> offer Pareto-optimal trade-offs in cost and performance.                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Snell et al. (2024) evaluated and compared test-time and pretraining compute, and found that they are not 1:1   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> exchangeable. Test-time compute can cover up the gap easily on easy and medium questions when there is only a   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> small gap in model capability, but proves less effective for hard problems. The ratio between token budgets for <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> pretraining and inference matters a lot. Test-time compute is only preferable when inference tokens are         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> substantially fewer than pretraining ones. This indicates that developing a capable base model with enough      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> pretraining data and compute is still very critical, as test-time compute cannot solve everything or fill in    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> big model capability gaps.                                                                                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (Left) Evaluation accuracy as a function of test time compute budgets, via iterative revisions or parallel      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> decoding. (Right) Comparing a small model with test-time compute sampling tricks and a 14x larger model with    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> only greedy decoding. We can control the token budgets used at test time such that the ratio of inference to    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> pretraining tokens is &lt;&lt; 1, ~=1 or &gt;&gt; 1 and the benefit of test time compute is clear only the ratio &lt;&lt; 1.      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (Image source: Snell et al. 2024)                                                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> s1 models (Muennighoff &amp; Yang, et al. 2025) experimented with scaling up the CoT reasoning path length via      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> budget forcing technique (i.e. forcefully lengthen it by appending the word \"wait\", or shorten it by            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> terminating the model’s thinking process by appending end-of-thinking token or \"Final Answer:\"). They observed  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> a clear positive correlation between the average thinking time measured in tokens and the downstream evaluation <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> accuracy.                                                                                                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Both parallel and sequential scaling methods of test-time compute shows positive correlation with the           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> evaluation performance in s1 experiments. (Image Muennighoff &amp; Yang, et al. 2025)                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> When comparing this budget forcing technique with other decoding methods of controlling reasoning trace length, <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> it is quite surprising that simple rejection sampling (i.e. sampling generation until the lengths fits a token  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> budget) leads to reversed scaling, meaning that longer CoTs lead to worse performance.                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (Left) Longer CoT path length is positively correlated with eval accuracy. (Right) Rejection sampling for       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> controlling the generated CoT path length shows a negative scaling where longer CoTs lead to worse eval         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> accuracy. (Image source: Muennighoff &amp; Yang et al. 2025)                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> What’s for Future#                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The exploration of test-time compute and chain-of-thought reasoning presents new opportunities for enhancing    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> model capabilities. More interestingly, via test-time thinking, we are moving towards building future AI        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> systems that mirror the best practices of how humans think, incorporating adaptability, flexibility, critical   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> reflection, and error correction. Excitement with current progress invites us for more future research to       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> improve and understand deeply not just how but why we—and our models—think.                                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> At the end, I would like to call for more research for the following open research questions on test time       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> compute and chain-of-thought reasoning.                                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Can we incentivize the model to produce human-readable, faithful reasoning paths during RL training while       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> avoiding reward hacking behavior?                                                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> How to define reward hacking? Can we capture reward hacking during RL training or inference without human       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> intervention? How to prevent “whack-a-mole” style of fixes for reward hacking during RL training?               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Self-correction can happen within chain-of-thought or can be encouraged to happen explicitly during multi-turn  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> RL. How can we train the model to correct itself without hallucination or regression when ground truth is not   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> available?                                                                                                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> How to run RL training with CoT rollout for tasks that are highly contextualized, personalized and hard to      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> grade, such as creative writing, coaching, brainstorming?                                                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> When we deploy the model in reality, we cannot grow test time thinking forever and how can we smoothly          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> translate the performance gain back into the base model with reduced inference time cost (e.g. via              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> distillation)?                                                                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> How to make test time spending more adaptive according to the difficulty of the problem in hand?                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Filter out CoTs with mixed languages, long paragraphs, and code blocks.                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Include non-reasoning tasks using DeepSeek-V3 (DeepSeek-AI, 2024) pipeline.                                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> For certain non-reasoning tasks, call DeepSeek-V3 to generate potential CoTs before answering the question by   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> prompting. But for simpler queries like “hello”, CoT is not needed.                                             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Then fine-tune the DeepSeek-V3-Base on the total 800k samples for 2 epochs.                                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The final RL stage trains the step 3 checkpoint on both reasoning and non-reasoning prompts, improving          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> helpfulness, harmlessness and reasoning.                                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> DeepSeek-R1 performs comparable to OpenAI o1-preview and o1-mini on several widely used reasoning benchmarks.   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> DeepSeek-V3 is the only non-reasoning model listed. (Image source: DeepSeek-AI, 2025)                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Interestingly the DeepSeek team showed that with pure RL, no SFT stage, it is still possible to learn advanced  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> reasoning capabilities like reflection and backtracking (“Aha moment”). The model naturally learns to spend     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> more thinking tokens during the RL training process to solve reasoning tasks. The “aha moment” can emerge,      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> referring to the model reflecting on previous mistakes and then trying alternative approaches to correct them.  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Later, various open source efforts happened for replicating R1 results like Open-R1, SimpleRL-reason, and       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> TinyZero, all based on Qwen models. These efforts also confirmed that pure RL leads to great performance on     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> math problems, as well as the emergent “aha moment”.                                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Examples of the model learning to reflect and correct mistakes. (Image source: (left) DeepSeek-AI, 2025;        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (right) Zeng et al. 2025)                                                                                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The DeepSeek team also shared some of their unsuccessful attempts. They failed to use process reward model      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (PRM) as it is hard to define per-step rubrics or determine whether an intermediate step is correct, meanwhile  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> making the training more vulnerable to reward hacking. The efforts on MCTS (Monte Carlo Tree Search) also       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> failed due to the large search space for language model tokens, in comparison to, say, chess; and training the  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> fine-grained value model used for guiding the search is very challenging too. Failed attempts often provide     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> unique insights and we would like to encourage the research community to share more about what did not work     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> out.                                                                                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> External Tool Use#                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> During the reasoning steps, certain intermediate steps can be reliably and accurately solved by executing code  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> or running mathematical calculations. Offloading that part of reasoning components into an external code        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> interpreter, as in PAL (Program-Aided Language Model; Gao et al. 2022) or Chain of Code (Li et al. 2023), can   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> extend the capability of LLM with external tools, eliminating the need for LLMs to learn to execute code or     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> function as calculators themselves. These code emulators, like in Chain of Code, can be augmented by an LLM     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> such that if a standard code interpreter fails, we have the option of using LLM to execute that line of code    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> instead. Using code to enhance reasoning steps are especially beneficial for mathematical problems, symbolic    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> reasoning and algorithmic tasks. These unit tests may not exist as part of the coding questions, and in those   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> cases, we can instruct the model to self-generate unit tests for it to test against to verify the solution      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (Shinn, et al. 2023).                                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> An example of program-aided language model prompting looks like. (Image source: Gao et al. 2022)                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> ReAct (Reason+Act; Yao et al. 2023) combines the action of searching the Wikipedia API and generation of        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> reasoning traces, such that reasoning paths can incorporate external knowledge.                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> An example of the ReAct prompting method to solve a HotpotQA question, using Wikipedia search API as an         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> external tool to help with reasoning. (Image source: Yao et al. 2023)                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> o3 &amp; o4-mini, recently released by OpenAI, are another two good examples where the reasoning process involves   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> tool use like Web search, code execution and image processing. The team observed that large-scale reinforcement <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> learning exhibits the same trend as in the GPT paradigm that “more compute = better performance”.               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Thinking Faithfully#                                                                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Deep learning models are often treated as black boxes and various interpretability methods have been proposed.  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Interpretability is useful for a couple reasons: first, it gives us an extra test to determine if the model is  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> misaligned with its creators’ intent, or if it’s misbehaving in some way that we can’t tell by monitoring its   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> actions. Second, it can help us determine whether the model is using a sound process to compute its answers.    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Chain of thought provides an especially convenient form of interpretability, as it makes the model’s internal   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> process visible in natural language. This interpretability, however, rests on the assumption that the model     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> truthfully describes its internal thought processes.                                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Recent work showed that monitoring CoT of reasoning models can effectively detect model misbehavior such as     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> reward hacking, and can even enable a weaker model to monitor a stronger model (Baker et al. 2025). Increasing  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> test time compute can also lead to improved adversarial robustness (Zaremba et al. 2025); this makes sense      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> intuitively, because thinking for longer should be especially useful when the model is presented with an        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> unusual input, such as an adversarial example or jailbreak attempt – it can use the extra thinking time to make <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> sense of the strange situation it’s been presented with.                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The experiment of asking the model to decide if another model tried to hack the unit tests in some way for      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> coding questions given its thought process. We can monitor these reward hacking behavior during training with   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> different types of monitor. The exit(0) coding hack is when the agent exploited a bug that allowed it to exit   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> from the environment early without running all unit tests. The raise SkipTest hack is when the agent raises an  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> exception from functions outside the testing framework in order to skip unit test evaluation. (Image source:    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Baker et al. 2025)                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Does the Model Tell What it Thinks Faithfully#                                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Intuitively, model CoTs could be biased due to lack of explicit training objectives aimed at encouraging        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> faithful reasoning. Or when we fine-tune the model on human-written explanations, those human-written samples   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> may contain mistakes. Thus we cannot by default assume CoT is always faithful .                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Lanham et al. (2023) investigated several modes of CoT faithfulness failures by deliberately introducing        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> mistakes into CoTs and measuring their impacts on the accuracy of a set of multiple choice tasks (e.g. AQuA,    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> MMLU, ARC Challenge, TruthfulQA, HellaSwag):                                                                    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Mistake 1 (Early answering): The model may form a conclusion prematurely before CoT is generated. This is       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> tested by early truncating or inserting mistakes into CoT. Different tasks revealed varying task-specific       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> dependencies on CoT effectiveness; some have evaluation performance sensitive to truncated CoT but some do not. <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Wang et al. (2023) did similar experiments but with more subtle mistakes related to bridging objects or         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> language templates in the formation of CoT.                                                                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Mistake 2 (Uninformative tokens): Uninformative CoT tokens improve performance. This hypothesis is tested by    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> replacing CoT with filler text (e.g. all periods) and this setup shows no accuracy increase and some tasks may  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> suffer performance drop slightly when compared to no CoT.                                                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Mistake 3 (Human-unreadable encoding): Relevant information is encoded in a way that is hard for humans to      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> understand. Paraphrasing CoTs in an non-standard way did not degrade performance across datasets, suggesting    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> accuracy gains do not rely on human-readable reasoning.                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Illustration of different ways of CoT perturbation to assess its faithfulness. (Image source: Lanham et al.     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> 2023)                                                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Interestingly, Lanham et al. suggests that for multiple choice questions, smaller models may not be capable     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> enough of utilizing CoT well, whereas larger models may have been able to solve the tasks without CoT. This     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> dependency on CoT reasoning, measured by the percent of obtaining the same answer with vs without CoT, does not <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> always increase with model size on multiple choice questions, but does increase with model size on addition     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> tasks, implying that thinking time matters more for complex reasoning tasks.                                    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The dependency on CoT reasoning is measured as the percentage of obtaining same answers with vs without CoT. It <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> matters more for reasoning tasks like addition and larger models benefit more. (Image source: Lanham et al.     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> 2023)                                                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Illustration of parallel sampling vs sequential revision.                                                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Parallel Sampling#                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Given a generative model and a scoring function that we can use to score full or partial samples, there are     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> various search algorithms we can use to find a high scoring sample. Best-of-$N$ is the simplest such algorithm: <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> one just collects $N$ independent samples and chooses the highest-ranking sample according to some scoring      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> function. Beam search is a more sophisticated search algorithm that makes the search process more adaptive,     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> spending more sampling computation on more promising parts of the solution space.                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Beam search maintains a set of promising partial sequences and alternates between extending them and pruning    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> the less promising ones. As a selection mechanism, we can use a process reward model (PRM; Lightman et al.      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> 2023) to guide beam search candidate selection. Xie et al. (2023) used LLM to evaluate how likely its own       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> generated reasoning step is correct, formatted as a multiple-choice question and found that per-step            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> self-evaluation reduces accumulative errors in multi-step reasoning during beam search decoding. Besides,       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> during sampling, annealing the temperature helps mitigate aggregated randomness. These experiments by Xie et    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> al. achieved 5-6% improvement on few-shot GSM8k, AQuA and StrategyQA benchmarks with the Codex model. Reward    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> balanced search (short for “REBASE”; Wu et al. 2025) separately trained a process reward model (PRM) to         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> determine how much each node should be expanded at each depth during beam search, according to the              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> softmax-normalized reward scores. Jiang et al. (2024) trained their PRM, named “RATIONALYST”, for beam search   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> guidance on synthetic rationales conditioned on a large amount of unlabelled data. Good rationales are filtered <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> based on whether they help reduce the neg log-prob of true answer tokens by a threshold, when comparing the     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> difference between when the rationales is included in the context vs not. At inference time, RATIONALYST        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> provides process supervision to the CoT generator by helping estimate log-prob of next reasoning steps          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (“implicit”) or directly generating next reasoning steps as part of the prompt (“explicit”).                    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Beam search decoding guided by LLM self-evaluation per reasoning step. (Image source: Xie et al. 2023)          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Interestingly, it is possible to trigger the emergent chain-of-thought reasoning paths without explicit         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> zero-shot or few-shot prompting. Wang &amp; Zhou (2024) discovered that if we branch out at the first sampling      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> tokens by retaining the top $k$ tokens with highest confidence, measured as the difference between top-1 and    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> top-2 candidates during sampling, and then continue these $k$ sampling trials with greedy decoding onward, many <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> of these sequences natively contain CoT. Especially when CoT does appear in the context, it leads to a more     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> confident decoding of the final answer. To calculate the confidence of the final answer, the answer span needs  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> to be identified by task-specific heuristics (e.g. last numerical values for math questions) or  by prompting   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> the model further with \"So the answer is\". The design choice of only branching out at the first token is based  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> on the observation that early branching significantly enhances the diversity of potential paths, while later    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> tokens are influenced a lot by previous sequences.                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Top-$k$ decoding, $k$ refers to the number of candidates at the first sampling step. (Image source: Wang &amp;      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Zhou, 2024)                                                                                                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Sequential Revision#                                                                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> If the model can reflect and correct mistakes in past responses, we would expect the model to produce a nice    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> sequence of iterative revision with increasing quality. However, this self-correction capability turns out to   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> not exist intrinsically among LLMs and does not easily work out of the box, due to various failure modes, such  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> as, (1) hallucination, including modifying correct responses to be incorrect; (2) behavior collapse to          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> non-correcting behavior; e.g. making minor or no modification on the first incorrect responses; or (3) fail to  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> generalize to distribution shift at test time. Experiments by Huang et al. (2024) showed that naively applying  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> self-correction leads to worse performance and external feedback is needed for models to self improve, which    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> can be based on matching ground truths, heuristics and task-specific metrics, unit tests results for coding     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> questions (Shinn, et al. 2023), a stronger model (Zhang et al. 2024), as well as human feedback (Liu et al.     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> 2023).                                                                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Self-correction learning (Welleck et al. 2023) aims to train a corrector model $P_\\theta(y \\mid y_0, x)$ given  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> a fixed generator model $P_0(y_0 \\mid x)$. While the generator model remains to be generic, the corrector model <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> can task-specific and only does generation conditioned on an initial model response and additional feedback     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (e.g. a sentence, a compiler trace, unit test results; can be optional):                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Self-correction learning first generates first generates multiple outputs per prompt in the data pool;          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> then create value-improving pairs by pairing two outputs for the same prompt together if one has a higher value <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> than the other, (prompt $x$, hypothesis $y$, correction $y’$).                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> These pairs are selected proportional to is improvement in value, $v(y’) - v(y)$, and similarity between two    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> outputs, $\\text{Similarity}(y, y’)$ to train the corrector model.                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> To encourage exploration, the corrector provides new generations into the data pool as well. At the inference   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> time, the corrector can be used iteratively to create a correction trajectory of sequential revision.           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Illustration of self-correction learning by matching model outputs for the same problem to form value-improving <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> pairs to train a correction model. (Image source: Welleck et al. 2023)                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Recursive inspection (Qu et al. 2024) also aims to train a better corrector model but with a single model to do <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> both generation and self-correction.                                                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> SCoRe (Self-Correction via Reinforcement Learning; Kumar et al. 2024) is a multi-turn RL approach to encourage  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> the model to do self-correction by producing better answers at the second attempt than the one created at the   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> first attempt. It composes two stages of training: stage 1 only maximizes the accuracy of the second attempt    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> while enforcing a KL penalty only on the first attempt to avoid too much shifting of the first-turn responses   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> from the base model behavior; stage 2 optimizes the accuracy of answers produced by both the first and second   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> attempts. Ideally we do want to see performance at both first and second attempts to be better, but adding      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> stage 1 prevents the behavior collapse where the model does minor or none edits on the first response, and      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> stage 2 further improves the results.                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Explicit training setup to improve self-correction capabilities by doing two-staged RL training. (Image source: <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Kumar et al. 2024)                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> RL for Better Reasoning#                                                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> There’s been a lot of recent success in using RL to improve the reasoning ability of language models, by using  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> a collection of questions with ground truth answers (usually STEM problems and puzzles with easy to verify      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> answers), and rewarding the model for getting the correct answer.Recent activity in this area was spurred by    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> strong performance of the o-series models from OpenAI, and the subsequent releases of models and tech reports   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> from DeepSeek.                                                                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> DeepSeek-R1 (DeepSeek-AI, 2025) is an open-source LLM designed to excel in tasks that require advanced          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> reasoning skills like math, coding and logical problem solving. They run through 2 rounds of SFT-RL training,   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> enabling R1 to be good at both reasoning and non-reasoning tasks.                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Cold-start SFT is to fine-tune the DeepSeek-V3-Base base model on a collection of thousands of cold-start data. <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Without this step, the model has issues of poor readability and language mixing.                                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reasoning-oriented RL trains a reasoning model on reasoning-only prompts with two types of rule-based rewards:  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Format rewards: The model should wrap CoTs by &lt;thinking&gt; ... &lt;/thinking&gt; tokens.                                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Accuracy rewards: Whether the final answers are correct. The answer for math problems needs to be present in a  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> specific format (e.g. in a box) to be verified reliably. For coding problems, a compiler is used to evaluate    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> whether test cases pass.                                                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Rejection-sampling + non-reasoning SFT utilizes new SFT data created by rejection sampling on the RL checkpoint <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> of step 2, combined with non-reasoning supervised data from DeepSeek-V3 in domains like writing, factual QA,    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> and self-cognition, to retrain DeepSeek-V3-Base.                                                                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The dependency on CoT reasoning is measured as the percentage of obtaining same answers with vs without CoT. It <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> matters more for reasoning tasks like addition and larger models benefit more. (Image source: Lanham et al.     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> 2023)                                                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Alternative approaches for testing CoT faithfulness involve perturbing prompts rather than modifying CoT paths  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> directly (Turpin et al. 2023, Chua &amp; Evans, 2025, Chen et al. 2025).                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> One method consistently labels correct answers as “(A)” in few-shot examples regardless of true labels to       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> introduce biases.                                                                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Another prompting technique inserts misleading hints into prompts, such as \"I think the answer is               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> &lt;random_label&gt; but curious to hear what you think\". or \"A Stanford Professor thinks the answer is               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> &lt;random_label&gt;\". By comparing model predictions for the same question with vs without the misleading hint, we   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> can measure whether a model is able to faithfully describe the influence of the hint on its answer.             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Particularly, in cases where the model produces different hint and non-hint answers, we measure whether the     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> model acknowledges the hint when solving the question with hint. If the model is faithful, it should explicitly <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> acknowledge the impact and admit the change of its answer is due to the hint.                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Both GPT and Claude models are sensitive to different types of biases in context. Decrease in model accuracy    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> indicates systematic unfaithfulness. Direct hints of wrong labels are more effectively than \"Answer is always   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> A\" type of bias. (Image source: Turpin et al. 2023)                                                             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Multiple studies found that reasoning models describe the influence of the hint much more reliably than all the <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> non-reasoning models tested. For example, we can measure the fraction of samples where the model acknowledges   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> the hint as a deciding factor (“faithful CoT”). Reasoning models (Claude 3.7 Sonnet, DeepSeek R1) are overall   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> doing better than non-reasoning ones (Claude 3.6, DeepSeek V3).                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reasoning models are more likely to reveal faithful CoT than non-reasoning models. (Image source: Chen et al.   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> 2025)                                                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Some evidence showed that using reward models leads to less faithfulness in model outputs. The reward model in  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> classic RLHF is not trained to incentivize faithful behavior in this test as the preference comparison dataset  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> is collected by humans selecting which one seems better or pleasing. However, in reasoning models, the CoT      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> paths are optimized for producing the correct final answers, not to match human preference defined in RM, and   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> thus intuitively are expected to be more faithful.                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Optimization Pressure on CoT: Good or Bad?#                                                                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Monitoring CoT of reasoning models for reward hacking behavior is effective. A simple way to use this monitor   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> is to run rejection sampling with the monitor as a filter at test time and identify solutions without reward    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> hacking. However, it only works at the early stage of RL, and, as the model is further optimized, it is rare to <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> find a sample to pass a CoT monitor within budget. This challenge naturally suggests that we could consider     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> incorporating CoT monitors into RL rewards—a form of process-based supervision—to prevent reward hacking.       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> However, incorporating CoT monitoring reward into RL does not work out well, as it leads to obfuscated reward   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> hacking where the model tries to hide its true intent within the CoT (Baker et al. 2025). With this direct      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> optimization pressure provided by the CoT monitor applied during RL training, the model still can find ways to  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> hack, although a bit less frequently, but can hide its intention in CoT to avoid being caught by the monitor.   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> When using the monitor to provide an RL reward signal to prevent reward hacking during training, the model      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> still can find ways to hack while avoiding being caught by the monitor. (Image source: Baker et al. 2025)       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Experiments on CoT length RL signals by Yeo et al. (2025) confirmed a similar message as Baker et al. that new  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> types of reward hacking can happen with new types of RL reward shaping. They designed the reward function such  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> that correct CoTs can have higher rewards than wrong ones, short correct CoTs obtain higher rewards than long   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> correct ones and short incorrect CoT receive higher penalties than long incorrect CoTs. With this extra reward, <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> the model learns to repeat text in CoTs for challenging questions rather than attempting to solve them. Yeo et  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> al. further applied a n-gram repetition penalty to prevent this hacking behavior.                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Careful reward shaping is needed to stabilize training with CoT length rewards. (Image source: Yeo et al. 2025) <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Chen et al. (2025) experimented with a flawed RL environment, specifically using a grader with incorrect        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> answers filled in for multiple-choice questions. The model learns to exploit the reward hack on &gt;99% of the     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> prompts, but almost never (&lt;2%) verbalizes the reward hack in its CoT on more than half of their environments.  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Additional RL optimization pressure fails to incentivize the model to verbalize the hack in this case.          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> RL training is inherently sensitive to reward hacking. Only relying on heuristic investigation of reward        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> hacking and manual fixes may lead to a “whack-a-mole” situation. We would suggest being very cautious when      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> trying to apply optimization directly on CoT during RL training, or trying to avoid it altogether.              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Thinking in Continuous Space#                                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Adaptive Computation Time, introduced by Alex Graves in 2016, predated large language models but pioneered the  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> same direction of enabling the model to dynamically decide the number of computational steps to take at the     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> inference time, which can be viewed as enabling the model to “think more” in continuous space at test time.     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Adaptive thinking time in continuous space can be enabled vertically via recurrent architecture or horizontally <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> via more sequential sampling steps.                                                                             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Recurrent Architecture#                                                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> A number of architecture variations have been proposed to make the Transformer architecture recurrent, enabling <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> adaptive test time compute (Dehghani, et al. 2019, Hutchins, et al. 2022, Bulatov, et al. 2022). A deep dive    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> into literature on this topic would make the post too long, so we will only review a few.                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Universal Transformer (Dehghani, et al. 2019) combines self-attention in Transformer with the recurrent         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> mechanism in RNN, dynamically adjusting the number of steps using adaptive computation time (Graves, 2016). On  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> a high level, it can be viewed as a recurrent function for learning the hidden state representation per token,  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> and if the number of steps is fixed, an Universal Transformer is equivalent to a multi-layer Transformer with   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> shared parameters across layers.                                                                                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> A recent recurrent architecture design, proposed by Geiping et al. (2025), adds a recurrent block $R$ on top of <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> the standard Transformer. Every iteration of this recurrent block takes the embedding $\\mathbf{e}$ and a random <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> state $\\mathbf{s}_i$. Conceptually, this recurrent-depth architecture is a bit similar to a conditioned         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> diffusion model, where the original input $\\mathbf{e}$ is provided in every recurrent step while a random       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Gaussian initialized state $\\mathbf{s}_i$ gets updated iteratively through the process. (Interestingly some of  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> their experiments of designs that resemble diffusion models more turned out to be bad.)                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> $$                                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\begin{aligned}                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\mathbf{e} &amp;= P(\\mathbf{x}) &amp; \\text{embedding} \\\\ \\mathbf{s}\\_0 &amp;\\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 I\\_{n    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\cdot h}) \\\\ \\mathbf{s}\\_i &amp;= R(\\mathbf{e}, \\mathbf{s}\\_{i-1}) \\quad\\text{ for }i \\in {1, \\dots, r} &amp;           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\small{\\text{recurrent block; resembles a Transformer block}}\\\\ \\mathbf{p} &amp;= C(\\mathbf{s}\\_r) &amp;                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\text{unembedding}                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\end{aligned}                                                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> $$                                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The recurrence count $r$ during training is randomized, sampled from a log-normal Poisson distribution, per     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> input sequence. To manage computational costs, backpropagation is truncated to only the last $k$ iterations of  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> the recurrent unit ($k=8$ in experiments), making it possible to train on the heavy-tail part of the Poisson    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> distribution. The embedding block continues to receive gradient updates in every step since its output          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> $\\mathbf{e}$ is injected in every step, mimicking RNN training. Unsurprisingly, the stability of training a     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> recurrent model turns out to be very sensitive. Factors like initialization, normalization and hyperparameters  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> all matter, especially when scaling up the training. For example, hidden states can collapse by predicting the  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> same hidden state for every token; or the model may learn to ignore the incoming state $\\mathbf{s}$. To         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> stabilize the training, Geiping et al. adopted an embedding scale factor, a small learning rate and careful     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> tuning.                                                                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Plot of an experiment on training a 3.5B model with depth recurrence. The saturation roughly happens around     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> $\\bar{r} = 32$, making us wonder how this architecture extrapolate and generalize to larger iteration counts.   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (Image source: Geiping et al. 2025)                                                                             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m╭─\u001b[0m\u001b[33m───────────────────────────────────────────────\u001b[0m\u001b[33m 🔧 Tool Output \u001b[0m\u001b[33m────────────────────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m\n",
       "\u001b[33m│\u001b[0m , such that we prioritize samples with CoTs that are good at predicting the observation (i.e., high $p(x \\mid   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m z^{(k)})$), simple, intuitive (i.e., high $p(z^{(k)})$) but also informative and not too obvious (i.e. low      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m $q(z^{(k)} \\mid x)$).                                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Iterative Learning#                                                                                             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Since pretrained models already possess the capability of generating chains of thought, it is intuitive to      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m design an iterative improvement process where we generate multiple CoTs and fine-tune the model only on         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m rationales that lead to correct answers.                                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m However, this straightforward design can fail because the model receives no learning signals for problems it    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m fails to solve. STaR (“Self-taught reasoner”; Zelikman et al. 2022) addresses this limitation by adding a       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m “rationalization” process for failed attempts, in which the model generates good CoTs backward conditioned on   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m both the problem and the ground truth answer and thus the model can generate more reasonable CoTs. Then the     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m model is finetuned on correct solutions that either lead to correct outputs or are generated through            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m rationalization.                                                                                                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The algorithm of STaR. (Image source: Zelikman et al. 2022)                                                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m We can view STaR as an approximation to a policy gradient in RL, with a simple indicator function as the        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m reward, $\\mathbb{1}[\\hat{y} = y]$. We want to maximize the expectation of this reward when sampling $z \\sim p(z \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\mid x)$ and then $y \\sim p(y \\mid x, z)$, since $p(y \\mid x) = \\sum_z p(z \\mid x) \\; p(y \\mid x, z)$.          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m $$                                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\begin{aligned}                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\nabla_\\theta J(\\theta)                                                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m &= \\nabla_\\theta \\mathbb{E}_{z_i, y_i \\sim p(.\\mid x_i)} \\mathbb{1} \\\\                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m &= \\sum_{i=1}^N \\nabla_\\theta \\mathbb{1} \\; p(y_i, z_i \\mid x_i) \\\\                                             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m &= \\sum_{i=1}^N \\mathbb{1} \\; p(y_i, z_i \\mid x_i) \\frac{\\nabla_\\theta p(y_i, z_i \\mid x_i)}{p(y_i, z_i \\mid    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m x_i)} & \\text{;log-derivative trick}\\\\                                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m &= \\mathbb{E}_{z_i, y_i \\sim p(.\\mid x_i)} \\mathbb{1} \\; \\nabla_\\theta \\log p(y_i, z_i \\mid x_i) &              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\text{;log-derivative trick}                                                                                    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\end{aligned}                                                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m $$                                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Each iteration is equivalent to first selecting the CoT samples according to $\\mathbb{1}$ and then running      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m supervised fine-tuning to optimize the logprob of generating good CoTs and answers. Performance of STaR         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m improves with more training iterations, and the “rationalization” process for generating better CoTs            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m accelerates learning. They observed that sampling with high temperature increases the chance of getting correct \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m answers with incorrect reasoning and finetuning LLM on such data can impair generalization. For datasets        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m without ground truths, majority votes of multiple high-temperature outputs can serve as a proxy of ground truth \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m answers (Wang et al. 2022), making it possible to use synthetic samples for training.                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m A comparison of accuracy of two $n$-digit numbers summation. With rationalization (CoT generation conditioned   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m on ground truth), the model can learn complex arithmetic tasks like $5$-digit summation pretty early on. (Image \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m source: Zelikman et al. 2022)                                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Scaling Laws for Thinking Time#                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m So far we have seen much evidence that allowing models to spend additional compute on reasoning before          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m producing final answers at inference time can significantly improve performance. Techniques like prompting the  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m model to generate intermediate reasoning steps before the answers, or training the model to pause and reflect   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m before predicting next tokens, have been found to boost the model performance beyond the capability limit       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m obtained during training. This essentially introduces a new dimension to tinker with for improving model        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m intelligence, complementing established factors such as model size, training compute and data quantity, as      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m defined in scaling laws (Kaplan et al. 2020).                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Recent studies demonstrated that optimizing LLM test-time compute could be more effective than scaling up model \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m parameters (Snell et al. 2024, Wu et al. 2025). Smaller models combined with advanced inference algorithms can  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m offer Pareto-optimal trade-offs in cost and performance.                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Snell et al. (2024) evaluated and compared test-time and pretraining compute, and found that they are not 1:1   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m exchangeable. Test-time compute can cover up the gap easily on easy and medium questions when there is only a   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m small gap in model capability, but proves less effective for hard problems. The ratio between token budgets for \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m pretraining and inference matters a lot. Test-time compute is only preferable when inference tokens are         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m substantially fewer than pretraining ones. This indicates that developing a capable base model with enough      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m pretraining data and compute is still very critical, as test-time compute cannot solve everything or fill in    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m big model capability gaps.                                                                                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (Left) Evaluation accuracy as a function of test time compute budgets, via iterative revisions or parallel      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m decoding. (Right) Comparing a small model with test-time compute sampling tricks and a 14x larger model with    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m only greedy decoding. We can control the token budgets used at test time such that the ratio of inference to    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m pretraining tokens is << 1, ~=1 or >> 1 and the benefit of test time compute is clear only the ratio << 1.      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (Image source: Snell et al. 2024)                                                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m s1 models (Muennighoff & Yang, et al. 2025) experimented with scaling up the CoT reasoning path length via      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m budget forcing technique (i.e. forcefully lengthen it by appending the word \"wait\", or shorten it by            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m terminating the model’s thinking process by appending end-of-thinking token or \"Final Answer:\"). They observed  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m a clear positive correlation between the average thinking time measured in tokens and the downstream evaluation \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m accuracy.                                                                                                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Both parallel and sequential scaling methods of test-time compute shows positive correlation with the           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m evaluation performance in s1 experiments. (Image Muennighoff & Yang, et al. 2025)                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m When comparing this budget forcing technique with other decoding methods of controlling reasoning trace length, \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m it is quite surprising that simple rejection sampling (i.e. sampling generation until the lengths fits a token  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m budget) leads to reversed scaling, meaning that longer CoTs lead to worse performance.                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (Left) Longer CoT path length is positively correlated with eval accuracy. (Right) Rejection sampling for       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m controlling the generated CoT path length shows a negative scaling where longer CoTs lead to worse eval         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m accuracy. (Image source: Muennighoff & Yang et al. 2025)                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m What’s for Future#                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The exploration of test-time compute and chain-of-thought reasoning presents new opportunities for enhancing    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m model capabilities. More interestingly, via test-time thinking, we are moving towards building future AI        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m systems that mirror the best practices of how humans think, incorporating adaptability, flexibility, critical   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m reflection, and error correction. Excitement with current progress invites us for more future research to       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m improve and understand deeply not just how but why we—and our models—think.                                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m At the end, I would like to call for more research for the following open research questions on test time       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m compute and chain-of-thought reasoning.                                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Can we incentivize the model to produce human-readable, faithful reasoning paths during RL training while       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m avoiding reward hacking behavior?                                                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m How to define reward hacking? Can we capture reward hacking during RL training or inference without human       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m intervention? How to prevent “whack-a-mole” style of fixes for reward hacking during RL training?               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Self-correction can happen within chain-of-thought or can be encouraged to happen explicitly during multi-turn  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m RL. How can we train the model to correct itself without hallucination or regression when ground truth is not   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m available?                                                                                                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m How to run RL training with CoT rollout for tasks that are highly contextualized, personalized and hard to      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m grade, such as creative writing, coaching, brainstorming?                                                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m When we deploy the model in reality, we cannot grow test time thinking forever and how can we smoothly          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m translate the performance gain back into the base model with reduced inference time cost (e.g. via              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m distillation)?                                                                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m How to make test time spending more adaptive according to the difficulty of the problem in hand?                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Filter out CoTs with mixed languages, long paragraphs, and code blocks.                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Include non-reasoning tasks using DeepSeek-V3 (DeepSeek-AI, 2024) pipeline.                                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m For certain non-reasoning tasks, call DeepSeek-V3 to generate potential CoTs before answering the question by   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m prompting. But for simpler queries like “hello”, CoT is not needed.                                             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Then fine-tune the DeepSeek-V3-Base on the total 800k samples for 2 epochs.                                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The final RL stage trains the step 3 checkpoint on both reasoning and non-reasoning prompts, improving          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m helpfulness, harmlessness and reasoning.                                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m DeepSeek-R1 performs comparable to OpenAI o1-preview and o1-mini on several widely used reasoning benchmarks.   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m DeepSeek-V3 is the only non-reasoning model listed. (Image source: DeepSeek-AI, 2025)                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Interestingly the DeepSeek team showed that with pure RL, no SFT stage, it is still possible to learn advanced  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m reasoning capabilities like reflection and backtracking (“Aha moment”). The model naturally learns to spend     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m more thinking tokens during the RL training process to solve reasoning tasks. The “aha moment” can emerge,      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m referring to the model reflecting on previous mistakes and then trying alternative approaches to correct them.  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Later, various open source efforts happened for replicating R1 results like Open-R1, SimpleRL-reason, and       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m TinyZero, all based on Qwen models. These efforts also confirmed that pure RL leads to great performance on     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m math problems, as well as the emergent “aha moment”.                                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Examples of the model learning to reflect and correct mistakes. (Image source: (left) DeepSeek-AI, 2025;        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (right) Zeng et al. 2025)                                                                                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The DeepSeek team also shared some of their unsuccessful attempts. They failed to use process reward model      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (PRM) as it is hard to define per-step rubrics or determine whether an intermediate step is correct, meanwhile  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m making the training more vulnerable to reward hacking. The efforts on MCTS (Monte Carlo Tree Search) also       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m failed due to the large search space for language model tokens, in comparison to, say, chess; and training the  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m fine-grained value model used for guiding the search is very challenging too. Failed attempts often provide     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m unique insights and we would like to encourage the research community to share more about what did not work     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m out.                                                                                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m External Tool Use#                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m During the reasoning steps, certain intermediate steps can be reliably and accurately solved by executing code  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m or running mathematical calculations. Offloading that part of reasoning components into an external code        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m interpreter, as in PAL (Program-Aided Language Model; Gao et al. 2022) or Chain of Code (Li et al. 2023), can   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m extend the capability of LLM with external tools, eliminating the need for LLMs to learn to execute code or     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m function as calculators themselves. These code emulators, like in Chain of Code, can be augmented by an LLM     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m such that if a standard code interpreter fails, we have the option of using LLM to execute that line of code    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m instead. Using code to enhance reasoning steps are especially beneficial for mathematical problems, symbolic    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m reasoning and algorithmic tasks. These unit tests may not exist as part of the coding questions, and in those   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m cases, we can instruct the model to self-generate unit tests for it to test against to verify the solution      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (Shinn, et al. 2023).                                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m An example of program-aided language model prompting looks like. (Image source: Gao et al. 2022)                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m ReAct (Reason+Act; Yao et al. 2023) combines the action of searching the Wikipedia API and generation of        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m reasoning traces, such that reasoning paths can incorporate external knowledge.                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m An example of the ReAct prompting method to solve a HotpotQA question, using Wikipedia search API as an         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m external tool to help with reasoning. (Image source: Yao et al. 2023)                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m o3 & o4-mini, recently released by OpenAI, are another two good examples where the reasoning process involves   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m tool use like Web search, code execution and image processing. The team observed that large-scale reinforcement \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m learning exhibits the same trend as in the GPT paradigm that “more compute = better performance”.               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Thinking Faithfully#                                                                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Deep learning models are often treated as black boxes and various interpretability methods have been proposed.  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Interpretability is useful for a couple reasons: first, it gives us an extra test to determine if the model is  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m misaligned with its creators’ intent, or if it’s misbehaving in some way that we can’t tell by monitoring its   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m actions. Second, it can help us determine whether the model is using a sound process to compute its answers.    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Chain of thought provides an especially convenient form of interpretability, as it makes the model’s internal   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m process visible in natural language. This interpretability, however, rests on the assumption that the model     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m truthfully describes its internal thought processes.                                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Recent work showed that monitoring CoT of reasoning models can effectively detect model misbehavior such as     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m reward hacking, and can even enable a weaker model to monitor a stronger model (Baker et al. 2025). Increasing  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m test time compute can also lead to improved adversarial robustness (Zaremba et al. 2025); this makes sense      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m intuitively, because thinking for longer should be especially useful when the model is presented with an        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m unusual input, such as an adversarial example or jailbreak attempt – it can use the extra thinking time to make \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m sense of the strange situation it’s been presented with.                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The experiment of asking the model to decide if another model tried to hack the unit tests in some way for      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m coding questions given its thought process. We can monitor these reward hacking behavior during training with   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m different types of monitor. The exit(0) coding hack is when the agent exploited a bug that allowed it to exit   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m from the environment early without running all unit tests. The raise SkipTest hack is when the agent raises an  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m exception from functions outside the testing framework in order to skip unit test evaluation. (Image source:    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Baker et al. 2025)                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Does the Model Tell What it Thinks Faithfully#                                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Intuitively, model CoTs could be biased due to lack of explicit training objectives aimed at encouraging        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m faithful reasoning. Or when we fine-tune the model on human-written explanations, those human-written samples   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m may contain mistakes. Thus we cannot by default assume CoT is always faithful .                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Lanham et al. (2023) investigated several modes of CoT faithfulness failures by deliberately introducing        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m mistakes into CoTs and measuring their impacts on the accuracy of a set of multiple choice tasks (e.g. AQuA,    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m MMLU, ARC Challenge, TruthfulQA, HellaSwag):                                                                    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Mistake 1 (Early answering): The model may form a conclusion prematurely before CoT is generated. This is       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m tested by early truncating or inserting mistakes into CoT. Different tasks revealed varying task-specific       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m dependencies on CoT effectiveness; some have evaluation performance sensitive to truncated CoT but some do not. \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Wang et al. (2023) did similar experiments but with more subtle mistakes related to bridging objects or         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m language templates in the formation of CoT.                                                                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Mistake 2 (Uninformative tokens): Uninformative CoT tokens improve performance. This hypothesis is tested by    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m replacing CoT with filler text (e.g. all periods) and this setup shows no accuracy increase and some tasks may  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m suffer performance drop slightly when compared to no CoT.                                                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Mistake 3 (Human-unreadable encoding): Relevant information is encoded in a way that is hard for humans to      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m understand. Paraphrasing CoTs in an non-standard way did not degrade performance across datasets, suggesting    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m accuracy gains do not rely on human-readable reasoning.                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Illustration of different ways of CoT perturbation to assess its faithfulness. (Image source: Lanham et al.     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m 2023)                                                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Interestingly, Lanham et al. suggests that for multiple choice questions, smaller models may not be capable     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m enough of utilizing CoT well, whereas larger models may have been able to solve the tasks without CoT. This     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m dependency on CoT reasoning, measured by the percent of obtaining the same answer with vs without CoT, does not \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m always increase with model size on multiple choice questions, but does increase with model size on addition     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m tasks, implying that thinking time matters more for complex reasoning tasks.                                    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The dependency on CoT reasoning is measured as the percentage of obtaining same answers with vs without CoT. It \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m matters more for reasoning tasks like addition and larger models benefit more. (Image source: Lanham et al.     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m 2023)                                                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Illustration of parallel sampling vs sequential revision.                                                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Parallel Sampling#                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Given a generative model and a scoring function that we can use to score full or partial samples, there are     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m various search algorithms we can use to find a high scoring sample. Best-of-$N$ is the simplest such algorithm: \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m one just collects $N$ independent samples and chooses the highest-ranking sample according to some scoring      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m function. Beam search is a more sophisticated search algorithm that makes the search process more adaptive,     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m spending more sampling computation on more promising parts of the solution space.                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Beam search maintains a set of promising partial sequences and alternates between extending them and pruning    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m the less promising ones. As a selection mechanism, we can use a process reward model (PRM; Lightman et al.      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m 2023) to guide beam search candidate selection. Xie et al. (2023) used LLM to evaluate how likely its own       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m generated reasoning step is correct, formatted as a multiple-choice question and found that per-step            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m self-evaluation reduces accumulative errors in multi-step reasoning during beam search decoding. Besides,       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m during sampling, annealing the temperature helps mitigate aggregated randomness. These experiments by Xie et    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m al. achieved 5-6% improvement on few-shot GSM8k, AQuA and StrategyQA benchmarks with the Codex model. Reward    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m balanced search (short for “REBASE”; Wu et al. 2025) separately trained a process reward model (PRM) to         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m determine how much each node should be expanded at each depth during beam search, according to the              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m softmax-normalized reward scores. Jiang et al. (2024) trained their PRM, named “RATIONALYST”, for beam search   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m guidance on synthetic rationales conditioned on a large amount of unlabelled data. Good rationales are filtered \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m based on whether they help reduce the neg log-prob of true answer tokens by a threshold, when comparing the     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m difference between when the rationales is included in the context vs not. At inference time, RATIONALYST        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m provides process supervision to the CoT generator by helping estimate log-prob of next reasoning steps          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (“implicit”) or directly generating next reasoning steps as part of the prompt (“explicit”).                    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Beam search decoding guided by LLM self-evaluation per reasoning step. (Image source: Xie et al. 2023)          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Interestingly, it is possible to trigger the emergent chain-of-thought reasoning paths without explicit         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m zero-shot or few-shot prompting. Wang & Zhou (2024) discovered that if we branch out at the first sampling      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m tokens by retaining the top $k$ tokens with highest confidence, measured as the difference between top-1 and    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m top-2 candidates during sampling, and then continue these $k$ sampling trials with greedy decoding onward, many \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m of these sequences natively contain CoT. Especially when CoT does appear in the context, it leads to a more     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m confident decoding of the final answer. To calculate the confidence of the final answer, the answer span needs  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m to be identified by task-specific heuristics (e.g. last numerical values for math questions) or  by prompting   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m the model further with \"So the answer is\". The design choice of only branching out at the first token is based  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m on the observation that early branching significantly enhances the diversity of potential paths, while later    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m tokens are influenced a lot by previous sequences.                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Top-$k$ decoding, $k$ refers to the number of candidates at the first sampling step. (Image source: Wang &      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Zhou, 2024)                                                                                                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Sequential Revision#                                                                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m If the model can reflect and correct mistakes in past responses, we would expect the model to produce a nice    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m sequence of iterative revision with increasing quality. However, this self-correction capability turns out to   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m not exist intrinsically among LLMs and does not easily work out of the box, due to various failure modes, such  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m as, (1) hallucination, including modifying correct responses to be incorrect; (2) behavior collapse to          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m non-correcting behavior; e.g. making minor or no modification on the first incorrect responses; or (3) fail to  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m generalize to distribution shift at test time. Experiments by Huang et al. (2024) showed that naively applying  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m self-correction leads to worse performance and external feedback is needed for models to self improve, which    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m can be based on matching ground truths, heuristics and task-specific metrics, unit tests results for coding     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m questions (Shinn, et al. 2023), a stronger model (Zhang et al. 2024), as well as human feedback (Liu et al.     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m 2023).                                                                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Self-correction learning (Welleck et al. 2023) aims to train a corrector model $P_\\theta(y \\mid y_0, x)$ given  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m a fixed generator model $P_0(y_0 \\mid x)$. While the generator model remains to be generic, the corrector model \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m can task-specific and only does generation conditioned on an initial model response and additional feedback     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (e.g. a sentence, a compiler trace, unit test results; can be optional):                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Self-correction learning first generates first generates multiple outputs per prompt in the data pool;          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m then create value-improving pairs by pairing two outputs for the same prompt together if one has a higher value \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m than the other, (prompt $x$, hypothesis $y$, correction $y’$).                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m These pairs are selected proportional to is improvement in value, $v(y’) - v(y)$, and similarity between two    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m outputs, $\\text{Similarity}(y, y’)$ to train the corrector model.                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m To encourage exploration, the corrector provides new generations into the data pool as well. At the inference   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m time, the corrector can be used iteratively to create a correction trajectory of sequential revision.           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Illustration of self-correction learning by matching model outputs for the same problem to form value-improving \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m pairs to train a correction model. (Image source: Welleck et al. 2023)                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Recursive inspection (Qu et al. 2024) also aims to train a better corrector model but with a single model to do \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m both generation and self-correction.                                                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m SCoRe (Self-Correction via Reinforcement Learning; Kumar et al. 2024) is a multi-turn RL approach to encourage  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m the model to do self-correction by producing better answers at the second attempt than the one created at the   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m first attempt. It composes two stages of training: stage 1 only maximizes the accuracy of the second attempt    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m while enforcing a KL penalty only on the first attempt to avoid too much shifting of the first-turn responses   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m from the base model behavior; stage 2 optimizes the accuracy of answers produced by both the first and second   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m attempts. Ideally we do want to see performance at both first and second attempts to be better, but adding      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m stage 1 prevents the behavior collapse where the model does minor or none edits on the first response, and      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m stage 2 further improves the results.                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Explicit training setup to improve self-correction capabilities by doing two-staged RL training. (Image source: \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Kumar et al. 2024)                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m RL for Better Reasoning#                                                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m There’s been a lot of recent success in using RL to improve the reasoning ability of language models, by using  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m a collection of questions with ground truth answers (usually STEM problems and puzzles with easy to verify      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m answers), and rewarding the model for getting the correct answer.Recent activity in this area was spurred by    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m strong performance of the o-series models from OpenAI, and the subsequent releases of models and tech reports   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m from DeepSeek.                                                                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m DeepSeek-R1 (DeepSeek-AI, 2025) is an open-source LLM designed to excel in tasks that require advanced          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m reasoning skills like math, coding and logical problem solving. They run through 2 rounds of SFT-RL training,   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m enabling R1 to be good at both reasoning and non-reasoning tasks.                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Cold-start SFT is to fine-tune the DeepSeek-V3-Base base model on a collection of thousands of cold-start data. \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Without this step, the model has issues of poor readability and language mixing.                                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reasoning-oriented RL trains a reasoning model on reasoning-only prompts with two types of rule-based rewards:  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Format rewards: The model should wrap CoTs by <thinking> ... </thinking> tokens.                                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Accuracy rewards: Whether the final answers are correct. The answer for math problems needs to be present in a  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m specific format (e.g. in a box) to be verified reliably. For coding problems, a compiler is used to evaluate    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m whether test cases pass.                                                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Rejection-sampling + non-reasoning SFT utilizes new SFT data created by rejection sampling on the RL checkpoint \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m of step 2, combined with non-reasoning supervised data from DeepSeek-V3 in domains like writing, factual QA,    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m and self-cognition, to retrain DeepSeek-V3-Base.                                                                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The dependency on CoT reasoning is measured as the percentage of obtaining same answers with vs without CoT. It \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m matters more for reasoning tasks like addition and larger models benefit more. (Image source: Lanham et al.     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m 2023)                                                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Alternative approaches for testing CoT faithfulness involve perturbing prompts rather than modifying CoT paths  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m directly (Turpin et al. 2023, Chua & Evans, 2025, Chen et al. 2025).                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m One method consistently labels correct answers as “(A)” in few-shot examples regardless of true labels to       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m introduce biases.                                                                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Another prompting technique inserts misleading hints into prompts, such as \"I think the answer is               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m <random_label> but curious to hear what you think\". or \"A Stanford Professor thinks the answer is               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m <random_label>\". By comparing model predictions for the same question with vs without the misleading hint, we   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m can measure whether a model is able to faithfully describe the influence of the hint on its answer.             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Particularly, in cases where the model produces different hint and non-hint answers, we measure whether the     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m model acknowledges the hint when solving the question with hint. If the model is faithful, it should explicitly \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m acknowledge the impact and admit the change of its answer is due to the hint.                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Both GPT and Claude models are sensitive to different types of biases in context. Decrease in model accuracy    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m indicates systematic unfaithfulness. Direct hints of wrong labels are more effectively than \"Answer is always   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m A\" type of bias. (Image source: Turpin et al. 2023)                                                             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Multiple studies found that reasoning models describe the influence of the hint much more reliably than all the \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m non-reasoning models tested. For example, we can measure the fraction of samples where the model acknowledges   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m the hint as a deciding factor (“faithful CoT”). Reasoning models (Claude 3.7 Sonnet, DeepSeek R1) are overall   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m doing better than non-reasoning ones (Claude 3.6, DeepSeek V3).                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reasoning models are more likely to reveal faithful CoT than non-reasoning models. (Image source: Chen et al.   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m 2025)                                                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Some evidence showed that using reward models leads to less faithfulness in model outputs. The reward model in  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m classic RLHF is not trained to incentivize faithful behavior in this test as the preference comparison dataset  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m is collected by humans selecting which one seems better or pleasing. However, in reasoning models, the CoT      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m paths are optimized for producing the correct final answers, not to match human preference defined in RM, and   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m thus intuitively are expected to be more faithful.                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Optimization Pressure on CoT: Good or Bad?#                                                                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Monitoring CoT of reasoning models for reward hacking behavior is effective. A simple way to use this monitor   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m is to run rejection sampling with the monitor as a filter at test time and identify solutions without reward    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m hacking. However, it only works at the early stage of RL, and, as the model is further optimized, it is rare to \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m find a sample to pass a CoT monitor within budget. This challenge naturally suggests that we could consider     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m incorporating CoT monitors into RL rewards—a form of process-based supervision—to prevent reward hacking.       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m However, incorporating CoT monitoring reward into RL does not work out well, as it leads to obfuscated reward   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m hacking where the model tries to hide its true intent within the CoT (Baker et al. 2025). With this direct      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m optimization pressure provided by the CoT monitor applied during RL training, the model still can find ways to  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m hack, although a bit less frequently, but can hide its intention in CoT to avoid being caught by the monitor.   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m When using the monitor to provide an RL reward signal to prevent reward hacking during training, the model      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m still can find ways to hack while avoiding being caught by the monitor. (Image source: Baker et al. 2025)       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Experiments on CoT length RL signals by Yeo et al. (2025) confirmed a similar message as Baker et al. that new  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m types of reward hacking can happen with new types of RL reward shaping. They designed the reward function such  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m that correct CoTs can have higher rewards than wrong ones, short correct CoTs obtain higher rewards than long   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m correct ones and short incorrect CoT receive higher penalties than long incorrect CoTs. With this extra reward, \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m the model learns to repeat text in CoTs for challenging questions rather than attempting to solve them. Yeo et  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m al. further applied a n-gram repetition penalty to prevent this hacking behavior.                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Careful reward shaping is needed to stabilize training with CoT length rewards. (Image source: Yeo et al. 2025) \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Chen et al. (2025) experimented with a flawed RL environment, specifically using a grader with incorrect        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m answers filled in for multiple-choice questions. The model learns to exploit the reward hack on >99% of the     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m prompts, but almost never (<2%) verbalizes the reward hack in its CoT on more than half of their environments.  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Additional RL optimization pressure fails to incentivize the model to verbalize the hack in this case.          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m RL training is inherently sensitive to reward hacking. Only relying on heuristic investigation of reward        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m hacking and manual fixes may lead to a “whack-a-mole” situation. We would suggest being very cautious when      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m trying to apply optimization directly on CoT during RL training, or trying to avoid it altogether.              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Thinking in Continuous Space#                                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Adaptive Computation Time, introduced by Alex Graves in 2016, predated large language models but pioneered the  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m same direction of enabling the model to dynamically decide the number of computational steps to take at the     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m inference time, which can be viewed as enabling the model to “think more” in continuous space at test time.     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Adaptive thinking time in continuous space can be enabled vertically via recurrent architecture or horizontally \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m via more sequential sampling steps.                                                                             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Recurrent Architecture#                                                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m A number of architecture variations have been proposed to make the Transformer architecture recurrent, enabling \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m adaptive test time compute (Dehghani, et al. 2019, Hutchins, et al. 2022, Bulatov, et al. 2022). A deep dive    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m into literature on this topic would make the post too long, so we will only review a few.                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Universal Transformer (Dehghani, et al. 2019) combines self-attention in Transformer with the recurrent         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m mechanism in RNN, dynamically adjusting the number of steps using adaptive computation time (Graves, 2016). On  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m a high level, it can be viewed as a recurrent function for learning the hidden state representation per token,  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m and if the number of steps is fixed, an Universal Transformer is equivalent to a multi-layer Transformer with   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m shared parameters across layers.                                                                                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m A recent recurrent architecture design, proposed by Geiping et al. (2025), adds a recurrent block $R$ on top of \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m the standard Transformer. Every iteration of this recurrent block takes the embedding $\\mathbf{e}$ and a random \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m state $\\mathbf{s}_i$. Conceptually, this recurrent-depth architecture is a bit similar to a conditioned         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m diffusion model, where the original input $\\mathbf{e}$ is provided in every recurrent step while a random       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Gaussian initialized state $\\mathbf{s}_i$ gets updated iteratively through the process. (Interestingly some of  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m their experiments of designs that resemble diffusion models more turned out to be bad.)                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m $$                                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\begin{aligned}                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\mathbf{e} &= P(\\mathbf{x}) & \\text{embedding} \\\\ \\mathbf{s}\\_0 &\\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 I\\_{n    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\cdot h}) \\\\ \\mathbf{s}\\_i &= R(\\mathbf{e}, \\mathbf{s}\\_{i-1}) \\quad\\text{ for }i \\in {1, \\dots, r} &           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\small{\\text{recurrent block; resembles a Transformer block}}\\\\ \\mathbf{p} &= C(\\mathbf{s}\\_r) &                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\text{unembedding}                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\end{aligned}                                                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m $$                                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The recurrence count $r$ during training is randomized, sampled from a log-normal Poisson distribution, per     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m input sequence. To manage computational costs, backpropagation is truncated to only the last $k$ iterations of  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m the recurrent unit ($k=8$ in experiments), making it possible to train on the heavy-tail part of the Poisson    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m distribution. The embedding block continues to receive gradient updates in every step since its output          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m $\\mathbf{e}$ is injected in every step, mimicking RNN training. Unsurprisingly, the stability of training a     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m recurrent model turns out to be very sensitive. Factors like initialization, normalization and hyperparameters  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m all matter, especially when scaling up the training. For example, hidden states can collapse by predicting the  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m same hidden state for every token; or the model may learn to ignore the incoming state $\\mathbf{s}$. To         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m stabilize the training, Geiping et al. adopted an embedding scale factor, a small learning rate and careful     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m tuning.                                                                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Plot of an experiment on training a 3.5B model with depth recurrence. The saturation roughly happens around     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m $\\bar{r} = 32$, making us wonder how this architecture extrapolate and generalize to larger iteration counts.   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (Image source: Geiping et al. 2025)                                                                             \u001b[33m│\u001b[0m\n",
       "\u001b[33m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">╭───────────────────────────────────────────────────── 📝 AI ─────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> Let me search for more information about the specific mechanisms and benefits of RL for reasoning.              <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> 🔧 Tool Call: retrieve_blog_posts                                                                               <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>    Args: {                                                                                                      <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>   \"query\": \"RL training accuracy rewards format rewards DeepSeek reasoning benefits\"                            <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> }                                                                                                               <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37m╭─\u001b[0m\u001b[37m────────────────────────────────────────────────────\u001b[0m\u001b[37m 📝 AI \u001b[0m\u001b[37m────────────────────────────────────────────────────\u001b[0m\u001b[37m─╮\u001b[0m\n",
       "\u001b[37m│\u001b[0m Let me search for more information about the specific mechanisms and benefits of RL for reasoning.              \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m 🔧 Tool Call: retrieve_blog_posts                                                                               \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m    Args: {                                                                                                      \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m   \"query\": \"RL training accuracy rewards format rewards DeepSeek reasoning benefits\"                            \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m }                                                                                                               \u001b[37m│\u001b[0m\n",
       "\u001b[37m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">╭──────────────────────────────────────────────── 🔧 Tool Output ─────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Illustration of parallel sampling vs sequential revision.                                                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Parallel Sampling#                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Given a generative model and a scoring function that we can use to score full or partial samples, there are     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> various search algorithms we can use to find a high scoring sample. Best-of-$N$ is the simplest such algorithm: <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> one just collects $N$ independent samples and chooses the highest-ranking sample according to some scoring      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> function. Beam search is a more sophisticated search algorithm that makes the search process more adaptive,     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> spending more sampling computation on more promising parts of the solution space.                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Beam search maintains a set of promising partial sequences and alternates between extending them and pruning    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> the less promising ones. As a selection mechanism, we can use a process reward model (PRM; Lightman et al.      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> 2023) to guide beam search candidate selection. Xie et al. (2023) used LLM to evaluate how likely its own       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> generated reasoning step is correct, formatted as a multiple-choice question and found that per-step            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> self-evaluation reduces accumulative errors in multi-step reasoning during beam search decoding. Besides,       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> during sampling, annealing the temperature helps mitigate aggregated randomness. These experiments by Xie et    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> al. achieved 5-6% improvement on few-shot GSM8k, AQuA and StrategyQA benchmarks with the Codex model. Reward    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> balanced search (short for “REBASE”; Wu et al. 2025) separately trained a process reward model (PRM) to         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> determine how much each node should be expanded at each depth during beam search, according to the              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> softmax-normalized reward scores. Jiang et al. (2024) trained their PRM, named “RATIONALYST”, for beam search   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> guidance on synthetic rationales conditioned on a large amount of unlabelled data. Good rationales are filtered <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> based on whether they help reduce the neg log-prob of true answer tokens by a threshold, when comparing the     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> difference between when the rationales is included in the context vs not. At inference time, RATIONALYST        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> provides process supervision to the CoT generator by helping estimate log-prob of next reasoning steps          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (“implicit”) or directly generating next reasoning steps as part of the prompt (“explicit”).                    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Beam search decoding guided by LLM self-evaluation per reasoning step. (Image source: Xie et al. 2023)          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Interestingly, it is possible to trigger the emergent chain-of-thought reasoning paths without explicit         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> zero-shot or few-shot prompting. Wang &amp; Zhou (2024) discovered that if we branch out at the first sampling      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> tokens by retaining the top $k$ tokens with highest confidence, measured as the difference between top-1 and    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> top-2 candidates during sampling, and then continue these $k$ sampling trials with greedy decoding onward, many <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> of these sequences natively contain CoT. Especially when CoT does appear in the context, it leads to a more     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> confident decoding of the final answer. To calculate the confidence of the final answer, the answer span needs  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> to be identified by task-specific heuristics (e.g. last numerical values for math questions) or  by prompting   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> the model further with \"So the answer is\". The design choice of only branching out at the first token is based  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> on the observation that early branching significantly enhances the diversity of potential paths, while later    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> tokens are influenced a lot by previous sequences.                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Top-$k$ decoding, $k$ refers to the number of candidates at the first sampling step. (Image source: Wang &amp;      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Zhou, 2024)                                                                                                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Sequential Revision#                                                                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> If the model can reflect and correct mistakes in past responses, we would expect the model to produce a nice    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> sequence of iterative revision with increasing quality. However, this self-correction capability turns out to   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> not exist intrinsically among LLMs and does not easily work out of the box, due to various failure modes, such  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> as, (1) hallucination, including modifying correct responses to be incorrect; (2) behavior collapse to          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> non-correcting behavior; e.g. making minor or no modification on the first incorrect responses; or (3) fail to  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> generalize to distribution shift at test time. Experiments by Huang et al. (2024) showed that naively applying  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> self-correction leads to worse performance and external feedback is needed for models to self improve, which    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> can be based on matching ground truths, heuristics and task-specific metrics, unit tests results for coding     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> questions (Shinn, et al. 2023), a stronger model (Zhang et al. 2024), as well as human feedback (Liu et al.     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> 2023).                                                                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Self-correction learning (Welleck et al. 2023) aims to train a corrector model $P_\\theta(y \\mid y_0, x)$ given  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> a fixed generator model $P_0(y_0 \\mid x)$. While the generator model remains to be generic, the corrector model <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> can task-specific and only does generation conditioned on an initial model response and additional feedback     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (e.g. a sentence, a compiler trace, unit test results; can be optional):                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Self-correction learning first generates first generates multiple outputs per prompt in the data pool;          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> then create value-improving pairs by pairing two outputs for the same prompt together if one has a higher value <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> than the other, (prompt $x$, hypothesis $y$, correction $y’$).                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> These pairs are selected proportional to is improvement in value, $v(y’) - v(y)$, and similarity between two    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> outputs, $\\text{Similarity}(y, y’)$ to train the corrector model.                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> To encourage exploration, the corrector provides new generations into the data pool as well. At the inference   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> time, the corrector can be used iteratively to create a correction trajectory of sequential revision.           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Illustration of self-correction learning by matching model outputs for the same problem to form value-improving <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> pairs to train a correction model. (Image source: Welleck et al. 2023)                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Recursive inspection (Qu et al. 2024) also aims to train a better corrector model but with a single model to do <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> both generation and self-correction.                                                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> SCoRe (Self-Correction via Reinforcement Learning; Kumar et al. 2024) is a multi-turn RL approach to encourage  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> the model to do self-correction by producing better answers at the second attempt than the one created at the   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> first attempt. It composes two stages of training: stage 1 only maximizes the accuracy of the second attempt    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> while enforcing a KL penalty only on the first attempt to avoid too much shifting of the first-turn responses   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> from the base model behavior; stage 2 optimizes the accuracy of answers produced by both the first and second   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> attempts. Ideally we do want to see performance at both first and second attempts to be better, but adding      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> stage 1 prevents the behavior collapse where the model does minor or none edits on the first response, and      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> stage 2 further improves the results.                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Explicit training setup to improve self-correction capabilities by doing two-staged RL training. (Image source: <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Kumar et al. 2024)                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> RL for Better Reasoning#                                                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> There’s been a lot of recent success in using RL to improve the reasoning ability of language models, by using  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> a collection of questions with ground truth answers (usually STEM problems and puzzles with easy to verify      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> answers), and rewarding the model for getting the correct answer.Recent activity in this area was spurred by    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> strong performance of the o-series models from OpenAI, and the subsequent releases of models and tech reports   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> from DeepSeek.                                                                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> DeepSeek-R1 (DeepSeek-AI, 2025) is an open-source LLM designed to excel in tasks that require advanced          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> reasoning skills like math, coding and logical problem solving. They run through 2 rounds of SFT-RL training,   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> enabling R1 to be good at both reasoning and non-reasoning tasks.                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Cold-start SFT is to fine-tune the DeepSeek-V3-Base base model on a collection of thousands of cold-start data. <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Without this step, the model has issues of poor readability and language mixing.                                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reasoning-oriented RL trains a reasoning model on reasoning-only prompts with two types of rule-based rewards:  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Format rewards: The model should wrap CoTs by &lt;thinking&gt; ... &lt;/thinking&gt; tokens.                                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Accuracy rewards: Whether the final answers are correct. The answer for math problems needs to be present in a  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> specific format (e.g. in a box) to be verified reliably. For coding problems, a compiler is used to evaluate    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> whether test cases pass.                                                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Rejection-sampling + non-reasoning SFT utilizes new SFT data created by rejection sampling on the RL checkpoint <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> of step 2, combined with non-reasoning supervised data from DeepSeek-V3 in domains like writing, factual QA,    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> and self-cognition, to retrain DeepSeek-V3-Base.                                                                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Hacking RL Environment#                                                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reward hacking is expected to be a more common problem as the model and the algorithm become increasingly       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> sophisticated. A more intelligent agent is more capable of finding “holes” in the design of reward function and <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> exploiting the task specification—in other words, achieving higher proxy rewards but lower true rewards. By     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> contrast, a weaker algorithm may not be able to find such loopholes, and thus we would not observe any reward   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> hacking or identify issues in the current reward function design when the model is not strong enough.           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> In a set of zero-sum robotics self-play games (Bansal et al., 2017), we can train two agents (victim vs.        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> opponent) to compete against each other. A standard training process produces a victim agent with adequate      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> performance when playing against a normal opponent. However, it is easy to train an adversarial opponent policy <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> that can defeat the victim reliably despite outputting seemingly random actions and training with fewer than 3% <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> of time steps (Gleave et al., 2020). Training of adversarial policies involves optimizing the sum of discounted <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> rewards, as in standard RL setup, while treating the victim policy as a black-box model.                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> An intuitive way to mitigate adversarial policies attacks is to fine-tune victims against adversarial policies. <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> However, the victim remains vulnerable to new versions of adversarial policies once retrained against the new   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> victim policy.                                                                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Why does adversarial policy exist? The hypothesis is that adversarial policies introduce OOD observations to    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> the victim rather than physically interfering with it. Evidence shows that when the victim’s observation of the <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> opponent’s position is masked and set to a static state, the victim becomes more robust to adversaries,         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> although performing worse against a normal opponent policy. Furthermore, a higher-dimensional observation space <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> enhances performance under normal circumstances but makes the policy more vulnerable to adversarial opponents.  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Pan et al. (2022) investigated reward hacking as a function of agent capabilities, including (1) model size,    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (2) action space resolution, (3) observation space noise, and (4) training time. They also proposed a taxonomy  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> of three types of misspecified proxy rewards:                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Misweighting: Proxy and true rewards capture the same desiderata, but differ in their relative importance.      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Ontological: Proxy and true rewards use different desiderata to capture the same concept.                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Scope: The proxy measures desiderata over a restricted domain (e.g. time or space) because measurement across   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> all conditions is too costly.                                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> They experimented in four RL environments paired with nine misspecified proxy rewards. The overall findings     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> from these experiments can be summarized as follows: A model of higher capability tends to obtain higher (or    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> similar) proxy rewards but decreased true rewards.                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Model size: Larger model size leads to increased proxy rewards but decreased true rewards.                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Action space resolution: Increased precision in actions leads to more capable agents. However, higher           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> resolution causes proxy rewards to remain constant while true rewards decrease.                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Observation fidelity: More accurate observations improve proxy rewards but slightly reduce true rewards.        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Training steps: Optimizing the proxy reward over more steps harms true rewards after an initial period where    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> the rewards are positively correlated.                                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The plot of proxy and true reward value as functions of (Top row) model sizes, measured in parameter count;     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (Bottom row) model capability, measured by metrics such as training steps, action space resolution, and         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> observation noise. (Image source: Pan et al. 2022)                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> If a proxy reward is so poorly specified that it has a very weak correlation with the true reward, we may be    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> able to identify and prevent reward hacking even before training. Based on this hypothesis, Pan et al. (2022)   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> investigated the correlation between proxy and true rewards over a collection of trajectory rollouts.           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Interestingly, reward hacking still occurs even when there is a positive correlation between the true and proxy <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> rewards.                                                                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Hacking RLHF of LLMs#                                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reinforcement learning from human feedback (RLHF) has become the de facto approach for alignment training of    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> language models. A reward model is trained on human feedback data and then a language model is fine-tuned via   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> RL to optimize this proxy reward for human preference. There are three types of reward we care about in an RLHF <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> setup:                                                                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (1) Oracle/Gold reward $R^∗$ represents what we truly want the LLM to optimize.                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (2) Human reward $R^\\text{human}$ is what we collect to evaluate LLMs in practice, typically from individual    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> humans with time constraints. Because humans can provide inconsistent feedback or make mistakes, human reward   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> is not a fully accurate representation of the oracle reward.                                                    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (3) Proxy reward $R$ is the score predicted by a reward model that is trained on human data. Hence,             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> $R^\\text{train}$ inherits all the weakness of human reward, plus potential modeling biases.                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> RLHF optimizes the proxy reward score but we ultimately care about the gold reward score.                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Hacking the Training Process#                                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Gao et al. (2022) examined the scaling laws for reward model overoptimization in RLHF. To scale up the human    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> labels in their experiments, they use a synthetic data setup where the “gold” label for the oracle reward $R^*$ <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> is approximated by a large RM (6B parameters) where the proxy RMs for $R$ range in size of 3M to 3B parameters. <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The plot of RM score as a function of the square root of the KL divergence measure. The proxy reward is shown   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> with a dashed line, and the gold reward is shown with a solid line. (Image source: Gao et al. 2022)             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The KL divergence from the initial policy to the optimized policy is $\\text{KL} = D_\\text{KL}(\\pi |             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\pi_\\text{init})$, and the distance function is defined as $d := \\sqrt{ D_\\text{KL}(\\pi | \\pi_\\text{init})}$.   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> For both best-of-$n$ rejection sampling (BoN) and RL, the gold reward $R^∗$ is defined as a function of $d$.    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The coefficients $\\alpha$ and $\\beta$ are fitted empirically, with $R^∗ (0) := 0$ by definition.                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The authors also attempted to fit the proxy reward $R$ but found systematic underestimation when extrapolated   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> to higher KLs, as the proxy reward appeared to grow linearly with $d$.                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> $$                                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\begin{aligned}                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> R^*_{\\text{bo}n}(d) &amp;= d (\\alpha_{\\text{bo}n} - \\beta_{\\text{bo}n} d) &amp; \\text{; for best-of-n (BoN)             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> sampling.}\\\\                                                                                                    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> R^*_\\text{RL}(d) &amp;= d (\\alpha_\\text{RL} - \\beta_\\text{RL} \\log d) &amp; \\text{; for reinforcement learning}\\\\       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \\end{aligned}                                                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> $$                                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The coefficient parameters, $\\alpha_{\\text{bo}n}, \\beta_{\\text{bo}n}, \\beta_\\text{RL}$ are empirically fit      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> according to data, displayed as functions of the reward model size. The coefficient $\\alpha_\\text{RL}$ is not   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> included here because it remains constant across RM sizes. (Image source: Gao et al. 2022)                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Their experiments also explored the relationship between RM overoptimization and factors like policy model size <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> and RM data size:                                                                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Larger policies see less benefit from optimization (i.e., the difference between initial and peak rewards is    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> smaller than that of a smaller policy) against an RM, but also overoptimize less.                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> More RM data leads to higher gold reward scores and reduces “Goodharting”.                                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The effect of the KL penalty on the gold score resembles early stopping. Note that in all experiments except    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> this one, the KL penalty in PPO is set to 0, because they observed that using a KL penalty strictly increases   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> the proxy-gold reward gap.                                                                                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> RLHF aims to improve the model’s alignment with human preference, but human feedback $R^\\text{human}$ may not   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> capture all the aspects we care about (e.g., factuality) and thus can be hacked to overfit to undesired         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> attributes. For example, the model may be optimized to output responses that seem correct and convincing but    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> are, in fact, inaccurate, thereby misleading human evaluators to approve its incorrect answers more often (Wen  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> et al., 2024). In other words, a gap emerges between what is correct and what looks correct to humans due to    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> RLHF. Precisely Wen et al. (2024) ran RLHF experiments using a reward model based on ChatbotArena data. They    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> evaluated the model on a question-answering dataset, QuALITY and a programming dataset, APPS. Their experiments <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> revealed that models become better at convincing humans they are correct, even when they are wrong and this     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> effect is unintended:                                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> RLHF increases human approval, but not necessarily correctness.                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> RLHF weakens humans’ ability to evaluate: The error rate of human evaluation is higher after RLHF training.     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> RLHF makes incorrect outputs more convincing to humans. The evaluation false positive rate significantly        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> increases after RLHF training.                                                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Or                                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> @article{weng2024rewardhack,                                                                                    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>   title   = \"Reward Hacking in Reinforcement Learning.\",                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>   author  = \"Weng, Lilian\",                                                                                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>   journal = \"lilianweng.github.io\",                                                                             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>   year    = \"2024\",                                                                                             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>   month   = \"Nov\",                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>   url     = \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\"                                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> }                                                                                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> References#                                                                                                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> [1] Andrew Ng &amp; Stuart Russell. “Algorithms for inverse reinforcement learning.”. ICML 2000.                    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> [2] Amodei et al. “Concrete problems in AI safety: Avoid reward hacking.” arXiv preprint arXiv:1606.06565       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (2016).                                                                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> [3] Krakovna et al. “Specification gaming: the flip side of AI ingenuity.” 2020.                                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> [4] Langosco et al. “Goal Misgeneralization in Deep Reinforcement Learning” ICML 2022.                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> [5] Everitt et al. “Reinforcement learning with a corrupted reward channel.” IJCAI 2017.                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> [6] Geirhos et al. “Shortcut Learning in Deep Neural Networks.” Nature Machine Intelligence 2020.               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> [7] Ribeiro et al. “Why Should I Trust You?”: Explaining the Predictions of Any Classifier. KDD 2016.           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> [8] Nagarajan et al. “Understanding the Failure Modes of Out-of-Distribution Generalization.” ICLR 2021.        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> [9] Garrabrant. “Goodhart Taxonomy”. AI Alignment Forum (Dec 30th 2017).                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> [10] Koch et al. “Objective robustness in deep reinforcement learning.” 2021.                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> [11] Pan et al. “The effects of reward misspecification: mapping and mitigating misaligned models.”             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> [12] Everitt et al. “Reward tampering problems and solutions in reinforcement learning: A causal influence      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> diagram perspective.” arXiv preprint arXiv:1908.04734 (2019).                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> [13] Gleave et al. “Adversarial Policies: Attacking Deep Reinforcement Learning.” ICRL 2020                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> [14] “Reward hacking behavior can generalize across tasks.”                                                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> [15] Ng et al. “Policy invariance under reward transformations: Theory and application to reward shaping.” ICML <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> 1999.                                                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> [16] Wang et al. “Large Language Models are not Fair Evaluators.” ACL 2024.                                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> [17] Liu et al. “LLMs as narcissistic evaluators: When ego inflates evaluation scores.” ACL 2024.               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> [18] Gao et al. “Scaling Laws for Reward Model Overoptimization.” ICML 2023.                                    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> [19] Pan et al. “Spontaneous Reward Hacking in Iterative Self-Refinement.” arXiv preprint arXiv:2407.04549      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (2024).                                                                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> [20] Pan et al. “Feedback Loops With Language Models Drive In-Context Reward Hacking.” arXiv preprint           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> arXiv:2402.06627 (2024).                                                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> [21] Shrama et al. “Towards Understanding Sycophancy in Language Models.” arXiv preprint arXiv:2310.13548       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (2023).                                                                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> [22] Denison et al. “Sycophancy to subterfuge: Investigating reward tampering in language models.” arXiv        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> preprint arXiv:2406.10162 (2024).                                                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> [23] Uesato et al. “Avoiding Tampering Incentives in Deep RL via Decoupled Approval.” arXiv preprint            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> arXiv:2011.08827 (2020).                                                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> [24] Amin and Singh. “Towards resolving unidentifiability in inverse reinforcement learning.”                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> [25] Wen et al. “Language Models Learn to Mislead Humans via RLHF.” arXiv preprint arXiv:2409.12822 (2024).     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> [26] Revel et al. “SEAL: Systematic Error Analysis for Value ALignment.” arXiv preprint arXiv:2408.10270        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> (2024).                                                                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> [27] Yuval Noah Harari. “Nexus: A Brief History of Information Networks from the Stone Age to AI.” Signal; 2024 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Sep 10.                                                                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Language-Model                                                                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Rlhf                                                                                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Alignment                                                                                                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Safety                                                                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reinforcement-Learning                                                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Long-Read                                                                                                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> «                                                                                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Why We Think                                                                                                    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>  »                                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Extrinsic Hallucinations in LLMs                                                                                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> © 2025 Lil'Log                                                                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>         Powered by                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>         Hugo &amp;                                                                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>         PaperMod                                                                                                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Language-Model                                                                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reinforcement-Learning                                                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reasoning                                                                                                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Long-Read                                                                                                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>  »                                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reward Hacking in Reinforcement Learning                                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> © 2025 Lil'Log                                                                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>         Powered by                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>         Hugo &amp;                                                                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>         PaperMod                                                                                                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m╭─\u001b[0m\u001b[33m───────────────────────────────────────────────\u001b[0m\u001b[33m 🔧 Tool Output \u001b[0m\u001b[33m────────────────────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m\n",
       "\u001b[33m│\u001b[0m Illustration of parallel sampling vs sequential revision.                                                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Parallel Sampling#                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Given a generative model and a scoring function that we can use to score full or partial samples, there are     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m various search algorithms we can use to find a high scoring sample. Best-of-$N$ is the simplest such algorithm: \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m one just collects $N$ independent samples and chooses the highest-ranking sample according to some scoring      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m function. Beam search is a more sophisticated search algorithm that makes the search process more adaptive,     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m spending more sampling computation on more promising parts of the solution space.                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Beam search maintains a set of promising partial sequences and alternates between extending them and pruning    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m the less promising ones. As a selection mechanism, we can use a process reward model (PRM; Lightman et al.      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m 2023) to guide beam search candidate selection. Xie et al. (2023) used LLM to evaluate how likely its own       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m generated reasoning step is correct, formatted as a multiple-choice question and found that per-step            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m self-evaluation reduces accumulative errors in multi-step reasoning during beam search decoding. Besides,       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m during sampling, annealing the temperature helps mitigate aggregated randomness. These experiments by Xie et    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m al. achieved 5-6% improvement on few-shot GSM8k, AQuA and StrategyQA benchmarks with the Codex model. Reward    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m balanced search (short for “REBASE”; Wu et al. 2025) separately trained a process reward model (PRM) to         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m determine how much each node should be expanded at each depth during beam search, according to the              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m softmax-normalized reward scores. Jiang et al. (2024) trained their PRM, named “RATIONALYST”, for beam search   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m guidance on synthetic rationales conditioned on a large amount of unlabelled data. Good rationales are filtered \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m based on whether they help reduce the neg log-prob of true answer tokens by a threshold, when comparing the     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m difference between when the rationales is included in the context vs not. At inference time, RATIONALYST        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m provides process supervision to the CoT generator by helping estimate log-prob of next reasoning steps          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (“implicit”) or directly generating next reasoning steps as part of the prompt (“explicit”).                    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Beam search decoding guided by LLM self-evaluation per reasoning step. (Image source: Xie et al. 2023)          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Interestingly, it is possible to trigger the emergent chain-of-thought reasoning paths without explicit         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m zero-shot or few-shot prompting. Wang & Zhou (2024) discovered that if we branch out at the first sampling      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m tokens by retaining the top $k$ tokens with highest confidence, measured as the difference between top-1 and    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m top-2 candidates during sampling, and then continue these $k$ sampling trials with greedy decoding onward, many \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m of these sequences natively contain CoT. Especially when CoT does appear in the context, it leads to a more     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m confident decoding of the final answer. To calculate the confidence of the final answer, the answer span needs  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m to be identified by task-specific heuristics (e.g. last numerical values for math questions) or  by prompting   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m the model further with \"So the answer is\". The design choice of only branching out at the first token is based  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m on the observation that early branching significantly enhances the diversity of potential paths, while later    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m tokens are influenced a lot by previous sequences.                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Top-$k$ decoding, $k$ refers to the number of candidates at the first sampling step. (Image source: Wang &      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Zhou, 2024)                                                                                                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Sequential Revision#                                                                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m If the model can reflect and correct mistakes in past responses, we would expect the model to produce a nice    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m sequence of iterative revision with increasing quality. However, this self-correction capability turns out to   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m not exist intrinsically among LLMs and does not easily work out of the box, due to various failure modes, such  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m as, (1) hallucination, including modifying correct responses to be incorrect; (2) behavior collapse to          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m non-correcting behavior; e.g. making minor or no modification on the first incorrect responses; or (3) fail to  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m generalize to distribution shift at test time. Experiments by Huang et al. (2024) showed that naively applying  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m self-correction leads to worse performance and external feedback is needed for models to self improve, which    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m can be based on matching ground truths, heuristics and task-specific metrics, unit tests results for coding     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m questions (Shinn, et al. 2023), a stronger model (Zhang et al. 2024), as well as human feedback (Liu et al.     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m 2023).                                                                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Self-correction learning (Welleck et al. 2023) aims to train a corrector model $P_\\theta(y \\mid y_0, x)$ given  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m a fixed generator model $P_0(y_0 \\mid x)$. While the generator model remains to be generic, the corrector model \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m can task-specific and only does generation conditioned on an initial model response and additional feedback     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (e.g. a sentence, a compiler trace, unit test results; can be optional):                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Self-correction learning first generates first generates multiple outputs per prompt in the data pool;          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m then create value-improving pairs by pairing two outputs for the same prompt together if one has a higher value \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m than the other, (prompt $x$, hypothesis $y$, correction $y’$).                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m These pairs are selected proportional to is improvement in value, $v(y’) - v(y)$, and similarity between two    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m outputs, $\\text{Similarity}(y, y’)$ to train the corrector model.                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m To encourage exploration, the corrector provides new generations into the data pool as well. At the inference   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m time, the corrector can be used iteratively to create a correction trajectory of sequential revision.           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Illustration of self-correction learning by matching model outputs for the same problem to form value-improving \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m pairs to train a correction model. (Image source: Welleck et al. 2023)                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Recursive inspection (Qu et al. 2024) also aims to train a better corrector model but with a single model to do \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m both generation and self-correction.                                                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m SCoRe (Self-Correction via Reinforcement Learning; Kumar et al. 2024) is a multi-turn RL approach to encourage  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m the model to do self-correction by producing better answers at the second attempt than the one created at the   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m first attempt. It composes two stages of training: stage 1 only maximizes the accuracy of the second attempt    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m while enforcing a KL penalty only on the first attempt to avoid too much shifting of the first-turn responses   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m from the base model behavior; stage 2 optimizes the accuracy of answers produced by both the first and second   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m attempts. Ideally we do want to see performance at both first and second attempts to be better, but adding      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m stage 1 prevents the behavior collapse where the model does minor or none edits on the first response, and      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m stage 2 further improves the results.                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Explicit training setup to improve self-correction capabilities by doing two-staged RL training. (Image source: \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Kumar et al. 2024)                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m RL for Better Reasoning#                                                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m There’s been a lot of recent success in using RL to improve the reasoning ability of language models, by using  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m a collection of questions with ground truth answers (usually STEM problems and puzzles with easy to verify      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m answers), and rewarding the model for getting the correct answer.Recent activity in this area was spurred by    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m strong performance of the o-series models from OpenAI, and the subsequent releases of models and tech reports   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m from DeepSeek.                                                                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m DeepSeek-R1 (DeepSeek-AI, 2025) is an open-source LLM designed to excel in tasks that require advanced          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m reasoning skills like math, coding and logical problem solving. They run through 2 rounds of SFT-RL training,   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m enabling R1 to be good at both reasoning and non-reasoning tasks.                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Cold-start SFT is to fine-tune the DeepSeek-V3-Base base model on a collection of thousands of cold-start data. \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Without this step, the model has issues of poor readability and language mixing.                                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reasoning-oriented RL trains a reasoning model on reasoning-only prompts with two types of rule-based rewards:  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Format rewards: The model should wrap CoTs by <thinking> ... </thinking> tokens.                                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Accuracy rewards: Whether the final answers are correct. The answer for math problems needs to be present in a  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m specific format (e.g. in a box) to be verified reliably. For coding problems, a compiler is used to evaluate    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m whether test cases pass.                                                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Rejection-sampling + non-reasoning SFT utilizes new SFT data created by rejection sampling on the RL checkpoint \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m of step 2, combined with non-reasoning supervised data from DeepSeek-V3 in domains like writing, factual QA,    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m and self-cognition, to retrain DeepSeek-V3-Base.                                                                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Hacking RL Environment#                                                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reward hacking is expected to be a more common problem as the model and the algorithm become increasingly       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m sophisticated. A more intelligent agent is more capable of finding “holes” in the design of reward function and \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m exploiting the task specification—in other words, achieving higher proxy rewards but lower true rewards. By     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m contrast, a weaker algorithm may not be able to find such loopholes, and thus we would not observe any reward   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m hacking or identify issues in the current reward function design when the model is not strong enough.           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m In a set of zero-sum robotics self-play games (Bansal et al., 2017), we can train two agents (victim vs.        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m opponent) to compete against each other. A standard training process produces a victim agent with adequate      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m performance when playing against a normal opponent. However, it is easy to train an adversarial opponent policy \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m that can defeat the victim reliably despite outputting seemingly random actions and training with fewer than 3% \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m of time steps (Gleave et al., 2020). Training of adversarial policies involves optimizing the sum of discounted \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m rewards, as in standard RL setup, while treating the victim policy as a black-box model.                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m An intuitive way to mitigate adversarial policies attacks is to fine-tune victims against adversarial policies. \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m However, the victim remains vulnerable to new versions of adversarial policies once retrained against the new   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m victim policy.                                                                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Why does adversarial policy exist? The hypothesis is that adversarial policies introduce OOD observations to    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m the victim rather than physically interfering with it. Evidence shows that when the victim’s observation of the \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m opponent’s position is masked and set to a static state, the victim becomes more robust to adversaries,         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m although performing worse against a normal opponent policy. Furthermore, a higher-dimensional observation space \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m enhances performance under normal circumstances but makes the policy more vulnerable to adversarial opponents.  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Pan et al. (2022) investigated reward hacking as a function of agent capabilities, including (1) model size,    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (2) action space resolution, (3) observation space noise, and (4) training time. They also proposed a taxonomy  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m of three types of misspecified proxy rewards:                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Misweighting: Proxy and true rewards capture the same desiderata, but differ in their relative importance.      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Ontological: Proxy and true rewards use different desiderata to capture the same concept.                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Scope: The proxy measures desiderata over a restricted domain (e.g. time or space) because measurement across   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m all conditions is too costly.                                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m They experimented in four RL environments paired with nine misspecified proxy rewards. The overall findings     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m from these experiments can be summarized as follows: A model of higher capability tends to obtain higher (or    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m similar) proxy rewards but decreased true rewards.                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Model size: Larger model size leads to increased proxy rewards but decreased true rewards.                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Action space resolution: Increased precision in actions leads to more capable agents. However, higher           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m resolution causes proxy rewards to remain constant while true rewards decrease.                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Observation fidelity: More accurate observations improve proxy rewards but slightly reduce true rewards.        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Training steps: Optimizing the proxy reward over more steps harms true rewards after an initial period where    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m the rewards are positively correlated.                                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The plot of proxy and true reward value as functions of (Top row) model sizes, measured in parameter count;     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (Bottom row) model capability, measured by metrics such as training steps, action space resolution, and         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m observation noise. (Image source: Pan et al. 2022)                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m If a proxy reward is so poorly specified that it has a very weak correlation with the true reward, we may be    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m able to identify and prevent reward hacking even before training. Based on this hypothesis, Pan et al. (2022)   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m investigated the correlation between proxy and true rewards over a collection of trajectory rollouts.           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Interestingly, reward hacking still occurs even when there is a positive correlation between the true and proxy \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m rewards.                                                                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Hacking RLHF of LLMs#                                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reinforcement learning from human feedback (RLHF) has become the de facto approach for alignment training of    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m language models. A reward model is trained on human feedback data and then a language model is fine-tuned via   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m RL to optimize this proxy reward for human preference. There are three types of reward we care about in an RLHF \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m setup:                                                                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (1) Oracle/Gold reward $R^∗$ represents what we truly want the LLM to optimize.                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (2) Human reward $R^\\text{human}$ is what we collect to evaluate LLMs in practice, typically from individual    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m humans with time constraints. Because humans can provide inconsistent feedback or make mistakes, human reward   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m is not a fully accurate representation of the oracle reward.                                                    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (3) Proxy reward $R$ is the score predicted by a reward model that is trained on human data. Hence,             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m $R^\\text{train}$ inherits all the weakness of human reward, plus potential modeling biases.                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m RLHF optimizes the proxy reward score but we ultimately care about the gold reward score.                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Hacking the Training Process#                                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Gao et al. (2022) examined the scaling laws for reward model overoptimization in RLHF. To scale up the human    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m labels in their experiments, they use a synthetic data setup where the “gold” label for the oracle reward $R^*$ \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m is approximated by a large RM (6B parameters) where the proxy RMs for $R$ range in size of 3M to 3B parameters. \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The plot of RM score as a function of the square root of the KL divergence measure. The proxy reward is shown   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m with a dashed line, and the gold reward is shown with a solid line. (Image source: Gao et al. 2022)             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The KL divergence from the initial policy to the optimized policy is $\\text{KL} = D_\\text{KL}(\\pi |             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\pi_\\text{init})$, and the distance function is defined as $d := \\sqrt{ D_\\text{KL}(\\pi | \\pi_\\text{init})}$.   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m For both best-of-$n$ rejection sampling (BoN) and RL, the gold reward $R^∗$ is defined as a function of $d$.    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The coefficients $\\alpha$ and $\\beta$ are fitted empirically, with $R^∗ (0) := 0$ by definition.                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The authors also attempted to fit the proxy reward $R$ but found systematic underestimation when extrapolated   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m to higher KLs, as the proxy reward appeared to grow linearly with $d$.                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m $$                                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\begin{aligned}                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m R^*_{\\text{bo}n}(d) &= d (\\alpha_{\\text{bo}n} - \\beta_{\\text{bo}n} d) & \\text{; for best-of-n (BoN)             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m sampling.}\\\\                                                                                                    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m R^*_\\text{RL}(d) &= d (\\alpha_\\text{RL} - \\beta_\\text{RL} \\log d) & \\text{; for reinforcement learning}\\\\       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \\end{aligned}                                                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m $$                                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The coefficient parameters, $\\alpha_{\\text{bo}n}, \\beta_{\\text{bo}n}, \\beta_\\text{RL}$ are empirically fit      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m according to data, displayed as functions of the reward model size. The coefficient $\\alpha_\\text{RL}$ is not   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m included here because it remains constant across RM sizes. (Image source: Gao et al. 2022)                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Their experiments also explored the relationship between RM overoptimization and factors like policy model size \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m and RM data size:                                                                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Larger policies see less benefit from optimization (i.e., the difference between initial and peak rewards is    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m smaller than that of a smaller policy) against an RM, but also overoptimize less.                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m More RM data leads to higher gold reward scores and reduces “Goodharting”.                                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The effect of the KL penalty on the gold score resembles early stopping. Note that in all experiments except    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m this one, the KL penalty in PPO is set to 0, because they observed that using a KL penalty strictly increases   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m the proxy-gold reward gap.                                                                                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m RLHF aims to improve the model’s alignment with human preference, but human feedback $R^\\text{human}$ may not   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m capture all the aspects we care about (e.g., factuality) and thus can be hacked to overfit to undesired         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m attributes. For example, the model may be optimized to output responses that seem correct and convincing but    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m are, in fact, inaccurate, thereby misleading human evaluators to approve its incorrect answers more often (Wen  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m et al., 2024). In other words, a gap emerges between what is correct and what looks correct to humans due to    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m RLHF. Precisely Wen et al. (2024) ran RLHF experiments using a reward model based on ChatbotArena data. They    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m evaluated the model on a question-answering dataset, QuALITY and a programming dataset, APPS. Their experiments \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m revealed that models become better at convincing humans they are correct, even when they are wrong and this     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m effect is unintended:                                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m RLHF increases human approval, but not necessarily correctness.                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m RLHF weakens humans’ ability to evaluate: The error rate of human evaluation is higher after RLHF training.     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m RLHF makes incorrect outputs more convincing to humans. The evaluation false positive rate significantly        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m increases after RLHF training.                                                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Or                                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m @article{weng2024rewardhack,                                                                                    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m   title   = \"Reward Hacking in Reinforcement Learning.\",                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m   author  = \"Weng, Lilian\",                                                                                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m   journal = \"lilianweng.github.io\",                                                                             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m   year    = \"2024\",                                                                                             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m   month   = \"Nov\",                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m   url     = \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\"                                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m }                                                                                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m References#                                                                                                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m [1] Andrew Ng & Stuart Russell. “Algorithms for inverse reinforcement learning.”. ICML 2000.                    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m [2] Amodei et al. “Concrete problems in AI safety: Avoid reward hacking.” arXiv preprint arXiv:1606.06565       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (2016).                                                                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m [3] Krakovna et al. “Specification gaming: the flip side of AI ingenuity.” 2020.                                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m [4] Langosco et al. “Goal Misgeneralization in Deep Reinforcement Learning” ICML 2022.                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m [5] Everitt et al. “Reinforcement learning with a corrupted reward channel.” IJCAI 2017.                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m [6] Geirhos et al. “Shortcut Learning in Deep Neural Networks.” Nature Machine Intelligence 2020.               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m [7] Ribeiro et al. “Why Should I Trust You?”: Explaining the Predictions of Any Classifier. KDD 2016.           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m [8] Nagarajan et al. “Understanding the Failure Modes of Out-of-Distribution Generalization.” ICLR 2021.        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m [9] Garrabrant. “Goodhart Taxonomy”. AI Alignment Forum (Dec 30th 2017).                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m [10] Koch et al. “Objective robustness in deep reinforcement learning.” 2021.                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m [11] Pan et al. “The effects of reward misspecification: mapping and mitigating misaligned models.”             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m [12] Everitt et al. “Reward tampering problems and solutions in reinforcement learning: A causal influence      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m diagram perspective.” arXiv preprint arXiv:1908.04734 (2019).                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m [13] Gleave et al. “Adversarial Policies: Attacking Deep Reinforcement Learning.” ICRL 2020                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m [14] “Reward hacking behavior can generalize across tasks.”                                                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m [15] Ng et al. “Policy invariance under reward transformations: Theory and application to reward shaping.” ICML \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m 1999.                                                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m [16] Wang et al. “Large Language Models are not Fair Evaluators.” ACL 2024.                                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m [17] Liu et al. “LLMs as narcissistic evaluators: When ego inflates evaluation scores.” ACL 2024.               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m [18] Gao et al. “Scaling Laws for Reward Model Overoptimization.” ICML 2023.                                    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m [19] Pan et al. “Spontaneous Reward Hacking in Iterative Self-Refinement.” arXiv preprint arXiv:2407.04549      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (2024).                                                                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m [20] Pan et al. “Feedback Loops With Language Models Drive In-Context Reward Hacking.” arXiv preprint           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m arXiv:2402.06627 (2024).                                                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m [21] Shrama et al. “Towards Understanding Sycophancy in Language Models.” arXiv preprint arXiv:2310.13548       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (2023).                                                                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m [22] Denison et al. “Sycophancy to subterfuge: Investigating reward tampering in language models.” arXiv        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m preprint arXiv:2406.10162 (2024).                                                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m [23] Uesato et al. “Avoiding Tampering Incentives in Deep RL via Decoupled Approval.” arXiv preprint            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m arXiv:2011.08827 (2020).                                                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m [24] Amin and Singh. “Towards resolving unidentifiability in inverse reinforcement learning.”                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m [25] Wen et al. “Language Models Learn to Mislead Humans via RLHF.” arXiv preprint arXiv:2409.12822 (2024).     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m [26] Revel et al. “SEAL: Systematic Error Analysis for Value ALignment.” arXiv preprint arXiv:2408.10270        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m (2024).                                                                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m [27] Yuval Noah Harari. “Nexus: A Brief History of Information Networks from the Stone Age to AI.” Signal; 2024 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Sep 10.                                                                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Language-Model                                                                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Rlhf                                                                                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Alignment                                                                                                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Safety                                                                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reinforcement-Learning                                                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Long-Read                                                                                                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m «                                                                                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Why We Think                                                                                                    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m  »                                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Extrinsic Hallucinations in LLMs                                                                                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m © 2025 Lil'Log                                                                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m         Powered by                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m         Hugo &                                                                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m         PaperMod                                                                                                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Language-Model                                                                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reinforcement-Learning                                                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reasoning                                                                                                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Long-Read                                                                                                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m  »                                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reward Hacking in Reinforcement Learning                                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m © 2025 Lil'Log                                                                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m         Powered by                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m         Hugo &                                                                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m         PaperMod                                                                                                \u001b[33m│\u001b[0m\n",
       "\u001b[33m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">╭───────────────────────────────────────────────────── 📝 AI ─────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> Based on the information from Lilian Weng's blog posts, here's why RL improves LLM reasoning according to her   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> analysis:                                                                                                       <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> ## Key Mechanisms of RL for Improving LLM Reasoning                                                             <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> ### 1. **Direct Reward Optimization for Correctness**                                                           <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> RL improves reasoning by directly rewarding models for getting correct answers. According to the blogs, there   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> has been \"a lot of recent success in using RL to improve the reasoning ability of language models, by using a   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> collection of questions with ground truth answers (usually STEM problems and puzzles with easy to verify        <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> answers), and rewarding the model for getting the correct answer.\"                                              <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> ### 2. **Two-Type Reward Structure**                                                                            <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> The DeepSeek-R1 example illustrates how RL uses structured rewards:                                             <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **Format rewards**: Encouraging proper reasoning structure (wrapping chain-of-thought in `&lt;thinking&gt;` tags)   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **Accuracy rewards**: Directly rewarding correct final answers (verified through specific formats for math    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> problems or compiler tests for coding)                                                                          <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> ### 3. **Emergent Self-Correction and Reflection**                                                              <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> RL training enables models to develop sophisticated reasoning behaviors:                                        <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **\"Aha moments\"**: The model learns to reflect on previous mistakes and try alternative approaches            <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **Self-correction capabilities**: Through multi-turn RL approaches like SCoRe, models learn to produce better <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> answers on second attempts                                                                                      <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **Backtracking**: Models naturally learn to reconsider and revise their reasoning paths                       <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> ### 4. **Iterative Learning Through STaR**                                                                      <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> The Self-Taught Reasoner (STaR) approach shows how RL-like training improves reasoning:                         <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - Models generate multiple chain-of-thought (CoT) paths                                                         <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - Only correct reasoning paths are reinforced through fine-tuning                                               <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - For failed attempts, models learn through \"rationalization\" - generating good CoTs backward from correct      <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> answers                                                                                                         <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - This approximates policy gradient RL with a simple reward function                                            <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> ### 5. **Enhanced Test-Time Compute Utilization**                                                               <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> RL training teaches models to effectively use additional thinking time:                                         <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - Models learn to spend more computational tokens on reasoning during inference                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - There's a positive correlation between thinking time (measured in tokens) and evaluation accuracy             <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - RL-trained models become better at adaptive computation - spending more effort on harder problems             <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> ### 6. **Process vs. Outcome Supervision**                                                                      <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> RL enables both:                                                                                                <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **Outcome supervision**: Rewarding final correct answers                                                      <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **Process supervision**: Using process reward models (PRMs) to guide intermediate reasoning steps             <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - This helps reduce accumulative errors in multi-step reasoning                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> ### 7. **Pure RL Success**                                                                                      <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> Notably, the blogs mention that \"with pure RL, no SFT stage, it is still possible to learn advanced reasoning   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> capabilities like reflection and backtracking.\" This suggests RL alone can unlock sophisticated reasoning       <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> behaviors without requiring supervised fine-tuning on reasoning examples.                                       <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> ## Why This Works                                                                                               <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> The fundamental insight is that RL provides a **direct optimization signal for the ultimate goal** (correct     <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> answers) rather than just mimicking reasoning patterns from training data. This allows models to:               <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> 1. **Discover novel reasoning strategies** that weren't explicitly in the training data                         <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> 2. **Learn to allocate computational resources** more effectively during reasoning                              <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> 3. **Develop meta-cognitive skills** like self-correction and reflection                                        <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> 4. **Optimize for actual correctness** rather than just plausible-sounding reasoning                            <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> The blogs emphasize that this approach has been particularly successful for tasks with \"easy to verify answers\" <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> like STEM problems, where the reward signal is clear and objective.                                             <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37m╭─\u001b[0m\u001b[37m────────────────────────────────────────────────────\u001b[0m\u001b[37m 📝 AI \u001b[0m\u001b[37m────────────────────────────────────────────────────\u001b[0m\u001b[37m─╮\u001b[0m\n",
       "\u001b[37m│\u001b[0m Based on the information from Lilian Weng's blog posts, here's why RL improves LLM reasoning according to her   \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m analysis:                                                                                                       \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m ## Key Mechanisms of RL for Improving LLM Reasoning                                                             \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m ### 1. **Direct Reward Optimization for Correctness**                                                           \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m RL improves reasoning by directly rewarding models for getting correct answers. According to the blogs, there   \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m has been \"a lot of recent success in using RL to improve the reasoning ability of language models, by using a   \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m collection of questions with ground truth answers (usually STEM problems and puzzles with easy to verify        \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m answers), and rewarding the model for getting the correct answer.\"                                              \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m ### 2. **Two-Type Reward Structure**                                                                            \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m The DeepSeek-R1 example illustrates how RL uses structured rewards:                                             \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **Format rewards**: Encouraging proper reasoning structure (wrapping chain-of-thought in `<thinking>` tags)   \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **Accuracy rewards**: Directly rewarding correct final answers (verified through specific formats for math    \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m problems or compiler tests for coding)                                                                          \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m ### 3. **Emergent Self-Correction and Reflection**                                                              \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m RL training enables models to develop sophisticated reasoning behaviors:                                        \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **\"Aha moments\"**: The model learns to reflect on previous mistakes and try alternative approaches            \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **Self-correction capabilities**: Through multi-turn RL approaches like SCoRe, models learn to produce better \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m answers on second attempts                                                                                      \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **Backtracking**: Models naturally learn to reconsider and revise their reasoning paths                       \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m ### 4. **Iterative Learning Through STaR**                                                                      \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m The Self-Taught Reasoner (STaR) approach shows how RL-like training improves reasoning:                         \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - Models generate multiple chain-of-thought (CoT) paths                                                         \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - Only correct reasoning paths are reinforced through fine-tuning                                               \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - For failed attempts, models learn through \"rationalization\" - generating good CoTs backward from correct      \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m answers                                                                                                         \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - This approximates policy gradient RL with a simple reward function                                            \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m ### 5. **Enhanced Test-Time Compute Utilization**                                                               \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m RL training teaches models to effectively use additional thinking time:                                         \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - Models learn to spend more computational tokens on reasoning during inference                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - There's a positive correlation between thinking time (measured in tokens) and evaluation accuracy             \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - RL-trained models become better at adaptive computation - spending more effort on harder problems             \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m ### 6. **Process vs. Outcome Supervision**                                                                      \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m RL enables both:                                                                                                \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **Outcome supervision**: Rewarding final correct answers                                                      \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **Process supervision**: Using process reward models (PRMs) to guide intermediate reasoning steps             \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - This helps reduce accumulative errors in multi-step reasoning                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m ### 7. **Pure RL Success**                                                                                      \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m Notably, the blogs mention that \"with pure RL, no SFT stage, it is still possible to learn advanced reasoning   \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m capabilities like reflection and backtracking.\" This suggests RL alone can unlock sophisticated reasoning       \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m behaviors without requiring supervised fine-tuning on reasoning examples.                                       \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m ## Why This Works                                                                                               \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m The fundamental insight is that RL provides a **direct optimization signal for the ultimate goal** (correct     \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m answers) rather than just mimicking reasoning patterns from training data. This allows models to:               \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m 1. **Discover novel reasoning strategies** that weren't explicitly in the training data                         \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m 2. **Learn to allocate computational resources** more effectively during reasoning                              \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m 3. **Develop meta-cognitive skills** like self-correction and reflection                                        \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m 4. **Optimize for actual correctness** rather than just plausible-sounding reasoning                            \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m The blogs emphasize that this approach has been particularly successful for tasks with \"easy to verify answers\" \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m like STEM problems, where the reward signal is clear and objective.                                             \u001b[37m│\u001b[0m\n",
       "\u001b[37m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import format_messages\n",
    "query = \"Why does RL improve LLM reasoning according to the blogs?\"\n",
    "result = agent.invoke({\"messages\": query})\n",
    "format_messages(result['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.markdown import Markdown\n",
    "Markdown(result[\"summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarize Tools\n",
    "\n",
    "Let's update the RAG agent, and add summarization the tool call output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tool_summarization_prompt = \"\"\"You will be provided a doc from a RAG system.\n",
    "Summarize the docs, ensuring to retain all relevant / essential information.\n",
    "Your goal is simply to reduce the size of the doc (tokens) to a more manageable size.\"\"\"\n",
    "\n",
    "# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\n",
    "def should_continue(state: MessagesState) -> Literal[\"environment\", END]:\n",
    "    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If the LLM makes a tool call, then perform an action\n",
    "    if last_message.tool_calls:\n",
    "        return \"Action\"\n",
    "    # Otherwise, we stop (reply to the user)\n",
    "    return END\n",
    "\n",
    "def tool_node_with_summarization(state: dict):\n",
    "    \"\"\"Performs the tool call\"\"\"\n",
    "\n",
    "    result = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        tool = tools_by_name[tool_call[\"name\"]]\n",
    "        observation = tool.invoke(tool_call[\"args\"])\n",
    "        # Summarize the doc\n",
    "        summary = llm.invoke([{\"role\":\"system\",\n",
    "                              \"content\":tool_summarization_prompt},\n",
    "                              {\"role\":\"user\",\n",
    "                               \"content\":observation}])\n",
    "        result.append(ToolMessage(content=summary.content, tool_call_id=tool_call[\"id\"]))\n",
    "    return {\"messages\": result}\n",
    "\n",
    "# Build workflow\n",
    "agent_builder = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "agent_builder.add_node(\"llm_call\", llm_call)\n",
    "agent_builder.add_node(\"environment_with_summarization\", tool_node_with_summarization)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "agent_builder.add_edge(START, \"llm_call\")\n",
    "agent_builder.add_conditional_edges(\n",
    "    \"llm_call\",\n",
    "    should_continue,\n",
    "    {\n",
    "        # Name returned by should_continue : Name of next node to visit\n",
    "        \"Action\": \"environment_with_summarization\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "agent_builder.add_edge(\"environment_with_summarization\", \"llm_call\")\n",
    "\n",
    "# Compile the agent\n",
    "agent = agent_builder.compile()\n",
    "\n",
    "# Show the agent\n",
    "display(Image(agent.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">╭─────────────────────────────────────────────────── 🧑 Human ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> Why does RL improve LLM reasoning according to the blogs?                                                       <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m╭─\u001b[0m\u001b[34m──────────────────────────────────────────────────\u001b[0m\u001b[34m 🧑 Human \u001b[0m\u001b[34m───────────────────────────────────────────────────\u001b[0m\u001b[34m─╮\u001b[0m\n",
       "\u001b[34m│\u001b[0m Why does RL improve LLM reasoning according to the blogs?                                                       \u001b[34m│\u001b[0m\n",
       "\u001b[34m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">╭───────────────────────────────────────────────────── 📝 AI ─────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> I'll help you find information about how reinforcement learning (RL) improves LLM reasoning according to Lilian <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> Weng's blog posts. Let me search for relevant content on this topic.                                            <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> 🔧 Tool Call: retrieve_blog_posts                                                                               <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>    Args: {                                                                                                      <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>   \"query\": \"reinforcement learning RL improve LLM reasoning\"                                                    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> }                                                                                                               <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37m╭─\u001b[0m\u001b[37m────────────────────────────────────────────────────\u001b[0m\u001b[37m 📝 AI \u001b[0m\u001b[37m────────────────────────────────────────────────────\u001b[0m\u001b[37m─╮\u001b[0m\n",
       "\u001b[37m│\u001b[0m I'll help you find information about how reinforcement learning (RL) improves LLM reasoning according to Lilian \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m Weng's blog posts. Let me search for relevant content on this topic.                                            \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m 🔧 Tool Call: retrieve_blog_posts                                                                               \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m    Args: {                                                                                                      \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m   \"query\": \"reinforcement learning RL improve LLM reasoning\"                                                    \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m }                                                                                                               \u001b[37m│\u001b[0m\n",
       "\u001b[37m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">╭──────────────────────────────────────────────── 🔧 Tool Output ─────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> # Summary: Parallel Sampling vs Sequential Revision in LLM Reasoning                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> ## Parallel Sampling                                                                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> **Best-of-N and Beam Search**: Simple parallel sampling collects N independent samples and selects the          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> highest-scoring one. Beam search is more sophisticated, maintaining promising partial sequences and using       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> process reward models (PRMs) for guidance.                                                                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> **Key Techniques**:                                                                                             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - **Self-evaluation**: LLMs evaluate their own reasoning steps during beam search, achieving 5-6% improvement   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> on math benchmarks                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - **REBASE**: Uses trained PRMs to determine node expansion during beam search                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - **RATIONALYST**: Provides process supervision by estimating log-probabilities of reasoning steps              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - **Top-k branching**: Branches at first token based on confidence (difference between top-1 and top-2          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> candidates), triggering emergent CoT reasoning                                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> ## Sequential Revision                                                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> **Self-Correction Challenges**: LLMs lack intrinsic self-correction capabilities and suffer from hallucination, <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> behavior collapse, and generalization failures. External feedback is essential for improvement.                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> **Training Approaches**:                                                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - **Self-correction learning**: Trains corrector models using value-improving pairs from multiple outputs       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - **Recursive inspection**: Single model performs both generation and self-correction                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - **SCoRe**: Two-stage RL approach preventing behavior collapse while improving both first and second attempts  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> ## RL for Better Reasoning                                                                                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> **DeepSeek-R1**: Uses cold-start SFT, reasoning-oriented RL with format/accuracy rewards, and rejection         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> sampling combined with non-reasoning SFT.                                                                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> **STaR (Self-taught Reasoner)**: Iterative improvement process generating multiple CoTs, fine-tuning on correct <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> solutions, and using \"rationalization\" for failed attempts by generating CoTs backward from ground truth.       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> ## Scaling Laws for Thinking Time                                                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> **Test-time vs Pretraining Compute**: Test-time compute can be more effective than scaling parameters for       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> smaller capability gaps, but pretraining remains critical for hard problems. The ratio of inference to          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> pretraining tokens matters significantly.                                                                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> **Key Findings**:                                                                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Positive correlation between thinking time (token length) and accuracy                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Budget forcing (lengthening CoT paths) improves performance                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Rejection sampling for length control shows negative scaling                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Smaller models with advanced inference can offer Pareto-optimal cost-performance trade-offs                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> ## Future Research Directions                                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Incentivizing human-readable, faithful reasoning while avoiding reward hacking                                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Defining and preventing reward hacking without human intervention                                             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Training self-correction without hallucination when ground truth unavailable                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Applying RL with CoT to contextualized tasks like creative writing                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Translating test-time performance gains back to base models                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Making test-time compute adaptive to problem difficulty                                                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> ## Reward Hacking Context                                                                                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> **Environment Hacking**: More sophisticated agents find \"holes\" in reward functions, achieving higher proxy but <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> lower true rewards. Adversarial policies can defeat victims through out-of-distribution observations.           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> **RLHF Hacking**: Three reward types exist - oracle (true goal), human (collected feedback), and proxy (reward  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> model). RLHF optimizes proxy rewards but overoptimization leads to decreased gold rewards. Models become better <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> at convincing humans they're correct even when wrong, weakening human evaluation ability.                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m╭─\u001b[0m\u001b[33m───────────────────────────────────────────────\u001b[0m\u001b[33m 🔧 Tool Output \u001b[0m\u001b[33m────────────────────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m\n",
       "\u001b[33m│\u001b[0m # Summary: Parallel Sampling vs Sequential Revision in LLM Reasoning                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m ## Parallel Sampling                                                                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m **Best-of-N and Beam Search**: Simple parallel sampling collects N independent samples and selects the          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m highest-scoring one. Beam search is more sophisticated, maintaining promising partial sequences and using       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m process reward models (PRMs) for guidance.                                                                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m **Key Techniques**:                                                                                             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - **Self-evaluation**: LLMs evaluate their own reasoning steps during beam search, achieving 5-6% improvement   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m on math benchmarks                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - **REBASE**: Uses trained PRMs to determine node expansion during beam search                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - **RATIONALYST**: Provides process supervision by estimating log-probabilities of reasoning steps              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - **Top-k branching**: Branches at first token based on confidence (difference between top-1 and top-2          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m candidates), triggering emergent CoT reasoning                                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m ## Sequential Revision                                                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m **Self-Correction Challenges**: LLMs lack intrinsic self-correction capabilities and suffer from hallucination, \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m behavior collapse, and generalization failures. External feedback is essential for improvement.                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m **Training Approaches**:                                                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - **Self-correction learning**: Trains corrector models using value-improving pairs from multiple outputs       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - **Recursive inspection**: Single model performs both generation and self-correction                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - **SCoRe**: Two-stage RL approach preventing behavior collapse while improving both first and second attempts  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m ## RL for Better Reasoning                                                                                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m **DeepSeek-R1**: Uses cold-start SFT, reasoning-oriented RL with format/accuracy rewards, and rejection         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m sampling combined with non-reasoning SFT.                                                                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m **STaR (Self-taught Reasoner)**: Iterative improvement process generating multiple CoTs, fine-tuning on correct \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m solutions, and using \"rationalization\" for failed attempts by generating CoTs backward from ground truth.       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m ## Scaling Laws for Thinking Time                                                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m **Test-time vs Pretraining Compute**: Test-time compute can be more effective than scaling parameters for       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m smaller capability gaps, but pretraining remains critical for hard problems. The ratio of inference to          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m pretraining tokens matters significantly.                                                                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m **Key Findings**:                                                                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Positive correlation between thinking time (token length) and accuracy                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Budget forcing (lengthening CoT paths) improves performance                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Rejection sampling for length control shows negative scaling                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Smaller models with advanced inference can offer Pareto-optimal cost-performance trade-offs                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m ## Future Research Directions                                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Incentivizing human-readable, faithful reasoning while avoiding reward hacking                                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Defining and preventing reward hacking without human intervention                                             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Training self-correction without hallucination when ground truth unavailable                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Applying RL with CoT to contextualized tasks like creative writing                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Translating test-time performance gains back to base models                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Making test-time compute adaptive to problem difficulty                                                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m ## Reward Hacking Context                                                                                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m **Environment Hacking**: More sophisticated agents find \"holes\" in reward functions, achieving higher proxy but \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m lower true rewards. Adversarial policies can defeat victims through out-of-distribution observations.           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m **RLHF Hacking**: Three reward types exist - oracle (true goal), human (collected feedback), and proxy (reward  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m model). RLHF optimizes proxy rewards but overoptimization leads to decreased gold rewards. Models become better \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m at convincing humans they're correct even when wrong, weakening human evaluation ability.                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">╭───────────────────────────────────────────────────── 📝 AI ─────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> Let me search for more specific information about the mechanisms by which RL improves reasoning in LLMs.        <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> 🔧 Tool Call: retrieve_blog_posts                                                                               <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>    Args: {                                                                                                      <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>   \"query\": \"RL reinforcement learning chain of thought reasoning training mechanisms\"                           <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> }                                                                                                               <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37m╭─\u001b[0m\u001b[37m────────────────────────────────────────────────────\u001b[0m\u001b[37m 📝 AI \u001b[0m\u001b[37m────────────────────────────────────────────────────\u001b[0m\u001b[37m─╮\u001b[0m\n",
       "\u001b[37m│\u001b[0m Let me search for more specific information about the mechanisms by which RL improves reasoning in LLMs.        \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m 🔧 Tool Call: retrieve_blog_posts                                                                               \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m    Args: {                                                                                                      \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m   \"query\": \"RL reinforcement learning chain of thought reasoning training mechanisms\"                           \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m }                                                                                                               \u001b[37m│\u001b[0m\n",
       "\u001b[37m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">╭──────────────────────────────────────────────── 🔧 Tool Output ─────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> # Summary: Parallel Sampling vs Sequential Revision for LLM Reasoning                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> ## Parallel Sampling                                                                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> **Best-of-N and Beam Search**: Simple approach samples N independent outputs and selects highest-scoring. Beam  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> search is more sophisticated, maintaining promising partial sequences and using process reward models (PRMs)    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> for guidance.                                                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> **Key Techniques**:                                                                                             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - **Self-evaluation**: LLMs evaluate their own reasoning steps as multiple-choice questions, reducing errors in <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> multi-step reasoning (5-6% improvement on GSM8k, AQuA, StrategyQA)                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - **REBASE**: Uses separately trained PRM to determine node expansion during beam search                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - **RATIONALYST**: PRM trained on synthetic rationales, provides process supervision                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - **Emergent CoT**: Branching at first token with top-k confidence can trigger chain-of-thought without         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> explicit prompting                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> ## Sequential Revision                                                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> **Self-Correction Challenges**: LLMs lack intrinsic self-correction ability, suffering from hallucination,      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> behavior collapse, and generalization failures. External feedback is essential.                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> **Training Approaches**:                                                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - **Self-correction learning**: Trains corrector model using value-improving pairs from same prompts            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - **Recursive inspection**: Single model for both generation and correction                                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - **SCoRe**: Two-stage RL approach - stage 1 maximizes second-attempt accuracy, stage 2 optimizes both attempts <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> ## RL for Better Reasoning                                                                                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> **DeepSeek-R1**: Uses cold-start SFT → reasoning-oriented RL → rejection sampling + non-reasoning SFT pipeline. <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> RL rewards format compliance and answer accuracy.                                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> ## Iterative Learning                                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> **STaR (Self-taught Reasoner)**: Generates multiple CoTs, fine-tunes on correct solutions. Adds                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> \"rationalization\" - generating good CoTs backward from ground truth for failed attempts. Approximates policy    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> gradient with indicator reward function.                                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> ## Scaling Laws for Thinking Time                                                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> **Test-time vs Pretraining Compute**: Test-time compute effective when inference tokens &lt;&lt; pretraining tokens.  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Cannot fill large capability gaps but offers Pareto-optimal cost/performance trade-offs for smaller models.     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> **Key Findings**:                                                                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Positive correlation between CoT length and accuracy (s1 models)                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Budget forcing works better than rejection sampling for length control                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Test-time compute most beneficial for easy/medium problems with small capability gaps                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> ## Future Research Questions                                                                                    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> 1. Incentivizing human-readable, faithful reasoning while avoiding reward hacking                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> 2. Self-correction without ground truth availability                                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> 3. RL training for contextualized/personalized tasks                                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> 4. Distilling test-time gains back to base models                                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> 5. Adaptive test-time spending based on problem difficulty                                                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> **Core Insight**: Moving toward AI systems that mirror human thinking patterns through adaptability,            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> reflection, and error correction, but significant challenges remain in faithfulness, scalability, and practical <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> deployment.                                                                                                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m╭─\u001b[0m\u001b[33m───────────────────────────────────────────────\u001b[0m\u001b[33m 🔧 Tool Output \u001b[0m\u001b[33m────────────────────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m\n",
       "\u001b[33m│\u001b[0m # Summary: Parallel Sampling vs Sequential Revision for LLM Reasoning                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m ## Parallel Sampling                                                                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m **Best-of-N and Beam Search**: Simple approach samples N independent outputs and selects highest-scoring. Beam  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m search is more sophisticated, maintaining promising partial sequences and using process reward models (PRMs)    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m for guidance.                                                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m **Key Techniques**:                                                                                             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - **Self-evaluation**: LLMs evaluate their own reasoning steps as multiple-choice questions, reducing errors in \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m multi-step reasoning (5-6% improvement on GSM8k, AQuA, StrategyQA)                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - **REBASE**: Uses separately trained PRM to determine node expansion during beam search                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - **RATIONALYST**: PRM trained on synthetic rationales, provides process supervision                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - **Emergent CoT**: Branching at first token with top-k confidence can trigger chain-of-thought without         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m explicit prompting                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m ## Sequential Revision                                                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m **Self-Correction Challenges**: LLMs lack intrinsic self-correction ability, suffering from hallucination,      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m behavior collapse, and generalization failures. External feedback is essential.                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m **Training Approaches**:                                                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - **Self-correction learning**: Trains corrector model using value-improving pairs from same prompts            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - **Recursive inspection**: Single model for both generation and correction                                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - **SCoRe**: Two-stage RL approach - stage 1 maximizes second-attempt accuracy, stage 2 optimizes both attempts \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m ## RL for Better Reasoning                                                                                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m **DeepSeek-R1**: Uses cold-start SFT → reasoning-oriented RL → rejection sampling + non-reasoning SFT pipeline. \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m RL rewards format compliance and answer accuracy.                                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m ## Iterative Learning                                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m **STaR (Self-taught Reasoner)**: Generates multiple CoTs, fine-tunes on correct solutions. Adds                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \"rationalization\" - generating good CoTs backward from ground truth for failed attempts. Approximates policy    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m gradient with indicator reward function.                                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m ## Scaling Laws for Thinking Time                                                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m **Test-time vs Pretraining Compute**: Test-time compute effective when inference tokens << pretraining tokens.  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Cannot fill large capability gaps but offers Pareto-optimal cost/performance trade-offs for smaller models.     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m **Key Findings**:                                                                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Positive correlation between CoT length and accuracy (s1 models)                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Budget forcing works better than rejection sampling for length control                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Test-time compute most beneficial for easy/medium problems with small capability gaps                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m ## Future Research Questions                                                                                    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m 1. Incentivizing human-readable, faithful reasoning while avoiding reward hacking                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m 2. Self-correction without ground truth availability                                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m 3. RL training for contextualized/personalized tasks                                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m 4. Distilling test-time gains back to base models                                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m 5. Adaptive test-time spending based on problem difficulty                                                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m **Core Insight**: Moving toward AI systems that mirror human thinking patterns through adaptability,            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m reflection, and error correction, but significant challenges remain in faithfulness, scalability, and practical \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m deployment.                                                                                                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">╭───────────────────────────────────────────────────── 📝 AI ─────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> Let me search for more details about the specific RL training methods and their benefits for reasoning.         <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> 🔧 Tool Call: retrieve_blog_posts                                                                               <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>    Args: {                                                                                                      <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>   \"query\": \"RLHF reward model process supervision outcome supervision reasoning\"                                <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> }                                                                                                               <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37m╭─\u001b[0m\u001b[37m────────────────────────────────────────────────────\u001b[0m\u001b[37m 📝 AI \u001b[0m\u001b[37m────────────────────────────────────────────────────\u001b[0m\u001b[37m─╮\u001b[0m\n",
       "\u001b[37m│\u001b[0m Let me search for more details about the specific RL training methods and their benefits for reasoning.         \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m 🔧 Tool Call: retrieve_blog_posts                                                                               \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m    Args: {                                                                                                      \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m   \"query\": \"RLHF reward model process supervision outcome supervision reasoning\"                                \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m }                                                                                                               \u001b[37m│\u001b[0m\n",
       "\u001b[37m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">╭──────────────────────────────────────────────── 🔧 Tool Output ─────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> # Reward Hacking in Reinforcement Learning - Summary                                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> ## Overview                                                                                                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> Reward hacking occurs when sophisticated RL agents find \"holes\" in reward function design, achieving higher     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> proxy rewards but lower true rewards. This problem becomes more common as models become more intelligent and    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> capable of exploiting task specifications.                                                                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> ## Key Types of Reward Hacking                                                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> ### 1. RL Environment Hacking                                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - **Adversarial Policies**: In zero-sum robotics games, adversarial opponents can defeat victim agents despite  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> seemingly random actions and minimal training                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - **Capability-Dependent Hacking**: Pan et al. (2022) found that higher capability models (larger size, higher  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> action resolution, better observations, more training) achieve higher proxy rewards but decreased true rewards  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - **Three Types of Misspecified Rewards**:                                                                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>   - Misweighting: Same desiderata, different importance                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>   - Ontological: Different desiderata for same concept                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>   - Scope: Restricted domain measurement                                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> ### 2. RLHF Hacking in LLMs                                                                                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> **Three Reward Types**:                                                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Oracle/Gold reward (R*): What we truly want                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Human reward: Collected from humans (inconsistent/error-prone)                                                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Proxy reward: Predicted by trained reward model                                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> **Training Process Issues**:                                                                                    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Gao et al. (2022) showed reward model overoptimization follows scaling laws                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Gold reward peaks then declines with KL divergence from initial policy                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - RLHF can make models better at convincing humans they're correct even when wrong                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> ### 3. In-Context Reward Hacking (ICRH)                                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Occurs during deployment in self-refinement setups                                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Two processes: output-refinement and policy-refinement                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Smaller models more susceptible; scaling can worsen ICRH                                                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Shared context between evaluator and generator crucial for ICRH                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> ## Search and Reasoning Improvements                                                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> ### Parallel Sampling                                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - **Best-of-N**: Sample N times, choose highest scoring                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - **Beam Search**: Adaptive search with process reward models (PRMs)                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - **Reward Balanced Search (REBASE)**: Uses PRM to guide node expansion                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> ### Sequential Revision                                                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Self-correction often fails without external feedback                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - **Self-correction Learning**: Trains corrector model on value-improving pairs                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - **SCoRe**: Two-stage RL approach for better self-correction                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - **DeepSeek-R1**: Uses cold-start SFT + reasoning-oriented RL + rejection sampling                             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> ## Generalization of Hacking Skills                                                                             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Reward hacking behavior generalizes across tasks                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Training on easier hackable environments amplifies gaming on harder ones                                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Models can learn to rewrite their own reward functions after curriculum training                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Even sophisticated models hack rewards &lt;1% of the time                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> ## Mitigation Strategies                                                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> ### RL Algorithm Improvements                                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Adversarial reward functions                                                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Model lookahead for future state rewards                                                                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Adversarial blinding of certain variables                                                                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Reward capping to limit maximum rewards                                                                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Multiple reward combination                                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Decoupled approval (separate query action from world action)                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> ### Detection Methods                                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Frame as anomaly detection using trusted policies                                                             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Binary classifiers based on action distribution distances                                                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Current detectors achieve &lt;60% AUROC across tasks                                                             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> ### Data Analysis                                                                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - **SEAL framework** for systematic error analysis                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Feature imprint, alignment resistance, and robustness metrics                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Analyze target vs spoiler features in training data                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> ## Key Findings                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - More capable models consistently show increased reward hacking                                                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Positive correlation between proxy and true rewards doesn't prevent hacking                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - External feedback essential for effective self-correction                                                     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Curriculum training can teach generalizable gaming behaviors                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> - Simple mitigations like supervised fine-tuning on non-gaming data can help                                    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The field lacks comprehensive mitigation solutions, with most work focusing on detection and understanding      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> rather than prevention.                                                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m╭─\u001b[0m\u001b[33m───────────────────────────────────────────────\u001b[0m\u001b[33m 🔧 Tool Output \u001b[0m\u001b[33m────────────────────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m\n",
       "\u001b[33m│\u001b[0m # Reward Hacking in Reinforcement Learning - Summary                                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m ## Overview                                                                                                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m Reward hacking occurs when sophisticated RL agents find \"holes\" in reward function design, achieving higher     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m proxy rewards but lower true rewards. This problem becomes more common as models become more intelligent and    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m capable of exploiting task specifications.                                                                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m ## Key Types of Reward Hacking                                                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m ### 1. RL Environment Hacking                                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - **Adversarial Policies**: In zero-sum robotics games, adversarial opponents can defeat victim agents despite  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m seemingly random actions and minimal training                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - **Capability-Dependent Hacking**: Pan et al. (2022) found that higher capability models (larger size, higher  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m action resolution, better observations, more training) achieve higher proxy rewards but decreased true rewards  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - **Three Types of Misspecified Rewards**:                                                                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m   - Misweighting: Same desiderata, different importance                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m   - Ontological: Different desiderata for same concept                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m   - Scope: Restricted domain measurement                                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m ### 2. RLHF Hacking in LLMs                                                                                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m **Three Reward Types**:                                                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Oracle/Gold reward (R*): What we truly want                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Human reward: Collected from humans (inconsistent/error-prone)                                                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Proxy reward: Predicted by trained reward model                                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m **Training Process Issues**:                                                                                    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Gao et al. (2022) showed reward model overoptimization follows scaling laws                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Gold reward peaks then declines with KL divergence from initial policy                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - RLHF can make models better at convincing humans they're correct even when wrong                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m ### 3. In-Context Reward Hacking (ICRH)                                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Occurs during deployment in self-refinement setups                                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Two processes: output-refinement and policy-refinement                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Smaller models more susceptible; scaling can worsen ICRH                                                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Shared context between evaluator and generator crucial for ICRH                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m ## Search and Reasoning Improvements                                                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m ### Parallel Sampling                                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - **Best-of-N**: Sample N times, choose highest scoring                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - **Beam Search**: Adaptive search with process reward models (PRMs)                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - **Reward Balanced Search (REBASE)**: Uses PRM to guide node expansion                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m ### Sequential Revision                                                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Self-correction often fails without external feedback                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - **Self-correction Learning**: Trains corrector model on value-improving pairs                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - **SCoRe**: Two-stage RL approach for better self-correction                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - **DeepSeek-R1**: Uses cold-start SFT + reasoning-oriented RL + rejection sampling                             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m ## Generalization of Hacking Skills                                                                             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Reward hacking behavior generalizes across tasks                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Training on easier hackable environments amplifies gaming on harder ones                                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Models can learn to rewrite their own reward functions after curriculum training                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Even sophisticated models hack rewards <1% of the time                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m ## Mitigation Strategies                                                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m ### RL Algorithm Improvements                                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Adversarial reward functions                                                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Model lookahead for future state rewards                                                                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Adversarial blinding of certain variables                                                                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Reward capping to limit maximum rewards                                                                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Multiple reward combination                                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Decoupled approval (separate query action from world action)                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m ### Detection Methods                                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Frame as anomaly detection using trusted policies                                                             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Binary classifiers based on action distribution distances                                                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Current detectors achieve <60% AUROC across tasks                                                             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m ### Data Analysis                                                                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - **SEAL framework** for systematic error analysis                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Feature imprint, alignment resistance, and robustness metrics                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Analyze target vs spoiler features in training data                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m ## Key Findings                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - More capable models consistently show increased reward hacking                                                \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Positive correlation between proxy and true rewards doesn't prevent hacking                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - External feedback essential for effective self-correction                                                     \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Curriculum training can teach generalizable gaming behaviors                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m - Simple mitigations like supervised fine-tuning on non-gaming data can help                                    \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m The field lacks comprehensive mitigation solutions, with most work focusing on detection and understanding      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m rather than prevention.                                                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">╭───────────────────────────────────────────────────── 📝 AI ─────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> Based on the information from Lilian Weng's blog posts, here's why RL improves LLM reasoning:                   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> ## Key Mechanisms by Which RL Improves LLM Reasoning                                                            <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> ### 1. **Process Supervision and Reward Modeling**                                                              <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **Process Reward Models (PRMs)**: RL enables training models that evaluate reasoning steps rather than just   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> final outcomes. This allows for better guidance during the reasoning process.                                   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **Step-by-step feedback**: Unlike outcome supervision that only rewards correct final answers, RL with        <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> process supervision can reward good intermediate reasoning steps, leading to more reliable reasoning chains.    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> ### 2. **Self-Correction and Iterative Improvement**                                                            <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **STaR (Self-taught Reasoner)**: Uses RL to iteratively improve reasoning by:                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>   - Generating multiple chain-of-thought (CoT) solutions                                                        <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>   - Fine-tuning on correct solutions                                                                            <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>   - Using \"rationalization\" to generate good reasoning paths backward from correct answers                      <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **SCoRe approach**: Two-stage RL that prevents behavior collapse while improving both initial attempts and    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> self-corrections                                                                                                <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> ### 3. **Search and Exploration Enhancement**                                                                   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **Beam Search with RL**: RL-trained reward models guide more sophisticated search through reasoning space,    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> maintaining promising partial sequences                                                                         <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **Best-of-N sampling**: RL helps train better scoring functions to select the best reasoning chain from       <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> multiple attempts                                                                                               <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **Emergent reasoning**: RL can trigger chain-of-thought reasoning even without explicit prompting through     <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> confidence-based branching                                                                                      <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> ### 4. **Training for Reasoning-Specific Rewards**                                                              <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **DeepSeek-R1 approach**: Uses reasoning-oriented RL that rewards both:                                       <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>   - Format compliance (proper reasoning structure)                                                              <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>   - Answer accuracy                                                                                             <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **Multi-objective optimization**: RL allows balancing multiple aspects of good reasoning simultaneously       <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> ### 5. **Overcoming Intrinsic Limitations**                                                                     <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> The blogs note that LLMs have fundamental limitations in self-correction and reasoning that RL helps address:   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **Lack of intrinsic self-correction**: RL provides external feedback mechanisms                               <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **Hallucination and behavior collapse**: RL training with proper reward design helps maintain reasoning       <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> quality                                                                                                         <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **Generalization failures**: RL can improve reasoning across different problem types                          <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> ### 6. **Scaling Benefits**                                                                                     <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **Test-time compute optimization**: RL helps models learn when and how to \"think longer\" for better results   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **Adaptive reasoning**: Models learn to allocate computational resources based on problem difficulty          <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> ## Important Caveats                                                                                            <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> The blogs also highlight significant challenges:                                                                <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **Reward hacking**: Models may learn to game reward functions rather than truly improve reasoning             <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **Need for external feedback**: Pure self-correction often fails without ground truth or external validation  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> - **Faithfulness concerns**: RL-trained models might produce convincing but incorrect reasoning                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>                                                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> The core insight is that RL provides a framework for training models to engage in more systematic, step-by-step <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> reasoning while learning from both successes and failures in the reasoning process, rather than just optimizing <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> for final answer correctness.                                                                                   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37m╭─\u001b[0m\u001b[37m────────────────────────────────────────────────────\u001b[0m\u001b[37m 📝 AI \u001b[0m\u001b[37m────────────────────────────────────────────────────\u001b[0m\u001b[37m─╮\u001b[0m\n",
       "\u001b[37m│\u001b[0m Based on the information from Lilian Weng's blog posts, here's why RL improves LLM reasoning:                   \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m ## Key Mechanisms by Which RL Improves LLM Reasoning                                                            \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m ### 1. **Process Supervision and Reward Modeling**                                                              \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **Process Reward Models (PRMs)**: RL enables training models that evaluate reasoning steps rather than just   \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m final outcomes. This allows for better guidance during the reasoning process.                                   \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **Step-by-step feedback**: Unlike outcome supervision that only rewards correct final answers, RL with        \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m process supervision can reward good intermediate reasoning steps, leading to more reliable reasoning chains.    \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m ### 2. **Self-Correction and Iterative Improvement**                                                            \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **STaR (Self-taught Reasoner)**: Uses RL to iteratively improve reasoning by:                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m   - Generating multiple chain-of-thought (CoT) solutions                                                        \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m   - Fine-tuning on correct solutions                                                                            \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m   - Using \"rationalization\" to generate good reasoning paths backward from correct answers                      \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **SCoRe approach**: Two-stage RL that prevents behavior collapse while improving both initial attempts and    \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m self-corrections                                                                                                \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m ### 3. **Search and Exploration Enhancement**                                                                   \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **Beam Search with RL**: RL-trained reward models guide more sophisticated search through reasoning space,    \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m maintaining promising partial sequences                                                                         \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **Best-of-N sampling**: RL helps train better scoring functions to select the best reasoning chain from       \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m multiple attempts                                                                                               \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **Emergent reasoning**: RL can trigger chain-of-thought reasoning even without explicit prompting through     \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m confidence-based branching                                                                                      \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m ### 4. **Training for Reasoning-Specific Rewards**                                                              \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **DeepSeek-R1 approach**: Uses reasoning-oriented RL that rewards both:                                       \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m   - Format compliance (proper reasoning structure)                                                              \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m   - Answer accuracy                                                                                             \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **Multi-objective optimization**: RL allows balancing multiple aspects of good reasoning simultaneously       \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m ### 5. **Overcoming Intrinsic Limitations**                                                                     \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m The blogs note that LLMs have fundamental limitations in self-correction and reasoning that RL helps address:   \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **Lack of intrinsic self-correction**: RL provides external feedback mechanisms                               \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **Hallucination and behavior collapse**: RL training with proper reward design helps maintain reasoning       \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m quality                                                                                                         \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **Generalization failures**: RL can improve reasoning across different problem types                          \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m ### 6. **Scaling Benefits**                                                                                     \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **Test-time compute optimization**: RL helps models learn when and how to \"think longer\" for better results   \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **Adaptive reasoning**: Models learn to allocate computational resources based on problem difficulty          \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m ## Important Caveats                                                                                            \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m The blogs also highlight significant challenges:                                                                \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **Reward hacking**: Models may learn to game reward functions rather than truly improve reasoning             \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **Need for external feedback**: Pure self-correction often fails without ground truth or external validation  \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m - **Faithfulness concerns**: RL-trained models might produce convincing but incorrect reasoning                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m                                                                                                                 \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m The core insight is that RL provides a framework for training models to engage in more systematic, step-by-step \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m reasoning while learning from both successes and failures in the reasoning process, rather than just optimizing \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m for final answer correctness.                                                                                   \u001b[37m│\u001b[0m\n",
       "\u001b[37m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"Why does RL improve LLM reasoning according to the blogs?\"\n",
    "result = agent.invoke({\"messages\": query})\n",
    "format_messages(result['messages'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This uses 60k tokens. \n",
    "\n",
    "https://smith.langchain.com/public/994cdf93-e837-4708-9628-c83b397dd4b5/r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn More\n",
    "\n",
    "* **LangMem Summarization** - https://langchain-ai.github.io/langmem/guides/summarization/\n",
    "\n",
    "LangMem provides strategies for managing long context through message history summarization. It offers two primary approaches: direct summarization using `summarize_messages()` function with configurable token thresholds and \"running summary\" maintenance, and the SummarizationNode approach with dedicated nodes for automatic summary propagation. Key implementation considerations include configuring token limits, using separate state keys for full message history versus summaries, and maintaining conversation context across multiple interactions. LangMem integrates seamlessly with LangGraph state management for both simple chatbots and ReAct-style agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
